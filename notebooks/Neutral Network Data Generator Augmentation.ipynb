{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1227 images belonging to 7 classes.\n",
      "Found 335 images belonging to 7 classes.\n",
      "Found 331 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Image Data Generator w/ no augmentation\n",
    "#Scaling for pixels\n",
    "piece_train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "piece_test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255)\n",
    "piece_valid_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255)\n",
    "\n",
    "\n",
    "#Flow data from directory\n",
    "\n",
    "piece_train_iter = piece_train_datagen.flow_from_directory(\n",
    "    directory = 'data/piece_data/train',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "piece_test_iter = piece_test_datagen.flow_from_directory(\n",
    "    directory = 'data/piece_data/test',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "piece_valid_iter = piece_valid_datagen.flow_from_directory(\n",
    "    directory = 'data/piece_data/valid',\n",
    "    target_size = (135,135),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 135, 135, 16)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 67, 67, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 67, 67, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 67, 67, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 33, 33, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 33, 33, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 33, 33, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       32896     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 64)          32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 64)          16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              133120    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 14343     \n",
      "=================================================================\n",
      "Total params: 4,506,439\n",
      "Trainable params: 4,505,703\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define NN architecture\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "piece_model = Sequential()\n",
    "piece_model.add(Conv2D(filters=16, kernel_size=5, padding='same', activation='relu', \n",
    "                        input_shape=(135, 135, 1)))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "piece_model.add(MaxPooling2D(pool_size=2))\n",
    "piece_model.add(BatchNormalization())\n",
    "piece_model.add(Dropout(0.3))\n",
    "piece_model.add(GlobalAveragePooling2D())\n",
    "piece_model.add(Dense(2048, activation='relu'))\n",
    "piece_model.add(Dense(2048, activation='relu'))\n",
    "piece_model.add(Dropout(0.4))\n",
    "piece_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "piece_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "piece_model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.00001), \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "STEP_SIZE_TRAIN = piece_train_iter.n/piece_train_iter.batch_size\n",
    "STEP_SIZE_VALID = piece_valid_iter.n/piece_valid_iter.batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "Epoch 00000: val_loss improved from inf to 1.93953, saving model to piece_model.weights.best.hdf5\n",
      "4s - loss: 1.9072 - acc: 0.2446 - val_loss: 1.9395 - val_acc: 0.1511\n",
      "Epoch 2/1500\n",
      "Epoch 00001: val_loss did not improve\n",
      "2s - loss: 1.8536 - acc: 0.2491 - val_loss: 1.9489 - val_acc: 0.1511\n",
      "Epoch 3/1500\n",
      "Epoch 00002: val_loss did not improve\n",
      "2s - loss: 1.8069 - acc: 0.2507 - val_loss: 1.9860 - val_acc: 0.1420\n",
      "Epoch 4/1500\n",
      "Epoch 00003: val_loss did not improve\n",
      "2s - loss: 1.7909 - acc: 0.2716 - val_loss: 2.0259 - val_acc: 0.1631\n",
      "Epoch 5/1500\n",
      "Epoch 00004: val_loss did not improve\n",
      "2s - loss: 1.7484 - acc: 0.2913 - val_loss: 2.1151 - val_acc: 0.1571\n",
      "Epoch 6/1500\n",
      "Epoch 00005: val_loss did not improve\n",
      "2s - loss: 1.7174 - acc: 0.3003 - val_loss: 2.2203 - val_acc: 0.1420\n",
      "Epoch 7/1500\n",
      "Epoch 00006: val_loss did not improve\n",
      "2s - loss: 1.6845 - acc: 0.3297 - val_loss: 2.2654 - val_acc: 0.1450\n",
      "Epoch 8/1500\n",
      "Epoch 00007: val_loss did not improve\n",
      "2s - loss: 1.6755 - acc: 0.3316 - val_loss: 2.2093 - val_acc: 0.1480\n",
      "Epoch 9/1500\n",
      "Epoch 00008: val_loss did not improve\n",
      "2s - loss: 1.6524 - acc: 0.3553 - val_loss: 2.1655 - val_acc: 0.1571\n",
      "Epoch 10/1500\n",
      "Epoch 00009: val_loss did not improve\n",
      "2s - loss: 1.6154 - acc: 0.3578 - val_loss: 2.1625 - val_acc: 0.1541\n",
      "Epoch 11/1500\n",
      "Epoch 00010: val_loss did not improve\n",
      "2s - loss: 1.6024 - acc: 0.3705 - val_loss: 2.1649 - val_acc: 0.1299\n",
      "Epoch 12/1500\n",
      "Epoch 00011: val_loss did not improve\n",
      "2s - loss: 1.5896 - acc: 0.3498 - val_loss: 1.9791 - val_acc: 0.1813\n",
      "Epoch 13/1500\n",
      "Epoch 00012: val_loss improved from 1.93953 to 1.90697, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.5770 - acc: 0.3822 - val_loss: 1.9070 - val_acc: 0.2115\n",
      "Epoch 14/1500\n",
      "Epoch 00013: val_loss improved from 1.90697 to 1.81927, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.5684 - acc: 0.3842 - val_loss: 1.8193 - val_acc: 0.2779\n",
      "Epoch 15/1500\n",
      "Epoch 00014: val_loss improved from 1.81927 to 1.72084, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.5512 - acc: 0.3847 - val_loss: 1.7208 - val_acc: 0.2477\n",
      "Epoch 16/1500\n",
      "Epoch 00015: val_loss improved from 1.72084 to 1.68482, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.5340 - acc: 0.4085 - val_loss: 1.6848 - val_acc: 0.2991\n",
      "Epoch 17/1500\n",
      "Epoch 00016: val_loss improved from 1.68482 to 1.67529, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.5208 - acc: 0.3989 - val_loss: 1.6753 - val_acc: 0.2870\n",
      "Epoch 18/1500\n",
      "Epoch 00017: val_loss improved from 1.67529 to 1.62930, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.5226 - acc: 0.4011 - val_loss: 1.6293 - val_acc: 0.2900\n",
      "Epoch 19/1500\n",
      "Epoch 00018: val_loss improved from 1.62930 to 1.59453, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.4994 - acc: 0.4020 - val_loss: 1.5945 - val_acc: 0.3414\n",
      "Epoch 20/1500\n",
      "Epoch 00019: val_loss improved from 1.59453 to 1.58125, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.5194 - acc: 0.3958 - val_loss: 1.5813 - val_acc: 0.3293\n",
      "Epoch 21/1500\n",
      "Epoch 00020: val_loss improved from 1.58125 to 1.53013, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.4906 - acc: 0.3940 - val_loss: 1.5301 - val_acc: 0.3474\n",
      "Epoch 22/1500\n",
      "Epoch 00021: val_loss did not improve\n",
      "2s - loss: 1.4665 - acc: 0.3993 - val_loss: 1.5647 - val_acc: 0.3293\n",
      "Epoch 23/1500\n",
      "Epoch 00022: val_loss did not improve\n",
      "2s - loss: 1.4819 - acc: 0.4141 - val_loss: 1.5325 - val_acc: 0.3444\n",
      "Epoch 24/1500\n",
      "Epoch 00023: val_loss improved from 1.53013 to 1.49894, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.4589 - acc: 0.4242 - val_loss: 1.4989 - val_acc: 0.3233\n",
      "Epoch 25/1500\n",
      "Epoch 00024: val_loss did not improve\n",
      "2s - loss: 1.4583 - acc: 0.4186 - val_loss: 1.5373 - val_acc: 0.3565\n",
      "Epoch 26/1500\n",
      "Epoch 00025: val_loss did not improve\n",
      "2s - loss: 1.4384 - acc: 0.4316 - val_loss: 1.5254 - val_acc: 0.3535\n",
      "Epoch 27/1500\n",
      "Epoch 00026: val_loss did not improve\n",
      "2s - loss: 1.4253 - acc: 0.4411 - val_loss: 1.5033 - val_acc: 0.3323\n",
      "Epoch 28/1500\n",
      "Epoch 00027: val_loss improved from 1.49894 to 1.44729, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.4401 - acc: 0.4163 - val_loss: 1.4473 - val_acc: 0.4018\n",
      "Epoch 29/1500\n",
      "Epoch 00028: val_loss did not improve\n",
      "2s - loss: 1.4255 - acc: 0.4203 - val_loss: 1.4934 - val_acc: 0.3716\n",
      "Epoch 30/1500\n",
      "Epoch 00029: val_loss did not improve\n",
      "2s - loss: 1.4226 - acc: 0.4203 - val_loss: 1.5254 - val_acc: 0.3716\n",
      "Epoch 31/1500\n",
      "Epoch 00030: val_loss improved from 1.44729 to 1.40475, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.4334 - acc: 0.4330 - val_loss: 1.4048 - val_acc: 0.4230\n",
      "Epoch 32/1500\n",
      "Epoch 00031: val_loss did not improve\n",
      "2s - loss: 1.4087 - acc: 0.4260 - val_loss: 1.5149 - val_acc: 0.3505\n",
      "Epoch 33/1500\n",
      "Epoch 00032: val_loss did not improve\n",
      "2s - loss: 1.4234 - acc: 0.4354 - val_loss: 1.4729 - val_acc: 0.3716\n",
      "Epoch 34/1500\n",
      "Epoch 00033: val_loss did not improve\n",
      "2s - loss: 1.3955 - acc: 0.4474 - val_loss: 1.4805 - val_acc: 0.3807\n",
      "Epoch 35/1500\n",
      "Epoch 00034: val_loss did not improve\n",
      "2s - loss: 1.4097 - acc: 0.4356 - val_loss: 1.4958 - val_acc: 0.3837\n",
      "Epoch 36/1500\n",
      "Epoch 00035: val_loss did not improve\n",
      "2s - loss: 1.3915 - acc: 0.4474 - val_loss: 1.4665 - val_acc: 0.3776\n",
      "Epoch 37/1500\n",
      "Epoch 00036: val_loss did not improve\n",
      "2s - loss: 1.4021 - acc: 0.4316 - val_loss: 1.4583 - val_acc: 0.3595\n",
      "Epoch 38/1500\n",
      "Epoch 00037: val_loss did not improve\n",
      "2s - loss: 1.3831 - acc: 0.4525 - val_loss: 1.4492 - val_acc: 0.3958\n",
      "Epoch 39/1500\n",
      "Epoch 00038: val_loss did not improve\n",
      "2s - loss: 1.3921 - acc: 0.4516 - val_loss: 1.4415 - val_acc: 0.3746\n",
      "Epoch 40/1500\n",
      "Epoch 00039: val_loss did not improve\n",
      "2s - loss: 1.3809 - acc: 0.4501 - val_loss: 1.4218 - val_acc: 0.3867\n",
      "Epoch 41/1500\n",
      "Epoch 00040: val_loss did not improve\n",
      "2s - loss: 1.3632 - acc: 0.4788 - val_loss: 1.4556 - val_acc: 0.3444\n",
      "Epoch 42/1500\n",
      "Epoch 00041: val_loss did not improve\n",
      "2s - loss: 1.3576 - acc: 0.4658 - val_loss: 1.4332 - val_acc: 0.3656\n",
      "Epoch 43/1500\n",
      "Epoch 00042: val_loss did not improve\n",
      "2s - loss: 1.3644 - acc: 0.4516 - val_loss: 1.4295 - val_acc: 0.3686\n",
      "Epoch 44/1500\n",
      "Epoch 00043: val_loss did not improve\n",
      "2s - loss: 1.3475 - acc: 0.4833 - val_loss: 1.4401 - val_acc: 0.3746\n",
      "Epoch 45/1500\n",
      "Epoch 00044: val_loss did not improve\n",
      "2s - loss: 1.3429 - acc: 0.4478 - val_loss: 1.4352 - val_acc: 0.3686\n",
      "Epoch 46/1500\n",
      "Epoch 00045: val_loss did not improve\n",
      "2s - loss: 1.3538 - acc: 0.4588 - val_loss: 1.4573 - val_acc: 0.3807\n",
      "Epoch 47/1500\n",
      "Epoch 00046: val_loss did not improve\n",
      "2s - loss: 1.3511 - acc: 0.4645 - val_loss: 1.5076 - val_acc: 0.3414\n",
      "Epoch 48/1500\n",
      "Epoch 00047: val_loss improved from 1.40475 to 1.38929, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.3398 - acc: 0.4630 - val_loss: 1.3893 - val_acc: 0.4230\n",
      "Epoch 49/1500\n",
      "Epoch 00048: val_loss did not improve\n",
      "2s - loss: 1.3242 - acc: 0.4794 - val_loss: 1.4076 - val_acc: 0.4109\n",
      "Epoch 50/1500\n",
      "Epoch 00049: val_loss did not improve\n",
      "2s - loss: 1.3610 - acc: 0.4481 - val_loss: 1.4351 - val_acc: 0.3867\n",
      "Epoch 51/1500\n",
      "Epoch 00050: val_loss did not improve\n",
      "2s - loss: 1.3293 - acc: 0.4772 - val_loss: 1.4242 - val_acc: 0.3988\n",
      "Epoch 52/1500\n",
      "Epoch 00051: val_loss did not improve\n",
      "2s - loss: 1.3244 - acc: 0.4797 - val_loss: 1.4433 - val_acc: 0.4260\n",
      "Epoch 53/1500\n",
      "Epoch 00052: val_loss did not improve\n",
      "2s - loss: 1.3346 - acc: 0.4718 - val_loss: 1.4045 - val_acc: 0.4139\n",
      "Epoch 54/1500\n",
      "Epoch 00053: val_loss did not improve\n",
      "2s - loss: 1.3252 - acc: 0.4691 - val_loss: 1.4130 - val_acc: 0.4048\n",
      "Epoch 55/1500\n",
      "Epoch 00054: val_loss improved from 1.38929 to 1.38413, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.3054 - acc: 0.5021 - val_loss: 1.3841 - val_acc: 0.4199\n",
      "Epoch 56/1500\n",
      "Epoch 00055: val_loss did not improve\n",
      "2s - loss: 1.3239 - acc: 0.4813 - val_loss: 1.4006 - val_acc: 0.4381\n",
      "Epoch 57/1500\n",
      "Epoch 00056: val_loss improved from 1.38413 to 1.38059, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.3210 - acc: 0.4648 - val_loss: 1.3806 - val_acc: 0.4230\n",
      "Epoch 58/1500\n",
      "Epoch 00057: val_loss did not improve\n",
      "2s - loss: 1.3050 - acc: 0.4828 - val_loss: 1.4406 - val_acc: 0.3958\n",
      "Epoch 59/1500\n",
      "Epoch 00058: val_loss did not improve\n",
      "2s - loss: 1.3146 - acc: 0.4721 - val_loss: 1.3877 - val_acc: 0.4260\n",
      "Epoch 60/1500\n",
      "Epoch 00059: val_loss did not improve\n",
      "2s - loss: 1.3101 - acc: 0.4788 - val_loss: 1.4033 - val_acc: 0.4048\n",
      "Epoch 61/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00060: val_loss improved from 1.38059 to 1.35009, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.2927 - acc: 0.4908 - val_loss: 1.3501 - val_acc: 0.4079\n",
      "Epoch 62/1500\n",
      "Epoch 00061: val_loss improved from 1.35009 to 1.33675, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.3080 - acc: 0.4718 - val_loss: 1.3368 - val_acc: 0.4411\n",
      "Epoch 63/1500\n",
      "Epoch 00062: val_loss did not improve\n",
      "2s - loss: 1.3175 - acc: 0.4740 - val_loss: 1.4083 - val_acc: 0.4079\n",
      "Epoch 64/1500\n",
      "Epoch 00063: val_loss did not improve\n",
      "2s - loss: 1.2884 - acc: 0.4782 - val_loss: 1.4218 - val_acc: 0.3927\n",
      "Epoch 65/1500\n",
      "Epoch 00064: val_loss did not improve\n",
      "2s - loss: 1.3258 - acc: 0.4903 - val_loss: 1.3817 - val_acc: 0.4290\n",
      "Epoch 66/1500\n",
      "Epoch 00065: val_loss did not improve\n",
      "2s - loss: 1.2752 - acc: 0.4971 - val_loss: 1.3453 - val_acc: 0.4290\n",
      "Epoch 67/1500\n",
      "Epoch 00066: val_loss did not improve\n",
      "2s - loss: 1.2778 - acc: 0.4814 - val_loss: 1.3624 - val_acc: 0.4350\n",
      "Epoch 68/1500\n",
      "Epoch 00067: val_loss improved from 1.33675 to 1.32148, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.2593 - acc: 0.5174 - val_loss: 1.3215 - val_acc: 0.4290\n",
      "Epoch 69/1500\n",
      "Epoch 00068: val_loss improved from 1.32148 to 1.31471, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.2555 - acc: 0.5048 - val_loss: 1.3147 - val_acc: 0.4471\n",
      "Epoch 70/1500\n",
      "Epoch 00069: val_loss improved from 1.31471 to 1.30728, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.2692 - acc: 0.5023 - val_loss: 1.3073 - val_acc: 0.4562\n",
      "Epoch 71/1500\n",
      "Epoch 00070: val_loss did not improve\n",
      "2s - loss: 1.2663 - acc: 0.5037 - val_loss: 1.3556 - val_acc: 0.4169\n",
      "Epoch 72/1500\n",
      "Epoch 00071: val_loss did not improve\n",
      "2s - loss: 1.2644 - acc: 0.5126 - val_loss: 1.3721 - val_acc: 0.3867\n",
      "Epoch 73/1500\n",
      "Epoch 00072: val_loss did not improve\n",
      "2s - loss: 1.2728 - acc: 0.4943 - val_loss: 1.3591 - val_acc: 0.4350\n",
      "Epoch 74/1500\n",
      "Epoch 00073: val_loss did not improve\n",
      "2s - loss: 1.2286 - acc: 0.5203 - val_loss: 1.3291 - val_acc: 0.4471\n",
      "Epoch 75/1500\n",
      "Epoch 00074: val_loss did not improve\n",
      "2s - loss: 1.2620 - acc: 0.4964 - val_loss: 1.3864 - val_acc: 0.4048\n",
      "Epoch 76/1500\n",
      "Epoch 00075: val_loss did not improve\n",
      "2s - loss: 1.2504 - acc: 0.5166 - val_loss: 1.3466 - val_acc: 0.4169\n",
      "Epoch 77/1500\n",
      "Epoch 00076: val_loss did not improve\n",
      "2s - loss: 1.2373 - acc: 0.5068 - val_loss: 1.3519 - val_acc: 0.4653\n",
      "Epoch 78/1500\n",
      "Epoch 00077: val_loss improved from 1.30728 to 1.28683, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.2123 - acc: 0.5380 - val_loss: 1.2868 - val_acc: 0.4773\n",
      "Epoch 79/1500\n",
      "Epoch 00078: val_loss did not improve\n",
      "2s - loss: 1.2496 - acc: 0.5126 - val_loss: 1.3127 - val_acc: 0.4683\n",
      "Epoch 80/1500\n",
      "Epoch 00079: val_loss did not improve\n",
      "2s - loss: 1.2506 - acc: 0.5050 - val_loss: 1.3102 - val_acc: 0.4471\n",
      "Epoch 81/1500\n",
      "Epoch 00080: val_loss did not improve\n",
      "2s - loss: 1.2366 - acc: 0.5117 - val_loss: 1.3090 - val_acc: 0.4562\n",
      "Epoch 82/1500\n",
      "Epoch 00081: val_loss did not improve\n",
      "2s - loss: 1.2237 - acc: 0.5101 - val_loss: 1.3089 - val_acc: 0.4653\n",
      "Epoch 83/1500\n",
      "Epoch 00082: val_loss did not improve\n",
      "2s - loss: 1.2570 - acc: 0.5231 - val_loss: 1.3471 - val_acc: 0.4592\n",
      "Epoch 84/1500\n",
      "Epoch 00083: val_loss did not improve\n",
      "2s - loss: 1.2289 - acc: 0.5285 - val_loss: 1.2949 - val_acc: 0.4381\n",
      "Epoch 85/1500\n",
      "Epoch 00084: val_loss did not improve\n",
      "2s - loss: 1.2345 - acc: 0.5219 - val_loss: 1.3082 - val_acc: 0.4562\n",
      "Epoch 86/1500\n",
      "Epoch 00085: val_loss did not improve\n",
      "2s - loss: 1.2384 - acc: 0.4989 - val_loss: 1.3297 - val_acc: 0.4532\n",
      "Epoch 87/1500\n",
      "Epoch 00086: val_loss did not improve\n",
      "2s - loss: 1.2131 - acc: 0.5408 - val_loss: 1.3036 - val_acc: 0.4653\n",
      "Epoch 88/1500\n",
      "Epoch 00087: val_loss did not improve\n",
      "2s - loss: 1.1883 - acc: 0.5284 - val_loss: 1.2919 - val_acc: 0.4743\n",
      "Epoch 89/1500\n",
      "Epoch 00088: val_loss did not improve\n",
      "2s - loss: 1.2008 - acc: 0.5235 - val_loss: 1.3032 - val_acc: 0.4411\n",
      "Epoch 90/1500\n",
      "Epoch 00089: val_loss improved from 1.28683 to 1.25751, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.2041 - acc: 0.5424 - val_loss: 1.2575 - val_acc: 0.4955\n",
      "Epoch 91/1500\n",
      "Epoch 00090: val_loss did not improve\n",
      "2s - loss: 1.2116 - acc: 0.5273 - val_loss: 1.3006 - val_acc: 0.4471\n",
      "Epoch 92/1500\n",
      "Epoch 00091: val_loss improved from 1.25751 to 1.21014, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.2353 - acc: 0.5155 - val_loss: 1.2101 - val_acc: 0.4955\n",
      "Epoch 93/1500\n",
      "Epoch 00092: val_loss did not improve\n",
      "2s - loss: 1.2164 - acc: 0.5038 - val_loss: 1.2795 - val_acc: 0.4713\n",
      "Epoch 94/1500\n",
      "Epoch 00093: val_loss did not improve\n",
      "2s - loss: 1.1799 - acc: 0.5301 - val_loss: 1.2734 - val_acc: 0.4713\n",
      "Epoch 95/1500\n",
      "Epoch 00094: val_loss did not improve\n",
      "2s - loss: 1.2209 - acc: 0.5139 - val_loss: 1.3301 - val_acc: 0.4441\n",
      "Epoch 96/1500\n",
      "Epoch 00095: val_loss did not improve\n",
      "2s - loss: 1.1876 - acc: 0.5333 - val_loss: 1.3406 - val_acc: 0.4773\n",
      "Epoch 97/1500\n",
      "Epoch 00096: val_loss did not improve\n",
      "2s - loss: 1.2099 - acc: 0.5158 - val_loss: 1.2947 - val_acc: 0.4713\n",
      "Epoch 98/1500\n",
      "Epoch 00097: val_loss did not improve\n",
      "2s - loss: 1.2007 - acc: 0.5284 - val_loss: 1.3245 - val_acc: 0.4532\n",
      "Epoch 99/1500\n",
      "Epoch 00098: val_loss improved from 1.21014 to 1.20436, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.2014 - acc: 0.5395 - val_loss: 1.2044 - val_acc: 0.4834\n",
      "Epoch 100/1500\n",
      "Epoch 00099: val_loss did not improve\n",
      "2s - loss: 1.1990 - acc: 0.5526 - val_loss: 1.2629 - val_acc: 0.4713\n",
      "Epoch 101/1500\n",
      "Epoch 00100: val_loss did not improve\n",
      "2s - loss: 1.1788 - acc: 0.5347 - val_loss: 1.2395 - val_acc: 0.4985\n",
      "Epoch 102/1500\n",
      "Epoch 00101: val_loss did not improve\n",
      "2s - loss: 1.1952 - acc: 0.5292 - val_loss: 1.2184 - val_acc: 0.5015\n",
      "Epoch 103/1500\n",
      "Epoch 00102: val_loss did not improve\n",
      "2s - loss: 1.1851 - acc: 0.5363 - val_loss: 1.2585 - val_acc: 0.4924\n",
      "Epoch 104/1500\n",
      "Epoch 00103: val_loss did not improve\n",
      "2s - loss: 1.1532 - acc: 0.5605 - val_loss: 1.3354 - val_acc: 0.4411\n",
      "Epoch 105/1500\n",
      "Epoch 00104: val_loss improved from 1.20436 to 1.18253, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.1748 - acc: 0.5495 - val_loss: 1.1825 - val_acc: 0.5227\n",
      "Epoch 106/1500\n",
      "Epoch 00105: val_loss did not improve\n",
      "2s - loss: 1.1808 - acc: 0.5394 - val_loss: 1.3025 - val_acc: 0.4441\n",
      "Epoch 107/1500\n",
      "Epoch 00106: val_loss did not improve\n",
      "2s - loss: 1.1757 - acc: 0.5308 - val_loss: 1.2405 - val_acc: 0.4804\n",
      "Epoch 108/1500\n",
      "Epoch 00107: val_loss did not improve\n",
      "2s - loss: 1.1992 - acc: 0.5307 - val_loss: 1.2226 - val_acc: 0.4804\n",
      "Epoch 109/1500\n",
      "Epoch 00108: val_loss did not improve\n",
      "2s - loss: 1.1536 - acc: 0.5586 - val_loss: 1.2390 - val_acc: 0.4834\n",
      "Epoch 110/1500\n",
      "Epoch 00109: val_loss did not improve\n",
      "2s - loss: 1.1427 - acc: 0.5613 - val_loss: 1.2335 - val_acc: 0.4834\n",
      "Epoch 111/1500\n",
      "Epoch 00110: val_loss did not improve\n",
      "2s - loss: 1.1594 - acc: 0.5558 - val_loss: 1.2436 - val_acc: 0.5076\n",
      "Epoch 112/1500\n",
      "Epoch 00111: val_loss did not improve\n",
      "2s - loss: 1.1402 - acc: 0.5757 - val_loss: 1.2852 - val_acc: 0.4773\n",
      "Epoch 113/1500\n",
      "Epoch 00112: val_loss did not improve\n",
      "2s - loss: 1.1570 - acc: 0.5405 - val_loss: 1.2719 - val_acc: 0.4743\n",
      "Epoch 114/1500\n",
      "Epoch 00113: val_loss did not improve\n",
      "2s - loss: 1.1739 - acc: 0.5447 - val_loss: 1.2631 - val_acc: 0.4683\n",
      "Epoch 115/1500\n",
      "Epoch 00114: val_loss did not improve\n",
      "2s - loss: 1.1533 - acc: 0.5472 - val_loss: 1.2204 - val_acc: 0.4985\n",
      "Epoch 116/1500\n",
      "Epoch 00115: val_loss improved from 1.18253 to 1.15558, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.1645 - acc: 0.5515 - val_loss: 1.1556 - val_acc: 0.5106\n",
      "Epoch 117/1500\n",
      "Epoch 00116: val_loss did not improve\n",
      "2s - loss: 1.1516 - acc: 0.5508 - val_loss: 1.1668 - val_acc: 0.4894\n",
      "Epoch 118/1500\n",
      "Epoch 00117: val_loss did not improve\n",
      "2s - loss: 1.1526 - acc: 0.5622 - val_loss: 1.2740 - val_acc: 0.4773\n",
      "Epoch 119/1500\n",
      "Epoch 00118: val_loss did not improve\n",
      "2s - loss: 1.1493 - acc: 0.5607 - val_loss: 1.2338 - val_acc: 0.4864\n",
      "Epoch 120/1500\n",
      "Epoch 00119: val_loss did not improve\n",
      "2s - loss: 1.1475 - acc: 0.5614 - val_loss: 1.2087 - val_acc: 0.5045\n",
      "Epoch 121/1500\n",
      "Epoch 00120: val_loss did not improve\n",
      "2s - loss: 1.1076 - acc: 0.5586 - val_loss: 1.1939 - val_acc: 0.5136\n",
      "Epoch 122/1500\n",
      "Epoch 00121: val_loss did not improve\n",
      "2s - loss: 1.1421 - acc: 0.5516 - val_loss: 1.2886 - val_acc: 0.4955\n",
      "Epoch 123/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00122: val_loss did not improve\n",
      "2s - loss: 1.1494 - acc: 0.5389 - val_loss: 1.2249 - val_acc: 0.4985\n",
      "Epoch 124/1500\n",
      "Epoch 00123: val_loss did not improve\n",
      "2s - loss: 1.1129 - acc: 0.5749 - val_loss: 1.2608 - val_acc: 0.4773\n",
      "Epoch 125/1500\n",
      "Epoch 00124: val_loss improved from 1.15558 to 1.12204, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.1331 - acc: 0.5776 - val_loss: 1.1220 - val_acc: 0.5347\n",
      "Epoch 126/1500\n",
      "Epoch 00125: val_loss did not improve\n",
      "2s - loss: 1.1332 - acc: 0.5628 - val_loss: 1.2095 - val_acc: 0.4955\n",
      "Epoch 127/1500\n",
      "Epoch 00126: val_loss did not improve\n",
      "2s - loss: 1.1054 - acc: 0.5645 - val_loss: 1.1239 - val_acc: 0.5529\n",
      "Epoch 128/1500\n",
      "Epoch 00127: val_loss did not improve\n",
      "2s - loss: 1.1131 - acc: 0.5812 - val_loss: 1.1824 - val_acc: 0.5015\n",
      "Epoch 129/1500\n",
      "Epoch 00128: val_loss did not improve\n",
      "2s - loss: 1.0934 - acc: 0.5753 - val_loss: 1.1760 - val_acc: 0.5438\n",
      "Epoch 130/1500\n",
      "Epoch 00129: val_loss did not improve\n",
      "2s - loss: 1.1215 - acc: 0.5651 - val_loss: 1.1756 - val_acc: 0.5287\n",
      "Epoch 131/1500\n",
      "Epoch 00130: val_loss did not improve\n",
      "2s - loss: 1.1046 - acc: 0.5693 - val_loss: 1.1483 - val_acc: 0.5468\n",
      "Epoch 132/1500\n",
      "Epoch 00131: val_loss did not improve\n",
      "2s - loss: 1.0989 - acc: 0.5689 - val_loss: 1.1745 - val_acc: 0.5257\n",
      "Epoch 133/1500\n",
      "Epoch 00132: val_loss did not improve\n",
      "2s - loss: 1.0983 - acc: 0.5729 - val_loss: 1.1275 - val_acc: 0.5619\n",
      "Epoch 134/1500\n",
      "Epoch 00133: val_loss did not improve\n",
      "2s - loss: 1.1092 - acc: 0.5655 - val_loss: 1.1470 - val_acc: 0.5227\n",
      "Epoch 135/1500\n",
      "Epoch 00134: val_loss did not improve\n",
      "2s - loss: 1.1149 - acc: 0.5665 - val_loss: 1.2079 - val_acc: 0.5045\n",
      "Epoch 136/1500\n",
      "Epoch 00135: val_loss did not improve\n",
      "2s - loss: 1.0810 - acc: 0.5851 - val_loss: 1.1611 - val_acc: 0.5166\n",
      "Epoch 137/1500\n",
      "Epoch 00136: val_loss did not improve\n",
      "2s - loss: 1.0958 - acc: 0.5891 - val_loss: 1.1947 - val_acc: 0.5015\n",
      "Epoch 138/1500\n",
      "Epoch 00137: val_loss did not improve\n",
      "2s - loss: 1.1054 - acc: 0.5688 - val_loss: 1.1582 - val_acc: 0.5317\n",
      "Epoch 139/1500\n",
      "Epoch 00138: val_loss did not improve\n",
      "2s - loss: 1.0855 - acc: 0.5812 - val_loss: 1.2116 - val_acc: 0.5015\n",
      "Epoch 140/1500\n",
      "Epoch 00139: val_loss improved from 1.12204 to 1.08319, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.0843 - acc: 0.5845 - val_loss: 1.0832 - val_acc: 0.5770\n",
      "Epoch 141/1500\n",
      "Epoch 00140: val_loss did not improve\n",
      "2s - loss: 1.1166 - acc: 0.5664 - val_loss: 1.1286 - val_acc: 0.5166\n",
      "Epoch 142/1500\n",
      "Epoch 00141: val_loss did not improve\n",
      "2s - loss: 1.1064 - acc: 0.5527 - val_loss: 1.1594 - val_acc: 0.5166\n",
      "Epoch 143/1500\n",
      "Epoch 00142: val_loss did not improve\n",
      "2s - loss: 1.0930 - acc: 0.5775 - val_loss: 1.1506 - val_acc: 0.5408\n",
      "Epoch 144/1500\n",
      "Epoch 00143: val_loss did not improve\n",
      "2s - loss: 1.0877 - acc: 0.5868 - val_loss: 1.1609 - val_acc: 0.5378\n",
      "Epoch 145/1500\n",
      "Epoch 00144: val_loss did not improve\n",
      "2s - loss: 1.0873 - acc: 0.5714 - val_loss: 1.2310 - val_acc: 0.5015\n",
      "Epoch 146/1500\n",
      "Epoch 00145: val_loss did not improve\n",
      "2s - loss: 1.0839 - acc: 0.5916 - val_loss: 1.1709 - val_acc: 0.5317\n",
      "Epoch 147/1500\n",
      "Epoch 00146: val_loss did not improve\n",
      "2s - loss: 1.0781 - acc: 0.5956 - val_loss: 1.1265 - val_acc: 0.5408\n",
      "Epoch 148/1500\n",
      "Epoch 00147: val_loss improved from 1.08319 to 1.07302, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.0838 - acc: 0.5836 - val_loss: 1.0730 - val_acc: 0.5861\n",
      "Epoch 149/1500\n",
      "Epoch 00148: val_loss did not improve\n",
      "2s - loss: 1.0644 - acc: 0.5780 - val_loss: 1.1724 - val_acc: 0.5559\n",
      "Epoch 150/1500\n",
      "Epoch 00149: val_loss did not improve\n",
      "2s - loss: 1.0558 - acc: 0.5957 - val_loss: 1.1359 - val_acc: 0.5166\n",
      "Epoch 151/1500\n",
      "Epoch 00150: val_loss did not improve\n",
      "2s - loss: 1.0685 - acc: 0.6020 - val_loss: 1.1462 - val_acc: 0.5227\n",
      "Epoch 152/1500\n",
      "Epoch 00151: val_loss improved from 1.07302 to 1.07286, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.1136 - acc: 0.5649 - val_loss: 1.0729 - val_acc: 0.5680\n",
      "Epoch 153/1500\n",
      "Epoch 00152: val_loss did not improve\n",
      "2s - loss: 1.0745 - acc: 0.5996 - val_loss: 1.1520 - val_acc: 0.5196\n",
      "Epoch 154/1500\n",
      "Epoch 00153: val_loss did not improve\n",
      "2s - loss: 1.0553 - acc: 0.6023 - val_loss: 1.0839 - val_acc: 0.5740\n",
      "Epoch 155/1500\n",
      "Epoch 00154: val_loss did not improve\n",
      "2s - loss: 1.0648 - acc: 0.5912 - val_loss: 1.0871 - val_acc: 0.5770\n",
      "Epoch 156/1500\n",
      "Epoch 00155: val_loss did not improve\n",
      "2s - loss: 1.0688 - acc: 0.5836 - val_loss: 1.1170 - val_acc: 0.5831\n",
      "Epoch 157/1500\n",
      "Epoch 00156: val_loss did not improve\n",
      "2s - loss: 1.0961 - acc: 0.5626 - val_loss: 1.0982 - val_acc: 0.5710\n",
      "Epoch 158/1500\n",
      "Epoch 00157: val_loss did not improve\n",
      "2s - loss: 1.0873 - acc: 0.5702 - val_loss: 1.1125 - val_acc: 0.5408\n",
      "Epoch 159/1500\n",
      "Epoch 00158: val_loss improved from 1.07286 to 1.05238, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.0483 - acc: 0.5884 - val_loss: 1.0524 - val_acc: 0.5650\n",
      "Epoch 160/1500\n",
      "Epoch 00159: val_loss did not improve\n",
      "2s - loss: 1.0704 - acc: 0.5979 - val_loss: 1.0912 - val_acc: 0.5529\n",
      "Epoch 161/1500\n",
      "Epoch 00160: val_loss did not improve\n",
      "2s - loss: 1.0114 - acc: 0.6232 - val_loss: 1.0895 - val_acc: 0.5468\n",
      "Epoch 162/1500\n",
      "Epoch 00161: val_loss improved from 1.05238 to 1.02897, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.0484 - acc: 0.5886 - val_loss: 1.0290 - val_acc: 0.6012\n",
      "Epoch 163/1500\n",
      "Epoch 00162: val_loss did not improve\n",
      "2s - loss: 0.9881 - acc: 0.6260 - val_loss: 1.1189 - val_acc: 0.5287\n",
      "Epoch 164/1500\n",
      "Epoch 00163: val_loss improved from 1.02897 to 0.98951, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 1.0529 - acc: 0.5874 - val_loss: 0.9895 - val_acc: 0.5891\n",
      "Epoch 165/1500\n",
      "Epoch 00164: val_loss did not improve\n",
      "2s - loss: 1.0264 - acc: 0.6148 - val_loss: 1.0944 - val_acc: 0.5498\n",
      "Epoch 166/1500\n",
      "Epoch 00165: val_loss did not improve\n",
      "2s - loss: 1.0538 - acc: 0.5881 - val_loss: 1.0199 - val_acc: 0.5982\n",
      "Epoch 167/1500\n",
      "Epoch 00166: val_loss did not improve\n",
      "2s - loss: 1.0648 - acc: 0.6015 - val_loss: 1.0783 - val_acc: 0.5921\n",
      "Epoch 168/1500\n",
      "Epoch 00167: val_loss did not improve\n",
      "2s - loss: 1.0486 - acc: 0.6005 - val_loss: 1.0636 - val_acc: 0.5770\n",
      "Epoch 169/1500\n",
      "Epoch 00168: val_loss did not improve\n",
      "2s - loss: 0.9806 - acc: 0.6182 - val_loss: 1.1012 - val_acc: 0.5498\n",
      "Epoch 170/1500\n",
      "Epoch 00169: val_loss did not improve\n",
      "2s - loss: 1.0018 - acc: 0.6244 - val_loss: 1.1359 - val_acc: 0.5287\n",
      "Epoch 171/1500\n",
      "Epoch 00170: val_loss did not improve\n",
      "2s - loss: 1.0150 - acc: 0.6030 - val_loss: 1.0282 - val_acc: 0.5801\n",
      "Epoch 172/1500\n",
      "Epoch 00171: val_loss did not improve\n",
      "2s - loss: 1.0696 - acc: 0.5915 - val_loss: 1.1113 - val_acc: 0.5498\n",
      "Epoch 173/1500\n",
      "Epoch 00172: val_loss did not improve\n",
      "2s - loss: 1.0214 - acc: 0.5972 - val_loss: 1.0290 - val_acc: 0.5710\n",
      "Epoch 174/1500\n",
      "Epoch 00173: val_loss did not improve\n",
      "2s - loss: 0.9855 - acc: 0.6260 - val_loss: 0.9981 - val_acc: 0.5861\n",
      "Epoch 175/1500\n",
      "Epoch 00174: val_loss did not improve\n",
      "2s - loss: 1.0521 - acc: 0.5847 - val_loss: 1.0944 - val_acc: 0.5408\n",
      "Epoch 176/1500\n",
      "Epoch 00175: val_loss did not improve\n",
      "2s - loss: 1.0439 - acc: 0.5915 - val_loss: 1.0297 - val_acc: 0.5982\n",
      "Epoch 177/1500\n",
      "Epoch 00176: val_loss did not improve\n",
      "2s - loss: 0.9789 - acc: 0.6323 - val_loss: 1.0481 - val_acc: 0.5650\n",
      "Epoch 178/1500\n",
      "Epoch 00177: val_loss did not improve\n",
      "2s - loss: 1.0422 - acc: 0.6068 - val_loss: 1.0202 - val_acc: 0.5982\n",
      "Epoch 179/1500\n",
      "Epoch 00178: val_loss did not improve\n",
      "2s - loss: 0.9868 - acc: 0.6201 - val_loss: 1.0449 - val_acc: 0.5861\n",
      "Epoch 180/1500\n",
      "Epoch 00179: val_loss did not improve\n",
      "2s - loss: 0.9738 - acc: 0.6136 - val_loss: 1.0251 - val_acc: 0.5801\n",
      "Epoch 181/1500\n",
      "Epoch 00180: val_loss did not improve\n",
      "2s - loss: 0.9889 - acc: 0.6260 - val_loss: 1.0769 - val_acc: 0.5982\n",
      "Epoch 182/1500\n",
      "Epoch 00181: val_loss did not improve\n",
      "2s - loss: 1.0121 - acc: 0.6159 - val_loss: 1.0334 - val_acc: 0.6103\n",
      "Epoch 183/1500\n",
      "Epoch 00182: val_loss did not improve\n",
      "2s - loss: 1.0311 - acc: 0.6062 - val_loss: 1.0332 - val_acc: 0.5770\n",
      "Epoch 184/1500\n",
      "Epoch 00183: val_loss did not improve\n",
      "2s - loss: 0.9841 - acc: 0.6331 - val_loss: 1.0377 - val_acc: 0.5921\n",
      "Epoch 185/1500\n",
      "Epoch 00184: val_loss did not improve\n",
      "2s - loss: 1.0328 - acc: 0.6056 - val_loss: 1.0223 - val_acc: 0.5831\n",
      "Epoch 186/1500\n",
      "Epoch 00185: val_loss improved from 0.98951 to 0.96563, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.9950 - acc: 0.6269 - val_loss: 0.9656 - val_acc: 0.6073\n",
      "Epoch 187/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00186: val_loss did not improve\n",
      "2s - loss: 0.9914 - acc: 0.6354 - val_loss: 1.0404 - val_acc: 0.5921\n",
      "Epoch 188/1500\n",
      "Epoch 00187: val_loss did not improve\n",
      "2s - loss: 1.0019 - acc: 0.6110 - val_loss: 1.0707 - val_acc: 0.5710\n",
      "Epoch 189/1500\n",
      "Epoch 00188: val_loss did not improve\n",
      "2s - loss: 0.9817 - acc: 0.6174 - val_loss: 1.0363 - val_acc: 0.5921\n",
      "Epoch 190/1500\n",
      "Epoch 00189: val_loss did not improve\n",
      "2s - loss: 0.9711 - acc: 0.6231 - val_loss: 1.0300 - val_acc: 0.5680\n",
      "Epoch 191/1500\n",
      "Epoch 00190: val_loss did not improve\n",
      "2s - loss: 0.9773 - acc: 0.6325 - val_loss: 0.9801 - val_acc: 0.6073\n",
      "Epoch 192/1500\n",
      "Epoch 00191: val_loss did not improve\n",
      "2s - loss: 0.9561 - acc: 0.6421 - val_loss: 0.9971 - val_acc: 0.6042\n",
      "Epoch 193/1500\n",
      "Epoch 00192: val_loss did not improve\n",
      "2s - loss: 0.9985 - acc: 0.6372 - val_loss: 0.9998 - val_acc: 0.5831\n",
      "Epoch 194/1500\n",
      "Epoch 00193: val_loss did not improve\n",
      "2s - loss: 0.9889 - acc: 0.6185 - val_loss: 1.0070 - val_acc: 0.5891\n",
      "Epoch 195/1500\n",
      "Epoch 00194: val_loss did not improve\n",
      "2s - loss: 0.9765 - acc: 0.6080 - val_loss: 1.0046 - val_acc: 0.5952\n",
      "Epoch 196/1500\n",
      "Epoch 00195: val_loss did not improve\n",
      "2s - loss: 0.9645 - acc: 0.6311 - val_loss: 1.0306 - val_acc: 0.5740\n",
      "Epoch 197/1500\n",
      "Epoch 00196: val_loss did not improve\n",
      "2s - loss: 0.9856 - acc: 0.6276 - val_loss: 1.0078 - val_acc: 0.5559\n",
      "Epoch 198/1500\n",
      "Epoch 00197: val_loss did not improve\n",
      "2s - loss: 0.9934 - acc: 0.6254 - val_loss: 1.0191 - val_acc: 0.5650\n",
      "Epoch 199/1500\n",
      "Epoch 00198: val_loss did not improve\n",
      "2s - loss: 0.9779 - acc: 0.6326 - val_loss: 0.9685 - val_acc: 0.6224\n",
      "Epoch 200/1500\n",
      "Epoch 00199: val_loss did not improve\n",
      "2s - loss: 0.9385 - acc: 0.6533 - val_loss: 1.0119 - val_acc: 0.5891\n",
      "Epoch 201/1500\n",
      "Epoch 00200: val_loss did not improve\n",
      "2s - loss: 0.9475 - acc: 0.6517 - val_loss: 0.9972 - val_acc: 0.5801\n",
      "Epoch 202/1500\n",
      "Epoch 00201: val_loss did not improve\n",
      "2s - loss: 0.9771 - acc: 0.6205 - val_loss: 0.9987 - val_acc: 0.5861\n",
      "Epoch 203/1500\n",
      "Epoch 00202: val_loss did not improve\n",
      "2s - loss: 0.9376 - acc: 0.6491 - val_loss: 1.0000 - val_acc: 0.5891\n",
      "Epoch 204/1500\n",
      "Epoch 00203: val_loss did not improve\n",
      "2s - loss: 0.9160 - acc: 0.6527 - val_loss: 0.9827 - val_acc: 0.5740\n",
      "Epoch 205/1500\n",
      "Epoch 00204: val_loss improved from 0.96563 to 0.94649, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.9675 - acc: 0.6374 - val_loss: 0.9465 - val_acc: 0.6254\n",
      "Epoch 206/1500\n",
      "Epoch 00205: val_loss improved from 0.94649 to 0.92638, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.9341 - acc: 0.6496 - val_loss: 0.9264 - val_acc: 0.6284\n",
      "Epoch 207/1500\n",
      "Epoch 00206: val_loss did not improve\n",
      "2s - loss: 0.9576 - acc: 0.6467 - val_loss: 0.9734 - val_acc: 0.5982\n",
      "Epoch 208/1500\n",
      "Epoch 00207: val_loss did not improve\n",
      "2s - loss: 0.9518 - acc: 0.6375 - val_loss: 0.9409 - val_acc: 0.6163\n",
      "Epoch 209/1500\n",
      "Epoch 00208: val_loss did not improve\n",
      "2s - loss: 0.9578 - acc: 0.6459 - val_loss: 0.9934 - val_acc: 0.6284\n",
      "Epoch 210/1500\n",
      "Epoch 00209: val_loss did not improve\n",
      "2s - loss: 0.9401 - acc: 0.6443 - val_loss: 0.9406 - val_acc: 0.6375\n",
      "Epoch 211/1500\n",
      "Epoch 00210: val_loss did not improve\n",
      "2s - loss: 0.9248 - acc: 0.6415 - val_loss: 0.9426 - val_acc: 0.6224\n",
      "Epoch 212/1500\n",
      "Epoch 00211: val_loss did not improve\n",
      "2s - loss: 0.9638 - acc: 0.6548 - val_loss: 0.9799 - val_acc: 0.5982\n",
      "Epoch 213/1500\n",
      "Epoch 00212: val_loss did not improve\n",
      "2s - loss: 0.9278 - acc: 0.6335 - val_loss: 0.9508 - val_acc: 0.6103\n",
      "Epoch 214/1500\n",
      "Epoch 00213: val_loss did not improve\n",
      "2s - loss: 0.9627 - acc: 0.6551 - val_loss: 0.9878 - val_acc: 0.6375\n",
      "Epoch 215/1500\n",
      "Epoch 00214: val_loss did not improve\n",
      "2s - loss: 0.9012 - acc: 0.6513 - val_loss: 0.9412 - val_acc: 0.6254\n",
      "Epoch 216/1500\n",
      "Epoch 00215: val_loss did not improve\n",
      "2s - loss: 0.9466 - acc: 0.6439 - val_loss: 0.9397 - val_acc: 0.6193\n",
      "Epoch 217/1500\n",
      "Epoch 00216: val_loss did not improve\n",
      "2s - loss: 0.9478 - acc: 0.6464 - val_loss: 0.9990 - val_acc: 0.5982\n",
      "Epoch 218/1500\n",
      "Epoch 00217: val_loss did not improve\n",
      "2s - loss: 0.9294 - acc: 0.6420 - val_loss: 0.9611 - val_acc: 0.6224\n",
      "Epoch 219/1500\n",
      "Epoch 00218: val_loss did not improve\n",
      "2s - loss: 0.9307 - acc: 0.6437 - val_loss: 0.9401 - val_acc: 0.6103\n",
      "Epoch 220/1500\n",
      "Epoch 00219: val_loss improved from 0.92638 to 0.89208, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.9513 - acc: 0.6382 - val_loss: 0.8921 - val_acc: 0.6193\n",
      "Epoch 221/1500\n",
      "Epoch 00220: val_loss improved from 0.89208 to 0.89140, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.9119 - acc: 0.6544 - val_loss: 0.8914 - val_acc: 0.6586\n",
      "Epoch 222/1500\n",
      "Epoch 00221: val_loss did not improve\n",
      "2s - loss: 0.9301 - acc: 0.6344 - val_loss: 0.9024 - val_acc: 0.6405\n",
      "Epoch 223/1500\n",
      "Epoch 00222: val_loss did not improve\n",
      "2s - loss: 0.9256 - acc: 0.6412 - val_loss: 0.9092 - val_acc: 0.6405\n",
      "Epoch 224/1500\n",
      "Epoch 00223: val_loss did not improve\n",
      "2s - loss: 0.9111 - acc: 0.6470 - val_loss: 0.8990 - val_acc: 0.6647\n",
      "Epoch 225/1500\n",
      "Epoch 00224: val_loss improved from 0.89140 to 0.87095, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.9277 - acc: 0.6556 - val_loss: 0.8710 - val_acc: 0.6737\n",
      "Epoch 226/1500\n",
      "Epoch 00225: val_loss did not improve\n",
      "2s - loss: 0.9011 - acc: 0.6630 - val_loss: 0.8954 - val_acc: 0.6616\n",
      "Epoch 227/1500\n",
      "Epoch 00226: val_loss did not improve\n",
      "2s - loss: 0.9202 - acc: 0.6551 - val_loss: 0.9766 - val_acc: 0.6193\n",
      "Epoch 228/1500\n",
      "Epoch 00227: val_loss did not improve\n",
      "2s - loss: 0.9013 - acc: 0.6438 - val_loss: 0.9286 - val_acc: 0.6314\n",
      "Epoch 229/1500\n",
      "Epoch 00228: val_loss improved from 0.87095 to 0.86152, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.8800 - acc: 0.6714 - val_loss: 0.8615 - val_acc: 0.6586\n",
      "Epoch 230/1500\n",
      "Epoch 00229: val_loss did not improve\n",
      "2s - loss: 0.8996 - acc: 0.6580 - val_loss: 0.8841 - val_acc: 0.6465\n",
      "Epoch 231/1500\n",
      "Epoch 00230: val_loss improved from 0.86152 to 0.84622, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.9029 - acc: 0.6557 - val_loss: 0.8462 - val_acc: 0.6828\n",
      "Epoch 232/1500\n",
      "Epoch 00231: val_loss did not improve\n",
      "2s - loss: 0.9100 - acc: 0.6724 - val_loss: 0.8908 - val_acc: 0.6435\n",
      "Epoch 233/1500\n",
      "Epoch 00232: val_loss did not improve\n",
      "2s - loss: 0.9423 - acc: 0.6294 - val_loss: 0.8818 - val_acc: 0.6556\n",
      "Epoch 234/1500\n",
      "Epoch 00233: val_loss did not improve\n",
      "2s - loss: 0.9146 - acc: 0.6509 - val_loss: 0.8811 - val_acc: 0.6556\n",
      "Epoch 235/1500\n",
      "Epoch 00234: val_loss did not improve\n",
      "2s - loss: 0.9060 - acc: 0.6549 - val_loss: 0.8706 - val_acc: 0.6737\n",
      "Epoch 236/1500\n",
      "Epoch 00235: val_loss did not improve\n",
      "2s - loss: 0.8578 - acc: 0.6808 - val_loss: 0.9175 - val_acc: 0.6435\n",
      "Epoch 237/1500\n",
      "Epoch 00236: val_loss did not improve\n",
      "2s - loss: 0.8858 - acc: 0.6588 - val_loss: 0.8676 - val_acc: 0.6737\n",
      "Epoch 238/1500\n",
      "Epoch 00237: val_loss did not improve\n",
      "2s - loss: 0.9010 - acc: 0.6684 - val_loss: 0.8867 - val_acc: 0.6737\n",
      "Epoch 239/1500\n",
      "Epoch 00238: val_loss did not improve\n",
      "2s - loss: 0.8692 - acc: 0.6710 - val_loss: 0.8884 - val_acc: 0.6405\n",
      "Epoch 240/1500\n",
      "Epoch 00239: val_loss did not improve\n",
      "2s - loss: 0.8861 - acc: 0.6529 - val_loss: 0.8890 - val_acc: 0.6586\n",
      "Epoch 241/1500\n",
      "Epoch 00240: val_loss did not improve\n",
      "2s - loss: 0.8808 - acc: 0.6599 - val_loss: 0.8642 - val_acc: 0.6435\n",
      "Epoch 242/1500\n",
      "Epoch 00241: val_loss did not improve\n",
      "2s - loss: 0.8905 - acc: 0.6575 - val_loss: 0.9109 - val_acc: 0.6616\n",
      "Epoch 243/1500\n",
      "Epoch 00242: val_loss improved from 0.84622 to 0.83947, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.8581 - acc: 0.6853 - val_loss: 0.8395 - val_acc: 0.6828\n",
      "Epoch 244/1500\n",
      "Epoch 00243: val_loss did not improve\n",
      "2s - loss: 0.8145 - acc: 0.7036 - val_loss: 0.8649 - val_acc: 0.6647\n",
      "Epoch 245/1500\n",
      "Epoch 00244: val_loss did not improve\n",
      "2s - loss: 0.8556 - acc: 0.6947 - val_loss: 0.8639 - val_acc: 0.6405\n",
      "Epoch 246/1500\n",
      "Epoch 00245: val_loss did not improve\n",
      "2s - loss: 0.8984 - acc: 0.6569 - val_loss: 0.8707 - val_acc: 0.6647\n",
      "Epoch 247/1500\n",
      "Epoch 00246: val_loss did not improve\n",
      "2s - loss: 0.8620 - acc: 0.6838 - val_loss: 0.9278 - val_acc: 0.6314\n",
      "Epoch 248/1500\n",
      "Epoch 00247: val_loss improved from 0.83947 to 0.82631, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.8760 - acc: 0.6781 - val_loss: 0.8263 - val_acc: 0.6888\n",
      "Epoch 249/1500\n",
      "Epoch 00248: val_loss did not improve\n",
      "2s - loss: 0.8817 - acc: 0.6728 - val_loss: 0.8800 - val_acc: 0.6798\n",
      "Epoch 250/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00249: val_loss did not improve\n",
      "2s - loss: 0.8701 - acc: 0.6569 - val_loss: 0.8573 - val_acc: 0.6798\n",
      "Epoch 251/1500\n",
      "Epoch 00250: val_loss did not improve\n",
      "2s - loss: 0.8596 - acc: 0.6789 - val_loss: 0.8991 - val_acc: 0.6405\n",
      "Epoch 252/1500\n",
      "Epoch 00251: val_loss did not improve\n",
      "2s - loss: 0.8569 - acc: 0.6744 - val_loss: 0.8576 - val_acc: 0.6495\n",
      "Epoch 253/1500\n",
      "Epoch 00252: val_loss did not improve\n",
      "2s - loss: 0.8477 - acc: 0.6837 - val_loss: 0.8383 - val_acc: 0.6767\n",
      "Epoch 254/1500\n",
      "Epoch 00253: val_loss did not improve\n",
      "2s - loss: 0.8171 - acc: 0.6918 - val_loss: 0.8667 - val_acc: 0.6767\n",
      "Epoch 255/1500\n",
      "Epoch 00254: val_loss improved from 0.82631 to 0.82106, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.8895 - acc: 0.6533 - val_loss: 0.8211 - val_acc: 0.6918\n",
      "Epoch 256/1500\n",
      "Epoch 00255: val_loss did not improve\n",
      "2s - loss: 0.8617 - acc: 0.6766 - val_loss: 0.8950 - val_acc: 0.6556\n",
      "Epoch 257/1500\n",
      "Epoch 00256: val_loss did not improve\n",
      "2s - loss: 0.8391 - acc: 0.6751 - val_loss: 0.8557 - val_acc: 0.6707\n",
      "Epoch 258/1500\n",
      "Epoch 00257: val_loss did not improve\n",
      "2s - loss: 0.8713 - acc: 0.6610 - val_loss: 0.8473 - val_acc: 0.6737\n",
      "Epoch 259/1500\n",
      "Epoch 00258: val_loss did not improve\n",
      "2s - loss: 0.8042 - acc: 0.6830 - val_loss: 0.8998 - val_acc: 0.6193\n",
      "Epoch 260/1500\n",
      "Epoch 00259: val_loss did not improve\n",
      "2s - loss: 0.9134 - acc: 0.6777 - val_loss: 0.8629 - val_acc: 0.6737\n",
      "Epoch 261/1500\n",
      "Epoch 00260: val_loss did not improve\n",
      "2s - loss: 0.8447 - acc: 0.6806 - val_loss: 0.9018 - val_acc: 0.6616\n",
      "Epoch 262/1500\n",
      "Epoch 00261: val_loss did not improve\n",
      "2s - loss: 0.8484 - acc: 0.6798 - val_loss: 0.8292 - val_acc: 0.6858\n",
      "Epoch 263/1500\n",
      "Epoch 00262: val_loss improved from 0.82106 to 0.79744, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.8831 - acc: 0.6560 - val_loss: 0.7974 - val_acc: 0.7009\n",
      "Epoch 264/1500\n",
      "Epoch 00263: val_loss did not improve\n",
      "2s - loss: 0.8674 - acc: 0.6765 - val_loss: 0.8461 - val_acc: 0.6979\n",
      "Epoch 265/1500\n",
      "Epoch 00264: val_loss improved from 0.79744 to 0.77529, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.8587 - acc: 0.6769 - val_loss: 0.7753 - val_acc: 0.7160\n",
      "Epoch 266/1500\n",
      "Epoch 00265: val_loss did not improve\n",
      "2s - loss: 0.8667 - acc: 0.6957 - val_loss: 0.8050 - val_acc: 0.7160\n",
      "Epoch 267/1500\n",
      "Epoch 00266: val_loss did not improve\n",
      "2s - loss: 0.7952 - acc: 0.6935 - val_loss: 0.8575 - val_acc: 0.6647\n",
      "Epoch 268/1500\n",
      "Epoch 00267: val_loss did not improve\n",
      "2s - loss: 0.8242 - acc: 0.7109 - val_loss: 0.8005 - val_acc: 0.6888\n",
      "Epoch 269/1500\n",
      "Epoch 00268: val_loss did not improve\n",
      "2s - loss: 0.8113 - acc: 0.6901 - val_loss: 0.8424 - val_acc: 0.6707\n",
      "Epoch 270/1500\n",
      "Epoch 00269: val_loss did not improve\n",
      "2s - loss: 0.8088 - acc: 0.6999 - val_loss: 0.8197 - val_acc: 0.6647\n",
      "Epoch 271/1500\n",
      "Epoch 00270: val_loss did not improve\n",
      "2s - loss: 0.8225 - acc: 0.6949 - val_loss: 0.8027 - val_acc: 0.7160\n",
      "Epoch 272/1500\n",
      "Epoch 00271: val_loss did not improve\n",
      "2s - loss: 0.8403 - acc: 0.6840 - val_loss: 0.8115 - val_acc: 0.6798\n",
      "Epoch 273/1500\n",
      "Epoch 00272: val_loss improved from 0.77529 to 0.77529, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.7856 - acc: 0.6953 - val_loss: 0.7753 - val_acc: 0.7069\n",
      "Epoch 274/1500\n",
      "Epoch 00273: val_loss did not improve\n",
      "2s - loss: 0.8361 - acc: 0.6750 - val_loss: 0.8129 - val_acc: 0.6767\n",
      "Epoch 275/1500\n",
      "Epoch 00274: val_loss did not improve\n",
      "2s - loss: 0.7927 - acc: 0.6909 - val_loss: 0.8684 - val_acc: 0.6465\n",
      "Epoch 276/1500\n",
      "Epoch 00275: val_loss improved from 0.77529 to 0.74060, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.8103 - acc: 0.7037 - val_loss: 0.7406 - val_acc: 0.7281\n",
      "Epoch 277/1500\n",
      "Epoch 00276: val_loss did not improve\n",
      "2s - loss: 0.8078 - acc: 0.6847 - val_loss: 0.7958 - val_acc: 0.7130\n",
      "Epoch 278/1500\n",
      "Epoch 00277: val_loss did not improve\n",
      "2s - loss: 0.8033 - acc: 0.7001 - val_loss: 0.7982 - val_acc: 0.7069\n",
      "Epoch 279/1500\n",
      "Epoch 00278: val_loss did not improve\n",
      "2s - loss: 0.8346 - acc: 0.6953 - val_loss: 0.8030 - val_acc: 0.7100\n",
      "Epoch 280/1500\n",
      "Epoch 00279: val_loss did not improve\n",
      "2s - loss: 0.7880 - acc: 0.7108 - val_loss: 0.7999 - val_acc: 0.7009\n",
      "Epoch 281/1500\n",
      "Epoch 00280: val_loss did not improve\n",
      "2s - loss: 0.8153 - acc: 0.6839 - val_loss: 0.8110 - val_acc: 0.7251\n",
      "Epoch 282/1500\n",
      "Epoch 00281: val_loss did not improve\n",
      "2s - loss: 0.8167 - acc: 0.6895 - val_loss: 0.8153 - val_acc: 0.6677\n",
      "Epoch 283/1500\n",
      "Epoch 00282: val_loss did not improve\n",
      "2s - loss: 0.7845 - acc: 0.6901 - val_loss: 0.8008 - val_acc: 0.6979\n",
      "Epoch 284/1500\n",
      "Epoch 00283: val_loss did not improve\n",
      "2s - loss: 0.7798 - acc: 0.6898 - val_loss: 0.7622 - val_acc: 0.6918\n",
      "Epoch 285/1500\n",
      "Epoch 00284: val_loss did not improve\n",
      "2s - loss: 0.7450 - acc: 0.7270 - val_loss: 0.8556 - val_acc: 0.6918\n",
      "Epoch 286/1500\n",
      "Epoch 00285: val_loss did not improve\n",
      "2s - loss: 0.8233 - acc: 0.6974 - val_loss: 0.7713 - val_acc: 0.7341\n",
      "Epoch 287/1500\n",
      "Epoch 00286: val_loss did not improve\n",
      "2s - loss: 0.8142 - acc: 0.6899 - val_loss: 0.7622 - val_acc: 0.7069\n",
      "Epoch 288/1500\n",
      "Epoch 00287: val_loss did not improve\n",
      "2s - loss: 0.8479 - acc: 0.6825 - val_loss: 0.7503 - val_acc: 0.7311\n",
      "Epoch 289/1500\n",
      "Epoch 00288: val_loss did not improve\n",
      "2s - loss: 0.8170 - acc: 0.7029 - val_loss: 0.7864 - val_acc: 0.7069\n",
      "Epoch 290/1500\n",
      "Epoch 00289: val_loss did not improve\n",
      "2s - loss: 0.7930 - acc: 0.7198 - val_loss: 0.7616 - val_acc: 0.6949\n",
      "Epoch 291/1500\n",
      "Epoch 00290: val_loss did not improve\n",
      "2s - loss: 0.7775 - acc: 0.7095 - val_loss: 0.8605 - val_acc: 0.6737\n",
      "Epoch 292/1500\n",
      "Epoch 00291: val_loss did not improve\n",
      "2s - loss: 0.7643 - acc: 0.7119 - val_loss: 0.8522 - val_acc: 0.6707\n",
      "Epoch 293/1500\n",
      "Epoch 00292: val_loss improved from 0.74060 to 0.70289, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.8108 - acc: 0.6885 - val_loss: 0.7029 - val_acc: 0.7251\n",
      "Epoch 294/1500\n",
      "Epoch 00293: val_loss did not improve\n",
      "2s - loss: 0.7841 - acc: 0.7056 - val_loss: 0.7665 - val_acc: 0.7341\n",
      "Epoch 295/1500\n",
      "Epoch 00294: val_loss did not improve\n",
      "2s - loss: 0.7997 - acc: 0.6983 - val_loss: 0.7903 - val_acc: 0.7160\n",
      "Epoch 296/1500\n",
      "Epoch 00295: val_loss did not improve\n",
      "2s - loss: 0.7773 - acc: 0.7048 - val_loss: 0.7742 - val_acc: 0.7251\n",
      "Epoch 297/1500\n",
      "Epoch 00296: val_loss did not improve\n",
      "2s - loss: 0.7555 - acc: 0.7255 - val_loss: 0.7796 - val_acc: 0.6949\n",
      "Epoch 298/1500\n",
      "Epoch 00297: val_loss did not improve\n",
      "2s - loss: 0.7952 - acc: 0.7103 - val_loss: 0.7801 - val_acc: 0.7130\n",
      "Epoch 299/1500\n",
      "Epoch 00298: val_loss did not improve\n",
      "2s - loss: 0.8168 - acc: 0.6848 - val_loss: 0.8861 - val_acc: 0.6767\n",
      "Epoch 300/1500\n",
      "Epoch 00299: val_loss did not improve\n",
      "2s - loss: 0.7737 - acc: 0.7126 - val_loss: 0.7627 - val_acc: 0.7341\n",
      "Epoch 301/1500\n",
      "Epoch 00300: val_loss did not improve\n",
      "2s - loss: 0.8079 - acc: 0.6974 - val_loss: 0.7982 - val_acc: 0.7160\n",
      "Epoch 302/1500\n",
      "Epoch 00301: val_loss did not improve\n",
      "2s - loss: 0.7833 - acc: 0.7270 - val_loss: 0.7725 - val_acc: 0.6979\n",
      "Epoch 303/1500\n",
      "Epoch 00302: val_loss did not improve\n",
      "2s - loss: 0.7643 - acc: 0.7206 - val_loss: 0.7392 - val_acc: 0.7644\n",
      "Epoch 304/1500\n",
      "Epoch 00303: val_loss improved from 0.70289 to 0.69254, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.7896 - acc: 0.7185 - val_loss: 0.6925 - val_acc: 0.7704\n",
      "Epoch 305/1500\n",
      "Epoch 00304: val_loss did not improve\n",
      "2s - loss: 0.7881 - acc: 0.7004 - val_loss: 0.7035 - val_acc: 0.7281\n",
      "Epoch 306/1500\n",
      "Epoch 00305: val_loss did not improve\n",
      "2s - loss: 0.7576 - acc: 0.7183 - val_loss: 0.8365 - val_acc: 0.6616\n",
      "Epoch 307/1500\n",
      "Epoch 00306: val_loss did not improve\n",
      "2s - loss: 0.7547 - acc: 0.7215 - val_loss: 0.8213 - val_acc: 0.6828\n",
      "Epoch 308/1500\n",
      "Epoch 00307: val_loss did not improve\n",
      "2s - loss: 0.7627 - acc: 0.7146 - val_loss: 0.6935 - val_acc: 0.7553\n",
      "Epoch 309/1500\n",
      "Epoch 00308: val_loss did not improve\n",
      "2s - loss: 0.7803 - acc: 0.7014 - val_loss: 0.7475 - val_acc: 0.7492\n",
      "Epoch 310/1500\n",
      "Epoch 00309: val_loss did not improve\n",
      "2s - loss: 0.7648 - acc: 0.7148 - val_loss: 0.7192 - val_acc: 0.7341\n",
      "Epoch 311/1500\n",
      "Epoch 00310: val_loss improved from 0.69254 to 0.66759, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.7291 - acc: 0.7270 - val_loss: 0.6676 - val_acc: 0.7674\n",
      "Epoch 312/1500\n",
      "Epoch 00311: val_loss improved from 0.66759 to 0.65667, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.7717 - acc: 0.7046 - val_loss: 0.6567 - val_acc: 0.7523\n",
      "Epoch 313/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00312: val_loss did not improve\n",
      "2s - loss: 0.7715 - acc: 0.7128 - val_loss: 0.7367 - val_acc: 0.7311\n",
      "Epoch 314/1500\n",
      "Epoch 00313: val_loss did not improve\n",
      "2s - loss: 0.7793 - acc: 0.7214 - val_loss: 0.6811 - val_acc: 0.7553\n",
      "Epoch 315/1500\n",
      "Epoch 00314: val_loss did not improve\n",
      "2s - loss: 0.7245 - acc: 0.7245 - val_loss: 0.7335 - val_acc: 0.7251\n",
      "Epoch 316/1500\n",
      "Epoch 00315: val_loss did not improve\n",
      "2s - loss: 0.7534 - acc: 0.7201 - val_loss: 0.7312 - val_acc: 0.7372\n",
      "Epoch 317/1500\n",
      "Epoch 00316: val_loss did not improve\n",
      "2s - loss: 0.7705 - acc: 0.7002 - val_loss: 0.7065 - val_acc: 0.7583\n",
      "Epoch 318/1500\n",
      "Epoch 00317: val_loss did not improve\n",
      "2s - loss: 0.7550 - acc: 0.7151 - val_loss: 0.7031 - val_acc: 0.7372\n",
      "Epoch 319/1500\n",
      "Epoch 00318: val_loss did not improve\n",
      "2s - loss: 0.7426 - acc: 0.7270 - val_loss: 0.7208 - val_acc: 0.7311\n",
      "Epoch 320/1500\n",
      "Epoch 00319: val_loss did not improve\n",
      "2s - loss: 0.7431 - acc: 0.7182 - val_loss: 0.7380 - val_acc: 0.7251\n",
      "Epoch 321/1500\n",
      "Epoch 00320: val_loss did not improve\n",
      "2s - loss: 0.7536 - acc: 0.7231 - val_loss: 0.7476 - val_acc: 0.7221\n",
      "Epoch 322/1500\n",
      "Epoch 00321: val_loss did not improve\n",
      "2s - loss: 0.7265 - acc: 0.7333 - val_loss: 0.7955 - val_acc: 0.7039\n",
      "Epoch 323/1500\n",
      "Epoch 00322: val_loss did not improve\n",
      "2s - loss: 0.7271 - acc: 0.7247 - val_loss: 0.7807 - val_acc: 0.7251\n",
      "Epoch 324/1500\n",
      "Epoch 00323: val_loss did not improve\n",
      "2s - loss: 0.7863 - acc: 0.7161 - val_loss: 0.7653 - val_acc: 0.7160\n",
      "Epoch 325/1500\n",
      "Epoch 00324: val_loss did not improve\n",
      "2s - loss: 0.7467 - acc: 0.7131 - val_loss: 0.7515 - val_acc: 0.7432\n",
      "Epoch 326/1500\n",
      "Epoch 00325: val_loss did not improve\n",
      "2s - loss: 0.7201 - acc: 0.7153 - val_loss: 0.7072 - val_acc: 0.7341\n",
      "Epoch 327/1500\n",
      "Epoch 00326: val_loss did not improve\n",
      "2s - loss: 0.8112 - acc: 0.6890 - val_loss: 0.7324 - val_acc: 0.7341\n",
      "Epoch 328/1500\n",
      "Epoch 00327: val_loss did not improve\n",
      "2s - loss: 0.6962 - acc: 0.7445 - val_loss: 0.7464 - val_acc: 0.6979\n",
      "Epoch 329/1500\n",
      "Epoch 00328: val_loss improved from 0.65667 to 0.65047, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.7145 - acc: 0.7319 - val_loss: 0.6505 - val_acc: 0.7402\n",
      "Epoch 330/1500\n",
      "Epoch 00329: val_loss did not improve\n",
      "2s - loss: 0.7463 - acc: 0.7190 - val_loss: 0.7019 - val_acc: 0.7644\n",
      "Epoch 331/1500\n",
      "Epoch 00330: val_loss did not improve\n",
      "2s - loss: 0.7157 - acc: 0.7256 - val_loss: 0.6856 - val_acc: 0.7402\n",
      "Epoch 332/1500\n",
      "Epoch 00331: val_loss did not improve\n",
      "2s - loss: 0.7075 - acc: 0.7327 - val_loss: 0.7266 - val_acc: 0.7130\n",
      "Epoch 333/1500\n",
      "Epoch 00332: val_loss did not improve\n",
      "2s - loss: 0.7167 - acc: 0.7317 - val_loss: 0.7572 - val_acc: 0.7341\n",
      "Epoch 334/1500\n",
      "Epoch 00333: val_loss improved from 0.65047 to 0.63873, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.7167 - acc: 0.7396 - val_loss: 0.6387 - val_acc: 0.7644\n",
      "Epoch 335/1500\n",
      "Epoch 00334: val_loss did not improve\n",
      "2s - loss: 0.7125 - acc: 0.7255 - val_loss: 0.6760 - val_acc: 0.7764\n",
      "Epoch 336/1500\n",
      "Epoch 00335: val_loss did not improve\n",
      "2s - loss: 0.7265 - acc: 0.7230 - val_loss: 0.6550 - val_acc: 0.7583\n",
      "Epoch 337/1500\n",
      "Epoch 00336: val_loss did not improve\n",
      "2s - loss: 0.7065 - acc: 0.7326 - val_loss: 0.6708 - val_acc: 0.7492\n",
      "Epoch 338/1500\n",
      "Epoch 00337: val_loss did not improve\n",
      "2s - loss: 0.7355 - acc: 0.7252 - val_loss: 0.6964 - val_acc: 0.7492\n",
      "Epoch 339/1500\n",
      "Epoch 00338: val_loss did not improve\n",
      "2s - loss: 0.7077 - acc: 0.7351 - val_loss: 0.7445 - val_acc: 0.7221\n",
      "Epoch 340/1500\n",
      "Epoch 00339: val_loss did not improve\n",
      "2s - loss: 0.6839 - acc: 0.7598 - val_loss: 0.7115 - val_acc: 0.7190\n",
      "Epoch 341/1500\n",
      "Epoch 00340: val_loss did not improve\n",
      "2s - loss: 0.6922 - acc: 0.7276 - val_loss: 0.6693 - val_acc: 0.7553\n",
      "Epoch 342/1500\n",
      "Epoch 00341: val_loss improved from 0.63873 to 0.61786, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.6985 - acc: 0.7423 - val_loss: 0.6179 - val_acc: 0.7855\n",
      "Epoch 343/1500\n",
      "Epoch 00342: val_loss did not improve\n",
      "2s - loss: 0.7056 - acc: 0.7219 - val_loss: 0.6630 - val_acc: 0.7583\n",
      "Epoch 344/1500\n",
      "Epoch 00343: val_loss did not improve\n",
      "2s - loss: 0.7225 - acc: 0.7222 - val_loss: 0.6487 - val_acc: 0.7734\n",
      "Epoch 345/1500\n",
      "Epoch 00344: val_loss did not improve\n",
      "2s - loss: 0.6725 - acc: 0.7535 - val_loss: 0.6448 - val_acc: 0.7644\n",
      "Epoch 346/1500\n",
      "Epoch 00345: val_loss did not improve\n",
      "2s - loss: 0.7231 - acc: 0.7401 - val_loss: 0.6952 - val_acc: 0.7492\n",
      "Epoch 347/1500\n",
      "Epoch 00346: val_loss did not improve\n",
      "2s - loss: 0.7272 - acc: 0.7302 - val_loss: 0.7468 - val_acc: 0.7160\n",
      "Epoch 348/1500\n",
      "Epoch 00347: val_loss did not improve\n",
      "2s - loss: 0.7229 - acc: 0.7246 - val_loss: 0.6641 - val_acc: 0.7674\n",
      "Epoch 349/1500\n",
      "Epoch 00348: val_loss did not improve\n",
      "2s - loss: 0.6992 - acc: 0.7419 - val_loss: 0.6997 - val_acc: 0.7311\n",
      "Epoch 350/1500\n",
      "Epoch 00349: val_loss did not improve\n",
      "2s - loss: 0.7186 - acc: 0.7207 - val_loss: 0.6248 - val_acc: 0.7492\n",
      "Epoch 351/1500\n",
      "Epoch 00350: val_loss did not improve\n",
      "2s - loss: 0.6890 - acc: 0.7327 - val_loss: 0.6226 - val_acc: 0.7795\n",
      "Epoch 352/1500\n",
      "Epoch 00351: val_loss improved from 0.61786 to 0.57193, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.7092 - acc: 0.7389 - val_loss: 0.5719 - val_acc: 0.7855\n",
      "Epoch 353/1500\n",
      "Epoch 00352: val_loss did not improve\n",
      "2s - loss: 0.6995 - acc: 0.7419 - val_loss: 0.6532 - val_acc: 0.7764\n",
      "Epoch 354/1500\n",
      "Epoch 00353: val_loss did not improve\n",
      "2s - loss: 0.7187 - acc: 0.7379 - val_loss: 0.6686 - val_acc: 0.7583\n",
      "Epoch 355/1500\n",
      "Epoch 00354: val_loss did not improve\n",
      "2s - loss: 0.7306 - acc: 0.7151 - val_loss: 0.6634 - val_acc: 0.7764\n",
      "Epoch 356/1500\n",
      "Epoch 00355: val_loss did not improve\n",
      "2s - loss: 0.7113 - acc: 0.7288 - val_loss: 0.6517 - val_acc: 0.7674\n",
      "Epoch 357/1500\n",
      "Epoch 00356: val_loss did not improve\n",
      "2s - loss: 0.6830 - acc: 0.7404 - val_loss: 0.6118 - val_acc: 0.7825\n",
      "Epoch 358/1500\n",
      "Epoch 00357: val_loss did not improve\n",
      "2s - loss: 0.7061 - acc: 0.7400 - val_loss: 0.6109 - val_acc: 0.7704\n",
      "Epoch 359/1500\n",
      "Epoch 00358: val_loss did not improve\n",
      "2s - loss: 0.6829 - acc: 0.7428 - val_loss: 0.6704 - val_acc: 0.7583\n",
      "Epoch 360/1500\n",
      "Epoch 00359: val_loss did not improve\n",
      "2s - loss: 0.6987 - acc: 0.7505 - val_loss: 0.6553 - val_acc: 0.7704\n",
      "Epoch 361/1500\n",
      "Epoch 00360: val_loss did not improve\n",
      "2s - loss: 0.6791 - acc: 0.7413 - val_loss: 0.5938 - val_acc: 0.7885\n",
      "Epoch 362/1500\n",
      "Epoch 00361: val_loss did not improve\n",
      "2s - loss: 0.6899 - acc: 0.7510 - val_loss: 0.7184 - val_acc: 0.7613\n",
      "Epoch 363/1500\n",
      "Epoch 00362: val_loss did not improve\n",
      "2s - loss: 0.6823 - acc: 0.7391 - val_loss: 0.6551 - val_acc: 0.7553\n",
      "Epoch 364/1500\n",
      "Epoch 00363: val_loss did not improve\n",
      "2s - loss: 0.6994 - acc: 0.7360 - val_loss: 0.6143 - val_acc: 0.8036\n",
      "Epoch 365/1500\n",
      "Epoch 00364: val_loss did not improve\n",
      "2s - loss: 0.7173 - acc: 0.7287 - val_loss: 0.6064 - val_acc: 0.7855\n",
      "Epoch 366/1500\n",
      "Epoch 00365: val_loss did not improve\n",
      "2s - loss: 0.6729 - acc: 0.7522 - val_loss: 0.6155 - val_acc: 0.7613\n",
      "Epoch 367/1500\n",
      "Epoch 00366: val_loss did not improve\n",
      "2s - loss: 0.6956 - acc: 0.7430 - val_loss: 0.6414 - val_acc: 0.7795\n",
      "Epoch 368/1500\n",
      "Epoch 00367: val_loss did not improve\n",
      "2s - loss: 0.6786 - acc: 0.7320 - val_loss: 0.6161 - val_acc: 0.7946\n",
      "Epoch 369/1500\n",
      "Epoch 00368: val_loss did not improve\n",
      "2s - loss: 0.6751 - acc: 0.7542 - val_loss: 0.6402 - val_acc: 0.7372\n",
      "Epoch 370/1500\n",
      "Epoch 00369: val_loss improved from 0.57193 to 0.56980, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.7103 - acc: 0.7357 - val_loss: 0.5698 - val_acc: 0.8097\n",
      "Epoch 371/1500\n",
      "Epoch 00370: val_loss did not improve\n",
      "2s - loss: 0.6997 - acc: 0.7311 - val_loss: 0.6346 - val_acc: 0.7704\n",
      "Epoch 372/1500\n",
      "Epoch 00371: val_loss did not improve\n",
      "2s - loss: 0.6931 - acc: 0.7399 - val_loss: 0.6329 - val_acc: 0.7795\n",
      "Epoch 373/1500\n",
      "Epoch 00372: val_loss did not improve\n",
      "2s - loss: 0.7001 - acc: 0.7358 - val_loss: 0.6622 - val_acc: 0.7613\n",
      "Epoch 374/1500\n",
      "Epoch 00373: val_loss did not improve\n",
      "2s - loss: 0.6719 - acc: 0.7454 - val_loss: 0.6673 - val_acc: 0.7492\n",
      "Epoch 375/1500\n",
      "Epoch 00374: val_loss did not improve\n",
      "2s - loss: 0.6516 - acc: 0.7535 - val_loss: 0.5950 - val_acc: 0.7795\n",
      "Epoch 376/1500\n",
      "Epoch 00375: val_loss did not improve\n",
      "2s - loss: 0.6539 - acc: 0.7604 - val_loss: 0.6462 - val_acc: 0.7915\n",
      "Epoch 377/1500\n",
      "Epoch 00376: val_loss did not improve\n",
      "2s - loss: 0.6915 - acc: 0.7511 - val_loss: 0.6580 - val_acc: 0.7583\n",
      "Epoch 378/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00377: val_loss did not improve\n",
      "2s - loss: 0.6733 - acc: 0.7390 - val_loss: 0.5886 - val_acc: 0.7855\n",
      "Epoch 379/1500\n",
      "Epoch 00378: val_loss did not improve\n",
      "2s - loss: 0.6693 - acc: 0.7480 - val_loss: 0.6518 - val_acc: 0.7734\n",
      "Epoch 380/1500\n",
      "Epoch 00379: val_loss did not improve\n",
      "2s - loss: 0.6631 - acc: 0.7589 - val_loss: 0.6451 - val_acc: 0.7795\n",
      "Epoch 381/1500\n",
      "Epoch 00380: val_loss did not improve\n",
      "2s - loss: 0.6579 - acc: 0.7535 - val_loss: 0.7091 - val_acc: 0.7402\n",
      "Epoch 382/1500\n",
      "Epoch 00381: val_loss did not improve\n",
      "2s - loss: 0.6714 - acc: 0.7399 - val_loss: 0.6173 - val_acc: 0.7704\n",
      "Epoch 383/1500\n",
      "Epoch 00382: val_loss improved from 0.56980 to 0.55280, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.6507 - acc: 0.7585 - val_loss: 0.5528 - val_acc: 0.8218\n",
      "Epoch 384/1500\n",
      "Epoch 00383: val_loss did not improve\n",
      "2s - loss: 0.6361 - acc: 0.7605 - val_loss: 0.6025 - val_acc: 0.7764\n",
      "Epoch 385/1500\n",
      "Epoch 00384: val_loss did not improve\n",
      "2s - loss: 0.6446 - acc: 0.7623 - val_loss: 0.6226 - val_acc: 0.7462\n",
      "Epoch 386/1500\n",
      "Epoch 00385: val_loss did not improve\n",
      "2s - loss: 0.6722 - acc: 0.7352 - val_loss: 0.6230 - val_acc: 0.7795\n",
      "Epoch 387/1500\n",
      "Epoch 00386: val_loss did not improve\n",
      "2s - loss: 0.6565 - acc: 0.7589 - val_loss: 0.6106 - val_acc: 0.7583\n",
      "Epoch 388/1500\n",
      "Epoch 00387: val_loss did not improve\n",
      "2s - loss: 0.6562 - acc: 0.7470 - val_loss: 0.6242 - val_acc: 0.7764\n",
      "Epoch 389/1500\n",
      "Epoch 00388: val_loss did not improve\n",
      "2s - loss: 0.6491 - acc: 0.7640 - val_loss: 0.6811 - val_acc: 0.7492\n",
      "Epoch 390/1500\n",
      "Epoch 00389: val_loss did not improve\n",
      "2s - loss: 0.6214 - acc: 0.7716 - val_loss: 0.5892 - val_acc: 0.8036\n",
      "Epoch 391/1500\n",
      "Epoch 00390: val_loss did not improve\n",
      "2s - loss: 0.6231 - acc: 0.7792 - val_loss: 0.6711 - val_acc: 0.7704\n",
      "Epoch 392/1500\n",
      "Epoch 00391: val_loss did not improve\n",
      "2s - loss: 0.6706 - acc: 0.7522 - val_loss: 0.6640 - val_acc: 0.7372\n",
      "Epoch 393/1500\n",
      "Epoch 00392: val_loss did not improve\n",
      "2s - loss: 0.6420 - acc: 0.7600 - val_loss: 0.5950 - val_acc: 0.7825\n",
      "Epoch 394/1500\n",
      "Epoch 00393: val_loss did not improve\n",
      "2s - loss: 0.6471 - acc: 0.7642 - val_loss: 0.6871 - val_acc: 0.7613\n",
      "Epoch 395/1500\n",
      "Epoch 00394: val_loss did not improve\n",
      "2s - loss: 0.6905 - acc: 0.7393 - val_loss: 0.6244 - val_acc: 0.7946\n",
      "Epoch 396/1500\n",
      "Epoch 00395: val_loss did not improve\n",
      "2s - loss: 0.6372 - acc: 0.7624 - val_loss: 0.5963 - val_acc: 0.8066\n",
      "Epoch 397/1500\n",
      "Epoch 00396: val_loss did not improve\n",
      "2s - loss: 0.6211 - acc: 0.7838 - val_loss: 0.6485 - val_acc: 0.7432\n",
      "Epoch 398/1500\n",
      "Epoch 00397: val_loss did not improve\n",
      "2s - loss: 0.6338 - acc: 0.7762 - val_loss: 0.5806 - val_acc: 0.7734\n",
      "Epoch 399/1500\n",
      "Epoch 00398: val_loss did not improve\n",
      "2s - loss: 0.6632 - acc: 0.7605 - val_loss: 0.5578 - val_acc: 0.7976\n",
      "Epoch 400/1500\n",
      "Epoch 00399: val_loss did not improve\n",
      "2s - loss: 0.6584 - acc: 0.7625 - val_loss: 0.6397 - val_acc: 0.7704\n",
      "Epoch 401/1500\n",
      "Epoch 00400: val_loss did not improve\n",
      "2s - loss: 0.6499 - acc: 0.7640 - val_loss: 0.6142 - val_acc: 0.7825\n",
      "Epoch 402/1500\n",
      "Epoch 00401: val_loss did not improve\n",
      "2s - loss: 0.6372 - acc: 0.7592 - val_loss: 0.6566 - val_acc: 0.7462\n",
      "Epoch 403/1500\n",
      "Epoch 00402: val_loss did not improve\n",
      "2s - loss: 0.6299 - acc: 0.7839 - val_loss: 0.5972 - val_acc: 0.7795\n",
      "Epoch 404/1500\n",
      "Epoch 00403: val_loss did not improve\n",
      "2s - loss: 0.6116 - acc: 0.7727 - val_loss: 0.5963 - val_acc: 0.7583\n",
      "Epoch 405/1500\n",
      "Epoch 00404: val_loss did not improve\n",
      "2s - loss: 0.6794 - acc: 0.7479 - val_loss: 0.6428 - val_acc: 0.7583\n",
      "Epoch 406/1500\n",
      "Epoch 00405: val_loss did not improve\n",
      "2s - loss: 0.6533 - acc: 0.7560 - val_loss: 0.6899 - val_acc: 0.7613\n",
      "Epoch 407/1500\n",
      "Epoch 00406: val_loss did not improve\n",
      "2s - loss: 0.6828 - acc: 0.7511 - val_loss: 0.6060 - val_acc: 0.7795\n",
      "Epoch 408/1500\n",
      "Epoch 00407: val_loss did not improve\n",
      "2s - loss: 0.5827 - acc: 0.7814 - val_loss: 0.5863 - val_acc: 0.7825\n",
      "Epoch 409/1500\n",
      "Epoch 00408: val_loss did not improve\n",
      "2s - loss: 0.6726 - acc: 0.7461 - val_loss: 0.6152 - val_acc: 0.7553\n",
      "Epoch 410/1500\n",
      "Epoch 00409: val_loss did not improve\n",
      "2s - loss: 0.6442 - acc: 0.7749 - val_loss: 0.6149 - val_acc: 0.7704\n",
      "Epoch 411/1500\n",
      "Epoch 00410: val_loss did not improve\n",
      "2s - loss: 0.6185 - acc: 0.7678 - val_loss: 0.6068 - val_acc: 0.7764\n",
      "Epoch 412/1500\n",
      "Epoch 00411: val_loss did not improve\n",
      "2s - loss: 0.6689 - acc: 0.7459 - val_loss: 0.5599 - val_acc: 0.8157\n",
      "Epoch 413/1500\n",
      "Epoch 00412: val_loss did not improve\n",
      "2s - loss: 0.6383 - acc: 0.7648 - val_loss: 0.5589 - val_acc: 0.7976\n",
      "Epoch 414/1500\n",
      "Epoch 00413: val_loss did not improve\n",
      "2s - loss: 0.6478 - acc: 0.7705 - val_loss: 0.6217 - val_acc: 0.7734\n",
      "Epoch 415/1500\n",
      "Epoch 00414: val_loss improved from 0.55280 to 0.54528, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.6405 - acc: 0.7497 - val_loss: 0.5453 - val_acc: 0.7764\n",
      "Epoch 416/1500\n",
      "Epoch 00415: val_loss did not improve\n",
      "2s - loss: 0.6328 - acc: 0.7616 - val_loss: 0.6094 - val_acc: 0.7855\n",
      "Epoch 417/1500\n",
      "Epoch 00416: val_loss did not improve\n",
      "2s - loss: 0.6333 - acc: 0.7529 - val_loss: 0.5646 - val_acc: 0.7734\n",
      "Epoch 418/1500\n",
      "Epoch 00417: val_loss did not improve\n",
      "2s - loss: 0.6248 - acc: 0.7615 - val_loss: 0.5655 - val_acc: 0.7946\n",
      "Epoch 419/1500\n",
      "Epoch 00418: val_loss did not improve\n",
      "2s - loss: 0.5957 - acc: 0.7822 - val_loss: 0.5796 - val_acc: 0.7855\n",
      "Epoch 420/1500\n",
      "Epoch 00419: val_loss improved from 0.54528 to 0.50856, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.6444 - acc: 0.7606 - val_loss: 0.5086 - val_acc: 0.8278\n",
      "Epoch 421/1500\n",
      "Epoch 00420: val_loss did not improve\n",
      "2s - loss: 0.6354 - acc: 0.7792 - val_loss: 0.5813 - val_acc: 0.7764\n",
      "Epoch 422/1500\n",
      "Epoch 00421: val_loss did not improve\n",
      "2s - loss: 0.6100 - acc: 0.7774 - val_loss: 0.5900 - val_acc: 0.7855\n",
      "Epoch 423/1500\n",
      "Epoch 00422: val_loss did not improve\n",
      "2s - loss: 0.6568 - acc: 0.7447 - val_loss: 0.6127 - val_acc: 0.7946\n",
      "Epoch 424/1500\n",
      "Epoch 00423: val_loss did not improve\n",
      "2s - loss: 0.6227 - acc: 0.7770 - val_loss: 0.6722 - val_acc: 0.7734\n",
      "Epoch 425/1500\n",
      "Epoch 00424: val_loss did not improve\n",
      "2s - loss: 0.6113 - acc: 0.7853 - val_loss: 0.5589 - val_acc: 0.7915\n",
      "Epoch 426/1500\n",
      "Epoch 00425: val_loss did not improve\n",
      "2s - loss: 0.5783 - acc: 0.7894 - val_loss: 0.6013 - val_acc: 0.7674\n",
      "Epoch 427/1500\n",
      "Epoch 00426: val_loss did not improve\n",
      "2s - loss: 0.6044 - acc: 0.7576 - val_loss: 0.5887 - val_acc: 0.7885\n",
      "Epoch 428/1500\n",
      "Epoch 00427: val_loss did not improve\n",
      "2s - loss: 0.5933 - acc: 0.7784 - val_loss: 0.5775 - val_acc: 0.7764\n",
      "Epoch 429/1500\n",
      "Epoch 00428: val_loss did not improve\n",
      "2s - loss: 0.5905 - acc: 0.7761 - val_loss: 0.6047 - val_acc: 0.8036\n",
      "Epoch 430/1500\n",
      "Epoch 00429: val_loss did not improve\n",
      "2s - loss: 0.5986 - acc: 0.7773 - val_loss: 0.5824 - val_acc: 0.8127\n",
      "Epoch 431/1500\n",
      "Epoch 00430: val_loss did not improve\n",
      "2s - loss: 0.6253 - acc: 0.7702 - val_loss: 0.6084 - val_acc: 0.7764\n",
      "Epoch 432/1500\n",
      "Epoch 00431: val_loss did not improve\n",
      "2s - loss: 0.5943 - acc: 0.7768 - val_loss: 0.5454 - val_acc: 0.8187\n",
      "Epoch 433/1500\n",
      "Epoch 00432: val_loss did not improve\n",
      "2s - loss: 0.6329 - acc: 0.7621 - val_loss: 0.5640 - val_acc: 0.8006\n",
      "Epoch 434/1500\n",
      "Epoch 00433: val_loss did not improve\n",
      "2s - loss: 0.6339 - acc: 0.7672 - val_loss: 0.5577 - val_acc: 0.8097\n",
      "Epoch 435/1500\n",
      "Epoch 00434: val_loss did not improve\n",
      "2s - loss: 0.6074 - acc: 0.7711 - val_loss: 0.6792 - val_acc: 0.7523\n",
      "Epoch 436/1500\n",
      "Epoch 00435: val_loss did not improve\n",
      "2s - loss: 0.6161 - acc: 0.7623 - val_loss: 0.6122 - val_acc: 0.7855\n",
      "Epoch 437/1500\n",
      "Epoch 00436: val_loss did not improve\n",
      "2s - loss: 0.5716 - acc: 0.7847 - val_loss: 0.5437 - val_acc: 0.8066\n",
      "Epoch 438/1500\n",
      "Epoch 00437: val_loss did not improve\n",
      "2s - loss: 0.5962 - acc: 0.7664 - val_loss: 0.5633 - val_acc: 0.8127\n",
      "Epoch 439/1500\n",
      "Epoch 00438: val_loss did not improve\n",
      "2s - loss: 0.6275 - acc: 0.7698 - val_loss: 0.6325 - val_acc: 0.7825\n",
      "Epoch 440/1500\n",
      "Epoch 00439: val_loss did not improve\n",
      "2s - loss: 0.6246 - acc: 0.7688 - val_loss: 0.5939 - val_acc: 0.7976\n",
      "Epoch 441/1500\n",
      "Epoch 00440: val_loss did not improve\n",
      "2s - loss: 0.6229 - acc: 0.7695 - val_loss: 0.6034 - val_acc: 0.7825\n",
      "Epoch 442/1500\n",
      "Epoch 00441: val_loss did not improve\n",
      "2s - loss: 0.6144 - acc: 0.7749 - val_loss: 0.6157 - val_acc: 0.7885\n",
      "Epoch 443/1500\n",
      "Epoch 00442: val_loss did not improve\n",
      "2s - loss: 0.5525 - acc: 0.7974 - val_loss: 0.5662 - val_acc: 0.7795\n",
      "Epoch 444/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00443: val_loss did not improve\n",
      "2s - loss: 0.5590 - acc: 0.7973 - val_loss: 0.5928 - val_acc: 0.8006\n",
      "Epoch 445/1500\n",
      "Epoch 00444: val_loss did not improve\n",
      "2s - loss: 0.5917 - acc: 0.7790 - val_loss: 0.5764 - val_acc: 0.7885\n",
      "Epoch 446/1500\n",
      "Epoch 00445: val_loss did not improve\n",
      "2s - loss: 0.6056 - acc: 0.7636 - val_loss: 0.5964 - val_acc: 0.7946\n",
      "Epoch 447/1500\n",
      "Epoch 00446: val_loss did not improve\n",
      "2s - loss: 0.6231 - acc: 0.7711 - val_loss: 0.5505 - val_acc: 0.7915\n",
      "Epoch 448/1500\n",
      "Epoch 00447: val_loss did not improve\n",
      "2s - loss: 0.5637 - acc: 0.7997 - val_loss: 0.6063 - val_acc: 0.7976\n",
      "Epoch 449/1500\n",
      "Epoch 00448: val_loss improved from 0.50856 to 0.50202, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.5417 - acc: 0.8045 - val_loss: 0.5020 - val_acc: 0.8248\n",
      "Epoch 450/1500\n",
      "Epoch 00449: val_loss did not improve\n",
      "2s - loss: 0.6278 - acc: 0.7728 - val_loss: 0.6046 - val_acc: 0.7855\n",
      "Epoch 451/1500\n",
      "Epoch 00450: val_loss did not improve\n",
      "2s - loss: 0.6042 - acc: 0.7785 - val_loss: 0.6514 - val_acc: 0.7644\n",
      "Epoch 452/1500\n",
      "Epoch 00451: val_loss did not improve\n",
      "2s - loss: 0.5946 - acc: 0.7758 - val_loss: 0.5866 - val_acc: 0.7885\n",
      "Epoch 453/1500\n",
      "Epoch 00452: val_loss did not improve\n",
      "2s - loss: 0.5945 - acc: 0.7657 - val_loss: 0.5785 - val_acc: 0.7734\n",
      "Epoch 454/1500\n",
      "Epoch 00453: val_loss did not improve\n",
      "2s - loss: 0.5857 - acc: 0.7727 - val_loss: 0.5791 - val_acc: 0.7946\n",
      "Epoch 455/1500\n",
      "Epoch 00454: val_loss did not improve\n",
      "2s - loss: 0.5708 - acc: 0.8052 - val_loss: 0.5236 - val_acc: 0.8157\n",
      "Epoch 456/1500\n",
      "Epoch 00455: val_loss did not improve\n",
      "2s - loss: 0.5743 - acc: 0.7918 - val_loss: 0.5642 - val_acc: 0.8036\n",
      "Epoch 457/1500\n",
      "Epoch 00456: val_loss did not improve\n",
      "2s - loss: 0.5315 - acc: 0.7980 - val_loss: 0.6541 - val_acc: 0.7764\n",
      "Epoch 458/1500\n",
      "Epoch 00457: val_loss did not improve\n",
      "2s - loss: 0.6403 - acc: 0.7698 - val_loss: 0.6007 - val_acc: 0.7613\n",
      "Epoch 459/1500\n",
      "Epoch 00458: val_loss did not improve\n",
      "2s - loss: 0.6151 - acc: 0.7696 - val_loss: 0.5877 - val_acc: 0.7674\n",
      "Epoch 460/1500\n",
      "Epoch 00459: val_loss did not improve\n",
      "2s - loss: 0.5819 - acc: 0.7790 - val_loss: 0.5711 - val_acc: 0.8006\n",
      "Epoch 461/1500\n",
      "Epoch 00460: val_loss did not improve\n",
      "2s - loss: 0.5853 - acc: 0.7848 - val_loss: 0.6580 - val_acc: 0.7402\n",
      "Epoch 462/1500\n",
      "Epoch 00461: val_loss did not improve\n",
      "2s - loss: 0.5880 - acc: 0.7862 - val_loss: 0.5301 - val_acc: 0.8036\n",
      "Epoch 463/1500\n",
      "Epoch 00462: val_loss did not improve\n",
      "2s - loss: 0.6352 - acc: 0.7692 - val_loss: 0.5403 - val_acc: 0.8187\n",
      "Epoch 464/1500\n",
      "Epoch 00463: val_loss did not improve\n",
      "2s - loss: 0.5679 - acc: 0.7927 - val_loss: 0.6350 - val_acc: 0.7674\n",
      "Epoch 465/1500\n",
      "Epoch 00464: val_loss did not improve\n",
      "2s - loss: 0.5917 - acc: 0.7950 - val_loss: 0.5514 - val_acc: 0.7795\n",
      "Epoch 466/1500\n",
      "Epoch 00465: val_loss did not improve\n",
      "2s - loss: 0.5683 - acc: 0.7879 - val_loss: 0.6180 - val_acc: 0.7644\n",
      "Epoch 467/1500\n",
      "Epoch 00466: val_loss did not improve\n",
      "2s - loss: 0.5969 - acc: 0.7712 - val_loss: 0.5356 - val_acc: 0.8036\n",
      "Epoch 468/1500\n",
      "Epoch 00467: val_loss did not improve\n",
      "2s - loss: 0.5889 - acc: 0.7759 - val_loss: 0.5805 - val_acc: 0.7885\n",
      "Epoch 469/1500\n",
      "Epoch 00468: val_loss did not improve\n",
      "2s - loss: 0.5777 - acc: 0.8005 - val_loss: 0.5967 - val_acc: 0.7704\n",
      "Epoch 470/1500\n",
      "Epoch 00469: val_loss did not improve\n",
      "2s - loss: 0.5754 - acc: 0.7807 - val_loss: 0.5379 - val_acc: 0.8187\n",
      "Epoch 471/1500\n",
      "Epoch 00470: val_loss did not improve\n",
      "2s - loss: 0.5484 - acc: 0.7934 - val_loss: 0.5994 - val_acc: 0.7704\n",
      "Epoch 472/1500\n",
      "Epoch 00471: val_loss did not improve\n",
      "2s - loss: 0.5705 - acc: 0.7846 - val_loss: 0.5670 - val_acc: 0.7885\n",
      "Epoch 473/1500\n",
      "Epoch 00472: val_loss did not improve\n",
      "2s - loss: 0.5535 - acc: 0.7975 - val_loss: 0.5629 - val_acc: 0.8097\n",
      "Epoch 474/1500\n",
      "Epoch 00473: val_loss improved from 0.50202 to 0.47468, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.5794 - acc: 0.7786 - val_loss: 0.4747 - val_acc: 0.8278\n",
      "Epoch 475/1500\n",
      "Epoch 00474: val_loss did not improve\n",
      "2s - loss: 0.5406 - acc: 0.8057 - val_loss: 0.4993 - val_acc: 0.8157\n",
      "Epoch 476/1500\n",
      "Epoch 00475: val_loss did not improve\n",
      "2s - loss: 0.5822 - acc: 0.7864 - val_loss: 0.5849 - val_acc: 0.7946\n",
      "Epoch 477/1500\n",
      "Epoch 00476: val_loss did not improve\n",
      "2s - loss: 0.5974 - acc: 0.7817 - val_loss: 0.5091 - val_acc: 0.8127\n",
      "Epoch 478/1500\n",
      "Epoch 00477: val_loss did not improve\n",
      "2s - loss: 0.5507 - acc: 0.7856 - val_loss: 0.5860 - val_acc: 0.7946\n",
      "Epoch 479/1500\n",
      "Epoch 00478: val_loss did not improve\n",
      "2s - loss: 0.5472 - acc: 0.7963 - val_loss: 0.5290 - val_acc: 0.8157\n",
      "Epoch 480/1500\n",
      "Epoch 00479: val_loss did not improve\n",
      "2s - loss: 0.5757 - acc: 0.8038 - val_loss: 0.5882 - val_acc: 0.7915\n",
      "Epoch 481/1500\n",
      "Epoch 00480: val_loss did not improve\n",
      "2s - loss: 0.5449 - acc: 0.7920 - val_loss: 0.5061 - val_acc: 0.8369\n",
      "Epoch 482/1500\n",
      "Epoch 00481: val_loss did not improve\n",
      "2s - loss: 0.5638 - acc: 0.7911 - val_loss: 0.5315 - val_acc: 0.7976\n",
      "Epoch 483/1500\n",
      "Epoch 00482: val_loss did not improve\n",
      "2s - loss: 0.5708 - acc: 0.7856 - val_loss: 0.5812 - val_acc: 0.7855\n",
      "Epoch 484/1500\n",
      "Epoch 00483: val_loss did not improve\n",
      "2s - loss: 0.5137 - acc: 0.8181 - val_loss: 0.5097 - val_acc: 0.8036\n",
      "Epoch 485/1500\n",
      "Epoch 00484: val_loss did not improve\n",
      "2s - loss: 0.5621 - acc: 0.7686 - val_loss: 0.5530 - val_acc: 0.7946\n",
      "Epoch 486/1500\n",
      "Epoch 00485: val_loss did not improve\n",
      "2s - loss: 0.5759 - acc: 0.7936 - val_loss: 0.4759 - val_acc: 0.8459\n",
      "Epoch 487/1500\n",
      "Epoch 00486: val_loss improved from 0.47468 to 0.46226, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.5530 - acc: 0.7893 - val_loss: 0.4623 - val_acc: 0.8429\n",
      "Epoch 488/1500\n",
      "Epoch 00487: val_loss did not improve\n",
      "2s - loss: 0.5576 - acc: 0.7838 - val_loss: 0.5045 - val_acc: 0.8157\n",
      "Epoch 489/1500\n",
      "Epoch 00488: val_loss did not improve\n",
      "2s - loss: 0.5600 - acc: 0.7947 - val_loss: 0.5226 - val_acc: 0.8066\n",
      "Epoch 490/1500\n",
      "Epoch 00489: val_loss improved from 0.46226 to 0.46085, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.5860 - acc: 0.7658 - val_loss: 0.4608 - val_acc: 0.8218\n",
      "Epoch 491/1500\n",
      "Epoch 00490: val_loss did not improve\n",
      "2s - loss: 0.5226 - acc: 0.8044 - val_loss: 0.5546 - val_acc: 0.8097\n",
      "Epoch 492/1500\n",
      "Epoch 00491: val_loss did not improve\n",
      "2s - loss: 0.5693 - acc: 0.7814 - val_loss: 0.5411 - val_acc: 0.8036\n",
      "Epoch 493/1500\n",
      "Epoch 00492: val_loss did not improve\n",
      "2s - loss: 0.5163 - acc: 0.8063 - val_loss: 0.6264 - val_acc: 0.7915\n",
      "Epoch 494/1500\n",
      "Epoch 00493: val_loss did not improve\n",
      "2s - loss: 0.5557 - acc: 0.7911 - val_loss: 0.4768 - val_acc: 0.8489\n",
      "Epoch 495/1500\n",
      "Epoch 00494: val_loss did not improve\n",
      "2s - loss: 0.5660 - acc: 0.7841 - val_loss: 0.5005 - val_acc: 0.8338\n",
      "Epoch 496/1500\n",
      "Epoch 00495: val_loss did not improve\n",
      "2s - loss: 0.5162 - acc: 0.8181 - val_loss: 0.5994 - val_acc: 0.8097\n",
      "Epoch 497/1500\n",
      "Epoch 00496: val_loss did not improve\n",
      "2s - loss: 0.5338 - acc: 0.8055 - val_loss: 0.5887 - val_acc: 0.7946\n",
      "Epoch 498/1500\n",
      "Epoch 00497: val_loss did not improve\n",
      "2s - loss: 0.5594 - acc: 0.7831 - val_loss: 0.4926 - val_acc: 0.8157\n",
      "Epoch 499/1500\n",
      "Epoch 00498: val_loss did not improve\n",
      "2s - loss: 0.5289 - acc: 0.8023 - val_loss: 0.5489 - val_acc: 0.7976\n",
      "Epoch 500/1500\n",
      "Epoch 00499: val_loss did not improve\n",
      "2s - loss: 0.5519 - acc: 0.7910 - val_loss: 0.4631 - val_acc: 0.8218\n",
      "Epoch 501/1500\n",
      "Epoch 00500: val_loss improved from 0.46085 to 0.44561, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.5570 - acc: 0.7871 - val_loss: 0.4456 - val_acc: 0.8338\n",
      "Epoch 502/1500\n",
      "Epoch 00501: val_loss did not improve\n",
      "2s - loss: 0.5392 - acc: 0.8031 - val_loss: 0.5613 - val_acc: 0.7976\n",
      "Epoch 503/1500\n",
      "Epoch 00502: val_loss did not improve\n",
      "2s - loss: 0.5660 - acc: 0.7927 - val_loss: 0.5578 - val_acc: 0.7795\n",
      "Epoch 504/1500\n",
      "Epoch 00503: val_loss did not improve\n",
      "2s - loss: 0.5344 - acc: 0.8104 - val_loss: 0.5534 - val_acc: 0.7976\n",
      "Epoch 505/1500\n",
      "Epoch 00504: val_loss did not improve\n",
      "2s - loss: 0.5608 - acc: 0.8000 - val_loss: 0.5595 - val_acc: 0.7976\n",
      "Epoch 506/1500\n",
      "Epoch 00505: val_loss did not improve\n",
      "2s - loss: 0.5277 - acc: 0.8035 - val_loss: 0.4650 - val_acc: 0.8308\n",
      "Epoch 507/1500\n",
      "Epoch 00506: val_loss did not improve\n",
      "2s - loss: 0.5331 - acc: 0.8005 - val_loss: 0.5337 - val_acc: 0.8006\n",
      "Epoch 508/1500\n",
      "Epoch 00507: val_loss did not improve\n",
      "2s - loss: 0.5202 - acc: 0.8102 - val_loss: 0.5834 - val_acc: 0.7915\n",
      "Epoch 509/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00508: val_loss did not improve\n",
      "2s - loss: 0.5011 - acc: 0.8144 - val_loss: 0.4500 - val_acc: 0.8520\n",
      "Epoch 510/1500\n",
      "Epoch 00509: val_loss did not improve\n",
      "2s - loss: 0.5254 - acc: 0.7911 - val_loss: 0.4986 - val_acc: 0.8218\n",
      "Epoch 511/1500\n",
      "Epoch 00510: val_loss did not improve\n",
      "2s - loss: 0.5019 - acc: 0.8240 - val_loss: 0.4828 - val_acc: 0.8218\n",
      "Epoch 512/1500\n",
      "Epoch 00511: val_loss did not improve\n",
      "2s - loss: 0.5825 - acc: 0.7910 - val_loss: 0.5666 - val_acc: 0.8066\n",
      "Epoch 513/1500\n",
      "Epoch 00512: val_loss did not improve\n",
      "2s - loss: 0.5579 - acc: 0.7920 - val_loss: 0.5808 - val_acc: 0.7825\n",
      "Epoch 514/1500\n",
      "Epoch 00513: val_loss did not improve\n",
      "2s - loss: 0.5124 - acc: 0.8122 - val_loss: 0.5035 - val_acc: 0.8066\n",
      "Epoch 515/1500\n",
      "Epoch 00514: val_loss did not improve\n",
      "2s - loss: 0.5274 - acc: 0.8037 - val_loss: 0.4635 - val_acc: 0.8459\n",
      "Epoch 516/1500\n",
      "Epoch 00515: val_loss did not improve\n",
      "2s - loss: 0.5534 - acc: 0.7919 - val_loss: 0.5087 - val_acc: 0.8066\n",
      "Epoch 517/1500\n",
      "Epoch 00516: val_loss did not improve\n",
      "2s - loss: 0.5133 - acc: 0.8293 - val_loss: 0.5535 - val_acc: 0.8036\n",
      "Epoch 518/1500\n",
      "Epoch 00517: val_loss did not improve\n",
      "2s - loss: 0.5296 - acc: 0.8097 - val_loss: 0.5552 - val_acc: 0.7976\n",
      "Epoch 519/1500\n",
      "Epoch 00518: val_loss did not improve\n",
      "2s - loss: 0.5288 - acc: 0.8005 - val_loss: 0.4502 - val_acc: 0.8459\n",
      "Epoch 520/1500\n",
      "Epoch 00519: val_loss did not improve\n",
      "2s - loss: 0.5402 - acc: 0.8034 - val_loss: 0.5508 - val_acc: 0.8218\n",
      "Epoch 521/1500\n",
      "Epoch 00520: val_loss did not improve\n",
      "2s - loss: 0.5554 - acc: 0.7911 - val_loss: 0.4976 - val_acc: 0.8187\n",
      "Epoch 522/1500\n",
      "Epoch 00521: val_loss did not improve\n",
      "2s - loss: 0.5208 - acc: 0.8022 - val_loss: 0.5123 - val_acc: 0.7915\n",
      "Epoch 523/1500\n",
      "Epoch 00522: val_loss did not improve\n",
      "2s - loss: 0.5495 - acc: 0.8110 - val_loss: 0.4745 - val_acc: 0.8187\n",
      "Epoch 524/1500\n",
      "Epoch 00523: val_loss did not improve\n",
      "2s - loss: 0.5545 - acc: 0.7945 - val_loss: 0.5217 - val_acc: 0.8187\n",
      "Epoch 525/1500\n",
      "Epoch 00524: val_loss did not improve\n",
      "2s - loss: 0.5519 - acc: 0.7887 - val_loss: 0.5032 - val_acc: 0.8218\n",
      "Epoch 526/1500\n",
      "Epoch 00525: val_loss did not improve\n",
      "2s - loss: 0.4887 - acc: 0.8189 - val_loss: 0.5463 - val_acc: 0.8066\n",
      "Epoch 527/1500\n",
      "Epoch 00526: val_loss did not improve\n",
      "2s - loss: 0.5645 - acc: 0.7895 - val_loss: 0.5609 - val_acc: 0.7885\n",
      "Epoch 528/1500\n",
      "Epoch 00527: val_loss did not improve\n",
      "2s - loss: 0.5738 - acc: 0.7921 - val_loss: 0.4660 - val_acc: 0.8308\n",
      "Epoch 529/1500\n",
      "Epoch 00528: val_loss did not improve\n",
      "2s - loss: 0.5278 - acc: 0.8134 - val_loss: 0.5497 - val_acc: 0.7976\n",
      "Epoch 530/1500\n",
      "Epoch 00529: val_loss did not improve\n",
      "2s - loss: 0.5077 - acc: 0.8079 - val_loss: 0.4989 - val_acc: 0.7855\n",
      "Epoch 531/1500\n",
      "Epoch 00530: val_loss did not improve\n",
      "2s - loss: 0.5187 - acc: 0.7944 - val_loss: 0.4952 - val_acc: 0.8218\n",
      "Epoch 532/1500\n",
      "Epoch 00531: val_loss did not improve\n",
      "2s - loss: 0.4921 - acc: 0.8157 - val_loss: 0.4740 - val_acc: 0.8308\n",
      "Epoch 533/1500\n",
      "Epoch 00532: val_loss did not improve\n",
      "2s - loss: 0.5437 - acc: 0.7990 - val_loss: 0.4530 - val_acc: 0.8399\n",
      "Epoch 534/1500\n",
      "Epoch 00533: val_loss did not improve\n",
      "2s - loss: 0.5281 - acc: 0.8098 - val_loss: 0.5648 - val_acc: 0.7795\n",
      "Epoch 535/1500\n",
      "Epoch 00534: val_loss did not improve\n",
      "2s - loss: 0.4823 - acc: 0.8167 - val_loss: 0.4479 - val_acc: 0.8278\n",
      "Epoch 536/1500\n",
      "Epoch 00535: val_loss did not improve\n",
      "2s - loss: 0.5416 - acc: 0.8086 - val_loss: 0.4884 - val_acc: 0.8097\n",
      "Epoch 537/1500\n",
      "Epoch 00536: val_loss did not improve\n",
      "2s - loss: 0.5333 - acc: 0.8030 - val_loss: 0.4876 - val_acc: 0.8187\n",
      "Epoch 538/1500\n",
      "Epoch 00537: val_loss did not improve\n",
      "2s - loss: 0.5632 - acc: 0.7953 - val_loss: 0.5132 - val_acc: 0.8127\n",
      "Epoch 539/1500\n",
      "Epoch 00538: val_loss did not improve\n",
      "2s - loss: 0.5125 - acc: 0.8104 - val_loss: 0.5261 - val_acc: 0.8278\n",
      "Epoch 540/1500\n",
      "Epoch 00539: val_loss improved from 0.44561 to 0.41228, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.5111 - acc: 0.8069 - val_loss: 0.4123 - val_acc: 0.8520\n",
      "Epoch 541/1500\n",
      "Epoch 00540: val_loss did not improve\n",
      "2s - loss: 0.5069 - acc: 0.8105 - val_loss: 0.4953 - val_acc: 0.8187\n",
      "Epoch 542/1500\n",
      "Epoch 00541: val_loss did not improve\n",
      "2s - loss: 0.5019 - acc: 0.8157 - val_loss: 0.6161 - val_acc: 0.7855\n",
      "Epoch 543/1500\n",
      "Epoch 00542: val_loss did not improve\n",
      "2s - loss: 0.5482 - acc: 0.8032 - val_loss: 0.4980 - val_acc: 0.8006\n",
      "Epoch 544/1500\n",
      "Epoch 00543: val_loss did not improve\n",
      "2s - loss: 0.5374 - acc: 0.7975 - val_loss: 0.5259 - val_acc: 0.8006\n",
      "Epoch 545/1500\n",
      "Epoch 00544: val_loss did not improve\n",
      "2s - loss: 0.5261 - acc: 0.8044 - val_loss: 0.5528 - val_acc: 0.7855\n",
      "Epoch 546/1500\n",
      "Epoch 00545: val_loss did not improve\n",
      "2s - loss: 0.5256 - acc: 0.8102 - val_loss: 0.4903 - val_acc: 0.8429\n",
      "Epoch 547/1500\n",
      "Epoch 00546: val_loss did not improve\n",
      "2s - loss: 0.5272 - acc: 0.8078 - val_loss: 0.4901 - val_acc: 0.8218\n",
      "Epoch 548/1500\n",
      "Epoch 00547: val_loss did not improve\n",
      "2s - loss: 0.4933 - acc: 0.8069 - val_loss: 0.4908 - val_acc: 0.8218\n",
      "Epoch 549/1500\n",
      "Epoch 00548: val_loss did not improve\n",
      "2s - loss: 0.5039 - acc: 0.8040 - val_loss: 0.5238 - val_acc: 0.8127\n",
      "Epoch 550/1500\n",
      "Epoch 00549: val_loss did not improve\n",
      "2s - loss: 0.5557 - acc: 0.7971 - val_loss: 0.4562 - val_acc: 0.8369\n",
      "Epoch 551/1500\n",
      "Epoch 00550: val_loss did not improve\n",
      "2s - loss: 0.4867 - acc: 0.8260 - val_loss: 0.4944 - val_acc: 0.8218\n",
      "Epoch 552/1500\n",
      "Epoch 00551: val_loss did not improve\n",
      "2s - loss: 0.5296 - acc: 0.7983 - val_loss: 0.4250 - val_acc: 0.8399\n",
      "Epoch 553/1500\n",
      "Epoch 00552: val_loss did not improve\n",
      "2s - loss: 0.5467 - acc: 0.7874 - val_loss: 0.4884 - val_acc: 0.8308\n",
      "Epoch 554/1500\n",
      "Epoch 00553: val_loss did not improve\n",
      "2s - loss: 0.4894 - acc: 0.8152 - val_loss: 0.5180 - val_acc: 0.7946\n",
      "Epoch 555/1500\n",
      "Epoch 00554: val_loss did not improve\n",
      "2s - loss: 0.5179 - acc: 0.8066 - val_loss: 0.5144 - val_acc: 0.8157\n",
      "Epoch 556/1500\n",
      "Epoch 00555: val_loss did not improve\n",
      "2s - loss: 0.5069 - acc: 0.8158 - val_loss: 0.4714 - val_acc: 0.8278\n",
      "Epoch 557/1500\n",
      "Epoch 00556: val_loss did not improve\n",
      "2s - loss: 0.5488 - acc: 0.8039 - val_loss: 0.5024 - val_acc: 0.8308\n",
      "Epoch 558/1500\n",
      "Epoch 00557: val_loss did not improve\n",
      "2s - loss: 0.5101 - acc: 0.8013 - val_loss: 0.4674 - val_acc: 0.8429\n",
      "Epoch 559/1500\n",
      "Epoch 00558: val_loss did not improve\n",
      "2s - loss: 0.4840 - acc: 0.8201 - val_loss: 0.4872 - val_acc: 0.8187\n",
      "Epoch 560/1500\n",
      "Epoch 00559: val_loss did not improve\n",
      "2s - loss: 0.5451 - acc: 0.7888 - val_loss: 0.4435 - val_acc: 0.8459\n",
      "Epoch 561/1500\n",
      "Epoch 00560: val_loss did not improve\n",
      "2s - loss: 0.5331 - acc: 0.7966 - val_loss: 0.4622 - val_acc: 0.8218\n",
      "Epoch 562/1500\n",
      "Epoch 00561: val_loss improved from 0.41228 to 0.40637, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.5278 - acc: 0.8055 - val_loss: 0.4064 - val_acc: 0.8580\n",
      "Epoch 563/1500\n",
      "Epoch 00562: val_loss did not improve\n",
      "2s - loss: 0.4753 - acc: 0.8165 - val_loss: 0.4359 - val_acc: 0.8248\n",
      "Epoch 564/1500\n",
      "Epoch 00563: val_loss did not improve\n",
      "2s - loss: 0.5176 - acc: 0.8122 - val_loss: 0.4791 - val_acc: 0.8127\n",
      "Epoch 565/1500\n",
      "Epoch 00564: val_loss did not improve\n",
      "2s - loss: 0.5161 - acc: 0.8069 - val_loss: 0.4374 - val_acc: 0.8218\n",
      "Epoch 566/1500\n",
      "Epoch 00565: val_loss did not improve\n",
      "2s - loss: 0.4751 - acc: 0.8213 - val_loss: 0.4715 - val_acc: 0.8187\n",
      "Epoch 567/1500\n",
      "Epoch 00566: val_loss did not improve\n",
      "2s - loss: 0.4759 - acc: 0.8350 - val_loss: 0.4694 - val_acc: 0.8218\n",
      "Epoch 568/1500\n",
      "Epoch 00567: val_loss did not improve\n",
      "2s - loss: 0.4946 - acc: 0.8264 - val_loss: 0.5966 - val_acc: 0.7734\n",
      "Epoch 569/1500\n",
      "Epoch 00568: val_loss did not improve\n",
      "2s - loss: 0.4720 - acc: 0.8232 - val_loss: 0.4662 - val_acc: 0.8338\n",
      "Epoch 570/1500\n",
      "Epoch 00569: val_loss did not improve\n",
      "2s - loss: 0.4680 - acc: 0.8328 - val_loss: 0.4443 - val_acc: 0.8127\n",
      "Epoch 571/1500\n",
      "Epoch 00570: val_loss did not improve\n",
      "2s - loss: 0.4949 - acc: 0.8141 - val_loss: 0.4338 - val_acc: 0.8338\n",
      "Epoch 572/1500\n",
      "Epoch 00571: val_loss did not improve\n",
      "2s - loss: 0.4640 - acc: 0.8264 - val_loss: 0.4401 - val_acc: 0.8489\n",
      "Epoch 573/1500\n",
      "Epoch 00572: val_loss did not improve\n",
      "2s - loss: 0.5044 - acc: 0.8152 - val_loss: 0.4924 - val_acc: 0.8278\n",
      "Epoch 574/1500\n",
      "Epoch 00573: val_loss did not improve\n",
      "2s - loss: 0.4978 - acc: 0.8123 - val_loss: 0.4365 - val_acc: 0.8369\n",
      "Epoch 575/1500\n",
      "Epoch 00574: val_loss did not improve\n",
      "2s - loss: 0.4853 - acc: 0.8199 - val_loss: 0.4463 - val_acc: 0.8066\n",
      "Epoch 576/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00575: val_loss did not improve\n",
      "2s - loss: 0.5148 - acc: 0.8177 - val_loss: 0.4345 - val_acc: 0.8489\n",
      "Epoch 577/1500\n",
      "Epoch 00576: val_loss did not improve\n",
      "2s - loss: 0.4522 - acc: 0.8390 - val_loss: 0.4740 - val_acc: 0.8338\n",
      "Epoch 578/1500\n",
      "Epoch 00577: val_loss did not improve\n",
      "2s - loss: 0.4978 - acc: 0.8175 - val_loss: 0.4468 - val_acc: 0.8429\n",
      "Epoch 579/1500\n",
      "Epoch 00578: val_loss did not improve\n",
      "2s - loss: 0.4630 - acc: 0.8325 - val_loss: 0.4327 - val_acc: 0.8308\n",
      "Epoch 580/1500\n",
      "Epoch 00579: val_loss did not improve\n",
      "2s - loss: 0.4792 - acc: 0.8264 - val_loss: 0.4521 - val_acc: 0.8369\n",
      "Epoch 581/1500\n",
      "Epoch 00580: val_loss did not improve\n",
      "2s - loss: 0.4632 - acc: 0.8454 - val_loss: 0.5443 - val_acc: 0.8006\n",
      "Epoch 582/1500\n",
      "Epoch 00581: val_loss did not improve\n",
      "2s - loss: 0.4972 - acc: 0.8158 - val_loss: 0.4242 - val_acc: 0.8489\n",
      "Epoch 583/1500\n",
      "Epoch 00582: val_loss did not improve\n",
      "2s - loss: 0.5085 - acc: 0.8066 - val_loss: 0.4949 - val_acc: 0.8248\n",
      "Epoch 584/1500\n",
      "Epoch 00583: val_loss did not improve\n",
      "2s - loss: 0.4926 - acc: 0.8258 - val_loss: 0.5027 - val_acc: 0.8187\n",
      "Epoch 585/1500\n",
      "Epoch 00584: val_loss did not improve\n",
      "2s - loss: 0.4924 - acc: 0.8098 - val_loss: 0.4743 - val_acc: 0.8097\n",
      "Epoch 586/1500\n",
      "Epoch 00585: val_loss did not improve\n",
      "2s - loss: 0.4572 - acc: 0.8295 - val_loss: 0.4298 - val_acc: 0.8369\n",
      "Epoch 587/1500\n",
      "Epoch 00586: val_loss improved from 0.40637 to 0.39841, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.4851 - acc: 0.8176 - val_loss: 0.3984 - val_acc: 0.8671\n",
      "Epoch 588/1500\n",
      "Epoch 00587: val_loss did not improve\n",
      "2s - loss: 0.4565 - acc: 0.8328 - val_loss: 0.4776 - val_acc: 0.8308\n",
      "Epoch 589/1500\n",
      "Epoch 00588: val_loss did not improve\n",
      "2s - loss: 0.4515 - acc: 0.8414 - val_loss: 0.5023 - val_acc: 0.8157\n",
      "Epoch 590/1500\n",
      "Epoch 00589: val_loss did not improve\n",
      "2s - loss: 0.4990 - acc: 0.8040 - val_loss: 0.4619 - val_acc: 0.8066\n",
      "Epoch 591/1500\n",
      "Epoch 00590: val_loss did not improve\n",
      "2s - loss: 0.4722 - acc: 0.8239 - val_loss: 0.4586 - val_acc: 0.8218\n",
      "Epoch 592/1500\n",
      "Epoch 00591: val_loss did not improve\n",
      "2s - loss: 0.4651 - acc: 0.8235 - val_loss: 0.5315 - val_acc: 0.7976\n",
      "Epoch 593/1500\n",
      "Epoch 00592: val_loss did not improve\n",
      "2s - loss: 0.5016 - acc: 0.8102 - val_loss: 0.4365 - val_acc: 0.8278\n",
      "Epoch 594/1500\n",
      "Epoch 00593: val_loss did not improve\n",
      "2s - loss: 0.5274 - acc: 0.8057 - val_loss: 0.4757 - val_acc: 0.8308\n",
      "Epoch 595/1500\n",
      "Epoch 00594: val_loss did not improve\n",
      "2s - loss: 0.4896 - acc: 0.8149 - val_loss: 0.5022 - val_acc: 0.8278\n",
      "Epoch 596/1500\n",
      "Epoch 00595: val_loss did not improve\n",
      "2s - loss: 0.4766 - acc: 0.8252 - val_loss: 0.4604 - val_acc: 0.8338\n",
      "Epoch 597/1500\n",
      "Epoch 00596: val_loss did not improve\n",
      "2s - loss: 0.4385 - acc: 0.8394 - val_loss: 0.4332 - val_acc: 0.8520\n",
      "Epoch 598/1500\n",
      "Epoch 00597: val_loss did not improve\n",
      "2s - loss: 0.4993 - acc: 0.8205 - val_loss: 0.4029 - val_acc: 0.8399\n",
      "Epoch 599/1500\n",
      "Epoch 00598: val_loss did not improve\n",
      "2s - loss: 0.4679 - acc: 0.8201 - val_loss: 0.4115 - val_acc: 0.8550\n",
      "Epoch 600/1500\n",
      "Epoch 00599: val_loss did not improve\n",
      "2s - loss: 0.4995 - acc: 0.8205 - val_loss: 0.4148 - val_acc: 0.8610\n",
      "Epoch 601/1500\n",
      "Epoch 00600: val_loss did not improve\n",
      "2s - loss: 0.4535 - acc: 0.8288 - val_loss: 0.4585 - val_acc: 0.8127\n",
      "Epoch 602/1500\n",
      "Epoch 00601: val_loss did not improve\n",
      "2s - loss: 0.4443 - acc: 0.8321 - val_loss: 0.5004 - val_acc: 0.8066\n",
      "Epoch 603/1500\n",
      "Epoch 00602: val_loss did not improve\n",
      "2s - loss: 0.4848 - acc: 0.8271 - val_loss: 0.4722 - val_acc: 0.8369\n",
      "Epoch 604/1500\n",
      "Epoch 00603: val_loss did not improve\n",
      "2s - loss: 0.4894 - acc: 0.8202 - val_loss: 0.4418 - val_acc: 0.8308\n",
      "Epoch 605/1500\n",
      "Epoch 00604: val_loss did not improve\n",
      "2s - loss: 0.4747 - acc: 0.8289 - val_loss: 0.4456 - val_acc: 0.8369\n",
      "Epoch 606/1500\n",
      "Epoch 00605: val_loss did not improve\n",
      "2s - loss: 0.4759 - acc: 0.8278 - val_loss: 0.4696 - val_acc: 0.7915\n",
      "Epoch 607/1500\n",
      "Epoch 00606: val_loss did not improve\n",
      "2s - loss: 0.4944 - acc: 0.8161 - val_loss: 0.4288 - val_acc: 0.8338\n",
      "Epoch 608/1500\n",
      "Epoch 00607: val_loss did not improve\n",
      "2s - loss: 0.5027 - acc: 0.8179 - val_loss: 0.4320 - val_acc: 0.8489\n",
      "Epoch 609/1500\n",
      "Epoch 00608: val_loss improved from 0.39841 to 0.38970, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.4779 - acc: 0.8414 - val_loss: 0.3897 - val_acc: 0.8580\n",
      "Epoch 610/1500\n",
      "Epoch 00609: val_loss did not improve\n",
      "2s - loss: 0.4197 - acc: 0.8382 - val_loss: 0.4293 - val_acc: 0.8278\n",
      "Epoch 611/1500\n",
      "Epoch 00610: val_loss did not improve\n",
      "2s - loss: 0.4919 - acc: 0.8318 - val_loss: 0.4155 - val_acc: 0.8580\n",
      "Epoch 612/1500\n",
      "Epoch 00611: val_loss did not improve\n",
      "2s - loss: 0.4931 - acc: 0.8230 - val_loss: 0.4754 - val_acc: 0.8459\n",
      "Epoch 613/1500\n",
      "Epoch 00612: val_loss did not improve\n",
      "2s - loss: 0.4711 - acc: 0.8319 - val_loss: 0.4275 - val_acc: 0.8520\n",
      "Epoch 614/1500\n",
      "Epoch 00613: val_loss did not improve\n",
      "2s - loss: 0.4638 - acc: 0.8280 - val_loss: 0.4110 - val_acc: 0.8640\n",
      "Epoch 615/1500\n",
      "Epoch 00614: val_loss did not improve\n",
      "2s - loss: 0.4535 - acc: 0.8336 - val_loss: 0.4235 - val_acc: 0.8550\n",
      "Epoch 616/1500\n",
      "Epoch 00615: val_loss did not improve\n",
      "2s - loss: 0.4593 - acc: 0.8280 - val_loss: 0.4287 - val_acc: 0.8580\n",
      "Epoch 617/1500\n",
      "Epoch 00616: val_loss did not improve\n",
      "2s - loss: 0.4718 - acc: 0.8230 - val_loss: 0.4288 - val_acc: 0.8459\n",
      "Epoch 618/1500\n",
      "Epoch 00617: val_loss did not improve\n",
      "2s - loss: 0.4669 - acc: 0.8273 - val_loss: 0.4764 - val_acc: 0.8338\n",
      "Epoch 619/1500\n",
      "Epoch 00618: val_loss did not improve\n",
      "2s - loss: 0.4432 - acc: 0.8288 - val_loss: 0.5010 - val_acc: 0.8157\n",
      "Epoch 620/1500\n",
      "Epoch 00619: val_loss did not improve\n",
      "2s - loss: 0.4383 - acc: 0.8327 - val_loss: 0.4322 - val_acc: 0.8308\n",
      "Epoch 621/1500\n",
      "Epoch 00620: val_loss did not improve\n",
      "2s - loss: 0.4516 - acc: 0.8263 - val_loss: 0.4817 - val_acc: 0.8248\n",
      "Epoch 622/1500\n",
      "Epoch 00621: val_loss did not improve\n",
      "2s - loss: 0.4697 - acc: 0.8126 - val_loss: 0.4127 - val_acc: 0.8459\n",
      "Epoch 623/1500\n",
      "Epoch 00622: val_loss did not improve\n",
      "2s - loss: 0.4263 - acc: 0.8519 - val_loss: 0.3953 - val_acc: 0.8640\n",
      "Epoch 624/1500\n",
      "Epoch 00623: val_loss did not improve\n",
      "2s - loss: 0.4123 - acc: 0.8430 - val_loss: 0.4730 - val_acc: 0.8248\n",
      "Epoch 625/1500\n",
      "Epoch 00624: val_loss did not improve\n",
      "2s - loss: 0.4547 - acc: 0.8322 - val_loss: 0.4524 - val_acc: 0.8489\n",
      "Epoch 626/1500\n",
      "Epoch 00625: val_loss did not improve\n",
      "2s - loss: 0.4475 - acc: 0.8480 - val_loss: 0.3987 - val_acc: 0.8701\n",
      "Epoch 627/1500\n",
      "Epoch 00626: val_loss did not improve\n",
      "2s - loss: 0.4666 - acc: 0.8342 - val_loss: 0.4284 - val_acc: 0.8459\n",
      "Epoch 628/1500\n",
      "Epoch 00627: val_loss did not improve\n",
      "2s - loss: 0.5049 - acc: 0.8204 - val_loss: 0.4774 - val_acc: 0.8399\n",
      "Epoch 629/1500\n",
      "Epoch 00628: val_loss did not improve\n",
      "2s - loss: 0.4371 - acc: 0.8454 - val_loss: 0.4217 - val_acc: 0.8610\n",
      "Epoch 630/1500\n",
      "Epoch 00629: val_loss did not improve\n",
      "2s - loss: 0.4618 - acc: 0.8397 - val_loss: 0.4395 - val_acc: 0.8399\n",
      "Epoch 631/1500\n",
      "Epoch 00630: val_loss did not improve\n",
      "2s - loss: 0.4312 - acc: 0.8400 - val_loss: 0.5348 - val_acc: 0.8218\n",
      "Epoch 632/1500\n",
      "Epoch 00631: val_loss did not improve\n",
      "2s - loss: 0.4600 - acc: 0.8319 - val_loss: 0.5055 - val_acc: 0.8157\n",
      "Epoch 633/1500\n",
      "Epoch 00632: val_loss did not improve\n",
      "2s - loss: 0.4775 - acc: 0.8280 - val_loss: 0.6271 - val_acc: 0.7825\n",
      "Epoch 634/1500\n",
      "Epoch 00633: val_loss did not improve\n",
      "2s - loss: 0.4554 - acc: 0.8144 - val_loss: 0.4648 - val_acc: 0.8278\n",
      "Epoch 635/1500\n",
      "Epoch 00634: val_loss did not improve\n",
      "2s - loss: 0.4680 - acc: 0.8415 - val_loss: 0.4237 - val_acc: 0.8459\n",
      "Epoch 636/1500\n",
      "Epoch 00635: val_loss did not improve\n",
      "2s - loss: 0.4520 - acc: 0.8248 - val_loss: 0.3984 - val_acc: 0.8580\n",
      "Epoch 637/1500\n",
      "Epoch 00636: val_loss did not improve\n",
      "2s - loss: 0.4338 - acc: 0.8352 - val_loss: 0.4567 - val_acc: 0.8550\n",
      "Epoch 638/1500\n",
      "Epoch 00637: val_loss improved from 0.38970 to 0.37334, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.4390 - acc: 0.8384 - val_loss: 0.3733 - val_acc: 0.8610\n",
      "Epoch 639/1500\n",
      "Epoch 00638: val_loss did not improve\n",
      "2s - loss: 0.4693 - acc: 0.8262 - val_loss: 0.3886 - val_acc: 0.8550\n",
      "Epoch 640/1500\n",
      "Epoch 00639: val_loss did not improve\n",
      "2s - loss: 0.4426 - acc: 0.8255 - val_loss: 0.4080 - val_acc: 0.8610\n",
      "Epoch 641/1500\n",
      "Epoch 00640: val_loss did not improve\n",
      "2s - loss: 0.4613 - acc: 0.8295 - val_loss: 0.4001 - val_acc: 0.8550\n",
      "Epoch 642/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00641: val_loss improved from 0.37334 to 0.37117, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.4614 - acc: 0.8311 - val_loss: 0.3712 - val_acc: 0.8580\n",
      "Epoch 643/1500\n",
      "Epoch 00642: val_loss did not improve\n",
      "2s - loss: 0.4406 - acc: 0.8367 - val_loss: 0.4276 - val_acc: 0.8580\n",
      "Epoch 644/1500\n",
      "Epoch 00643: val_loss did not improve\n",
      "2s - loss: 0.4513 - acc: 0.8209 - val_loss: 0.3840 - val_acc: 0.8822\n",
      "Epoch 645/1500\n",
      "Epoch 00644: val_loss did not improve\n",
      "2s - loss: 0.4489 - acc: 0.8336 - val_loss: 0.3996 - val_acc: 0.8701\n",
      "Epoch 646/1500\n",
      "Epoch 00645: val_loss did not improve\n",
      "2s - loss: 0.5118 - acc: 0.8148 - val_loss: 0.3871 - val_acc: 0.8701\n",
      "Epoch 647/1500\n",
      "Epoch 00646: val_loss did not improve\n",
      "2s - loss: 0.4363 - acc: 0.8382 - val_loss: 0.4740 - val_acc: 0.8338\n",
      "Epoch 648/1500\n",
      "Epoch 00647: val_loss did not improve\n",
      "2s - loss: 0.4561 - acc: 0.8399 - val_loss: 0.4243 - val_acc: 0.8731\n",
      "Epoch 649/1500\n",
      "Epoch 00648: val_loss did not improve\n",
      "2s - loss: 0.4489 - acc: 0.8400 - val_loss: 0.4496 - val_acc: 0.8459\n",
      "Epoch 650/1500\n",
      "Epoch 00649: val_loss did not improve\n",
      "2s - loss: 0.4605 - acc: 0.8335 - val_loss: 0.4041 - val_acc: 0.8792\n",
      "Epoch 651/1500\n",
      "Epoch 00650: val_loss did not improve\n",
      "2s - loss: 0.4468 - acc: 0.8303 - val_loss: 0.4249 - val_acc: 0.8550\n",
      "Epoch 652/1500\n",
      "Epoch 00651: val_loss did not improve\n",
      "2s - loss: 0.4883 - acc: 0.8242 - val_loss: 0.4537 - val_acc: 0.8127\n",
      "Epoch 653/1500\n",
      "Epoch 00652: val_loss did not improve\n",
      "2s - loss: 0.4454 - acc: 0.8279 - val_loss: 0.4610 - val_acc: 0.8369\n",
      "Epoch 654/1500\n",
      "Epoch 00653: val_loss did not improve\n",
      "2s - loss: 0.4515 - acc: 0.8294 - val_loss: 0.4142 - val_acc: 0.8520\n",
      "Epoch 655/1500\n",
      "Epoch 00654: val_loss did not improve\n",
      "2s - loss: 0.4373 - acc: 0.8480 - val_loss: 0.4380 - val_acc: 0.8459\n",
      "Epoch 656/1500\n",
      "Epoch 00655: val_loss did not improve\n",
      "2s - loss: 0.4508 - acc: 0.8361 - val_loss: 0.5022 - val_acc: 0.8218\n",
      "Epoch 657/1500\n",
      "Epoch 00656: val_loss did not improve\n",
      "2s - loss: 0.4633 - acc: 0.8377 - val_loss: 0.4463 - val_acc: 0.8399\n",
      "Epoch 658/1500\n",
      "Epoch 00657: val_loss did not improve\n",
      "2s - loss: 0.4321 - acc: 0.8440 - val_loss: 0.3982 - val_acc: 0.8550\n",
      "Epoch 659/1500\n",
      "Epoch 00658: val_loss improved from 0.37117 to 0.35040, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.4329 - acc: 0.8549 - val_loss: 0.3504 - val_acc: 0.8792\n",
      "Epoch 660/1500\n",
      "Epoch 00659: val_loss did not improve\n",
      "2s - loss: 0.4237 - acc: 0.8415 - val_loss: 0.4555 - val_acc: 0.8248\n",
      "Epoch 661/1500\n",
      "Epoch 00660: val_loss did not improve\n",
      "2s - loss: 0.4377 - acc: 0.8421 - val_loss: 0.4360 - val_acc: 0.8550\n",
      "Epoch 662/1500\n",
      "Epoch 00661: val_loss did not improve\n",
      "2s - loss: 0.4291 - acc: 0.8407 - val_loss: 0.3990 - val_acc: 0.8550\n",
      "Epoch 663/1500\n",
      "Epoch 00662: val_loss did not improve\n",
      "2s - loss: 0.4241 - acc: 0.8462 - val_loss: 0.4780 - val_acc: 0.8369\n",
      "Epoch 664/1500\n",
      "Epoch 00663: val_loss did not improve\n",
      "2s - loss: 0.4571 - acc: 0.8288 - val_loss: 0.4673 - val_acc: 0.8127\n",
      "Epoch 665/1500\n",
      "Epoch 00664: val_loss did not improve\n",
      "2s - loss: 0.4492 - acc: 0.8369 - val_loss: 0.4199 - val_acc: 0.8308\n",
      "Epoch 666/1500\n",
      "Epoch 00665: val_loss did not improve\n",
      "2s - loss: 0.3861 - acc: 0.8486 - val_loss: 0.4623 - val_acc: 0.8459\n",
      "Epoch 667/1500\n",
      "Epoch 00666: val_loss did not improve\n",
      "2s - loss: 0.4275 - acc: 0.8391 - val_loss: 0.4167 - val_acc: 0.8489\n",
      "Epoch 668/1500\n",
      "Epoch 00667: val_loss did not improve\n",
      "2s - loss: 0.3977 - acc: 0.8445 - val_loss: 0.4374 - val_acc: 0.8489\n",
      "Epoch 669/1500\n",
      "Epoch 00668: val_loss did not improve\n",
      "2s - loss: 0.4148 - acc: 0.8511 - val_loss: 0.4454 - val_acc: 0.8399\n",
      "Epoch 670/1500\n",
      "Epoch 00669: val_loss did not improve\n",
      "2s - loss: 0.4161 - acc: 0.8440 - val_loss: 0.4425 - val_acc: 0.8338\n",
      "Epoch 671/1500\n",
      "Epoch 00670: val_loss did not improve\n",
      "2s - loss: 0.4277 - acc: 0.8463 - val_loss: 0.4175 - val_acc: 0.8580\n",
      "Epoch 672/1500\n",
      "Epoch 00671: val_loss did not improve\n",
      "2s - loss: 0.4285 - acc: 0.8448 - val_loss: 0.4916 - val_acc: 0.8369\n",
      "Epoch 673/1500\n",
      "Epoch 00672: val_loss did not improve\n",
      "2s - loss: 0.4250 - acc: 0.8430 - val_loss: 0.3988 - val_acc: 0.8550\n",
      "Epoch 674/1500\n",
      "Epoch 00673: val_loss did not improve\n",
      "2s - loss: 0.4784 - acc: 0.8253 - val_loss: 0.4949 - val_acc: 0.8399\n",
      "Epoch 675/1500\n",
      "Epoch 00674: val_loss did not improve\n",
      "2s - loss: 0.4626 - acc: 0.8287 - val_loss: 0.4101 - val_acc: 0.8640\n",
      "Epoch 676/1500\n",
      "Epoch 00675: val_loss did not improve\n",
      "2s - loss: 0.4456 - acc: 0.8330 - val_loss: 0.3664 - val_acc: 0.8610\n",
      "Epoch 677/1500\n",
      "Epoch 00676: val_loss did not improve\n",
      "2s - loss: 0.3992 - acc: 0.8543 - val_loss: 0.4280 - val_acc: 0.8610\n",
      "Epoch 678/1500\n",
      "Epoch 00677: val_loss did not improve\n",
      "2s - loss: 0.3908 - acc: 0.8575 - val_loss: 0.4677 - val_acc: 0.8489\n",
      "Epoch 679/1500\n",
      "Epoch 00678: val_loss did not improve\n",
      "2s - loss: 0.3959 - acc: 0.8472 - val_loss: 0.4031 - val_acc: 0.8369\n",
      "Epoch 680/1500\n",
      "Epoch 00679: val_loss did not improve\n",
      "2s - loss: 0.4211 - acc: 0.8535 - val_loss: 0.4532 - val_acc: 0.8187\n",
      "Epoch 681/1500\n",
      "Epoch 00680: val_loss did not improve\n",
      "2s - loss: 0.4118 - acc: 0.8477 - val_loss: 0.4874 - val_acc: 0.8218\n",
      "Epoch 682/1500\n",
      "Epoch 00681: val_loss did not improve\n",
      "2s - loss: 0.4274 - acc: 0.8512 - val_loss: 0.4574 - val_acc: 0.8369\n",
      "Epoch 683/1500\n",
      "Epoch 00682: val_loss did not improve\n",
      "2s - loss: 0.4569 - acc: 0.8407 - val_loss: 0.4863 - val_acc: 0.8338\n",
      "Epoch 684/1500\n",
      "Epoch 00683: val_loss did not improve\n",
      "2s - loss: 0.4207 - acc: 0.8391 - val_loss: 0.4570 - val_acc: 0.8187\n",
      "Epoch 685/1500\n",
      "Epoch 00684: val_loss did not improve\n",
      "2s - loss: 0.4263 - acc: 0.8423 - val_loss: 0.4685 - val_acc: 0.8278\n",
      "Epoch 686/1500\n",
      "Epoch 00685: val_loss did not improve\n",
      "2s - loss: 0.4136 - acc: 0.8454 - val_loss: 0.4380 - val_acc: 0.8338\n",
      "Epoch 687/1500\n",
      "Epoch 00686: val_loss did not improve\n",
      "2s - loss: 0.4487 - acc: 0.8398 - val_loss: 0.4118 - val_acc: 0.8640\n",
      "Epoch 688/1500\n",
      "Epoch 00687: val_loss did not improve\n",
      "2s - loss: 0.4279 - acc: 0.8527 - val_loss: 0.5424 - val_acc: 0.8097\n",
      "Epoch 689/1500\n",
      "Epoch 00688: val_loss did not improve\n",
      "2s - loss: 0.4334 - acc: 0.8330 - val_loss: 0.3579 - val_acc: 0.8550\n",
      "Epoch 690/1500\n",
      "Epoch 00689: val_loss did not improve\n",
      "2s - loss: 0.4726 - acc: 0.8305 - val_loss: 0.4244 - val_acc: 0.8520\n",
      "Epoch 691/1500\n",
      "Epoch 00690: val_loss did not improve\n",
      "2s - loss: 0.4241 - acc: 0.8408 - val_loss: 0.5034 - val_acc: 0.8248\n",
      "Epoch 692/1500\n",
      "Epoch 00691: val_loss did not improve\n",
      "2s - loss: 0.4617 - acc: 0.8232 - val_loss: 0.4871 - val_acc: 0.8278\n",
      "Epoch 693/1500\n",
      "Epoch 00692: val_loss improved from 0.35040 to 0.30823, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.4340 - acc: 0.8369 - val_loss: 0.3082 - val_acc: 0.9003\n",
      "Epoch 694/1500\n",
      "Epoch 00693: val_loss did not improve\n",
      "2s - loss: 0.3975 - acc: 0.8589 - val_loss: 0.4377 - val_acc: 0.8520\n",
      "Epoch 695/1500\n",
      "Epoch 00694: val_loss did not improve\n",
      "2s - loss: 0.4293 - acc: 0.8408 - val_loss: 0.4012 - val_acc: 0.8610\n",
      "Epoch 696/1500\n",
      "Epoch 00695: val_loss did not improve\n",
      "2s - loss: 0.4113 - acc: 0.8479 - val_loss: 0.3419 - val_acc: 0.8580\n",
      "Epoch 697/1500\n",
      "Epoch 00696: val_loss did not improve\n",
      "2s - loss: 0.4008 - acc: 0.8542 - val_loss: 0.4548 - val_acc: 0.8489\n",
      "Epoch 698/1500\n",
      "Epoch 00697: val_loss did not improve\n",
      "2s - loss: 0.4159 - acc: 0.8464 - val_loss: 0.3879 - val_acc: 0.8671\n",
      "Epoch 699/1500\n",
      "Epoch 00698: val_loss did not improve\n",
      "2s - loss: 0.4621 - acc: 0.8281 - val_loss: 0.3704 - val_acc: 0.8640\n",
      "Epoch 700/1500\n",
      "Epoch 00699: val_loss did not improve\n",
      "2s - loss: 0.4412 - acc: 0.8453 - val_loss: 0.3465 - val_acc: 0.8701\n",
      "Epoch 701/1500\n",
      "Epoch 00700: val_loss did not improve\n",
      "2s - loss: 0.4024 - acc: 0.8575 - val_loss: 0.4144 - val_acc: 0.8489\n",
      "Epoch 702/1500\n",
      "Epoch 00701: val_loss did not improve\n",
      "2s - loss: 0.4441 - acc: 0.8456 - val_loss: 0.4444 - val_acc: 0.8399\n",
      "Epoch 703/1500\n",
      "Epoch 00702: val_loss did not improve\n",
      "2s - loss: 0.4193 - acc: 0.8448 - val_loss: 0.4619 - val_acc: 0.8520\n",
      "Epoch 704/1500\n",
      "Epoch 00703: val_loss did not improve\n",
      "2s - loss: 0.3861 - acc: 0.8487 - val_loss: 0.5319 - val_acc: 0.8097\n",
      "Epoch 705/1500\n",
      "Epoch 00704: val_loss did not improve\n",
      "2s - loss: 0.4262 - acc: 0.8424 - val_loss: 0.4090 - val_acc: 0.8429\n",
      "Epoch 706/1500\n",
      "Epoch 00705: val_loss did not improve\n",
      "2s - loss: 0.4600 - acc: 0.8303 - val_loss: 0.3865 - val_acc: 0.8610\n",
      "Epoch 707/1500\n",
      "Epoch 00706: val_loss did not improve\n",
      "2s - loss: 0.3887 - acc: 0.8598 - val_loss: 0.3601 - val_acc: 0.8671\n",
      "Epoch 708/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00707: val_loss did not improve\n",
      "2s - loss: 0.4096 - acc: 0.8477 - val_loss: 0.4105 - val_acc: 0.8580\n",
      "Epoch 709/1500\n",
      "Epoch 00708: val_loss did not improve\n",
      "2s - loss: 0.4460 - acc: 0.8470 - val_loss: 0.3853 - val_acc: 0.8671\n",
      "Epoch 710/1500\n",
      "Epoch 00709: val_loss did not improve\n",
      "2s - loss: 0.4475 - acc: 0.8386 - val_loss: 0.4151 - val_acc: 0.8580\n",
      "Epoch 711/1500\n",
      "Epoch 00710: val_loss did not improve\n",
      "2s - loss: 0.3984 - acc: 0.8558 - val_loss: 0.4566 - val_acc: 0.8580\n",
      "Epoch 712/1500\n",
      "Epoch 00711: val_loss did not improve\n",
      "2s - loss: 0.3849 - acc: 0.8550 - val_loss: 0.3883 - val_acc: 0.8580\n",
      "Epoch 713/1500\n",
      "Epoch 00712: val_loss did not improve\n",
      "2s - loss: 0.3650 - acc: 0.8688 - val_loss: 0.4026 - val_acc: 0.8701\n",
      "Epoch 714/1500\n",
      "Epoch 00713: val_loss did not improve\n",
      "2s - loss: 0.4024 - acc: 0.8535 - val_loss: 0.4749 - val_acc: 0.8036\n",
      "Epoch 715/1500\n",
      "Epoch 00714: val_loss did not improve\n",
      "2s - loss: 0.4159 - acc: 0.8526 - val_loss: 0.4322 - val_acc: 0.8429\n",
      "Epoch 716/1500\n",
      "Epoch 00715: val_loss did not improve\n",
      "2s - loss: 0.4137 - acc: 0.8431 - val_loss: 0.4636 - val_acc: 0.8550\n",
      "Epoch 717/1500\n",
      "Epoch 00716: val_loss did not improve\n",
      "2s - loss: 0.4250 - acc: 0.8290 - val_loss: 0.3964 - val_acc: 0.8218\n",
      "Epoch 718/1500\n",
      "Epoch 00717: val_loss did not improve\n",
      "2s - loss: 0.3932 - acc: 0.8576 - val_loss: 0.4310 - val_acc: 0.8399\n",
      "Epoch 719/1500\n",
      "Epoch 00718: val_loss did not improve\n",
      "2s - loss: 0.4199 - acc: 0.8500 - val_loss: 0.4257 - val_acc: 0.8278\n",
      "Epoch 720/1500\n",
      "Epoch 00719: val_loss did not improve\n",
      "2s - loss: 0.3767 - acc: 0.8645 - val_loss: 0.3469 - val_acc: 0.8761\n",
      "Epoch 721/1500\n",
      "Epoch 00720: val_loss did not improve\n",
      "2s - loss: 0.4116 - acc: 0.8414 - val_loss: 0.4280 - val_acc: 0.8429\n",
      "Epoch 722/1500\n",
      "Epoch 00721: val_loss did not improve\n",
      "2s - loss: 0.4415 - acc: 0.8368 - val_loss: 0.3943 - val_acc: 0.8520\n",
      "Epoch 723/1500\n",
      "Epoch 00722: val_loss did not improve\n",
      "2s - loss: 0.3849 - acc: 0.8542 - val_loss: 0.4139 - val_acc: 0.8671\n",
      "Epoch 724/1500\n",
      "Epoch 00723: val_loss did not improve\n",
      "2s - loss: 0.4104 - acc: 0.8625 - val_loss: 0.4756 - val_acc: 0.8369\n",
      "Epoch 725/1500\n",
      "Epoch 00724: val_loss did not improve\n",
      "2s - loss: 0.4249 - acc: 0.8303 - val_loss: 0.5068 - val_acc: 0.8459\n",
      "Epoch 726/1500\n",
      "Epoch 00725: val_loss did not improve\n",
      "2s - loss: 0.4525 - acc: 0.8373 - val_loss: 0.4079 - val_acc: 0.8459\n",
      "Epoch 727/1500\n",
      "Epoch 00726: val_loss did not improve\n",
      "2s - loss: 0.4035 - acc: 0.8453 - val_loss: 0.3806 - val_acc: 0.8731\n",
      "Epoch 728/1500\n",
      "Epoch 00727: val_loss did not improve\n",
      "2s - loss: 0.3688 - acc: 0.8641 - val_loss: 0.3965 - val_acc: 0.8459\n",
      "Epoch 729/1500\n",
      "Epoch 00728: val_loss did not improve\n",
      "2s - loss: 0.3961 - acc: 0.8504 - val_loss: 0.3888 - val_acc: 0.8610\n",
      "Epoch 730/1500\n",
      "Epoch 00729: val_loss did not improve\n",
      "2s - loss: 0.3952 - acc: 0.8551 - val_loss: 0.3937 - val_acc: 0.8640\n",
      "Epoch 731/1500\n",
      "Epoch 00730: val_loss did not improve\n",
      "2s - loss: 0.3849 - acc: 0.8513 - val_loss: 0.3863 - val_acc: 0.8580\n",
      "Epoch 732/1500\n",
      "Epoch 00731: val_loss did not improve\n",
      "2s - loss: 0.4062 - acc: 0.8629 - val_loss: 0.4429 - val_acc: 0.8489\n",
      "Epoch 733/1500\n",
      "Epoch 00732: val_loss did not improve\n",
      "2s - loss: 0.4189 - acc: 0.8542 - val_loss: 0.3983 - val_acc: 0.8761\n",
      "Epoch 734/1500\n",
      "Epoch 00733: val_loss did not improve\n",
      "2s - loss: 0.3968 - acc: 0.8488 - val_loss: 0.5188 - val_acc: 0.8248\n",
      "Epoch 735/1500\n",
      "Epoch 00734: val_loss did not improve\n",
      "2s - loss: 0.4651 - acc: 0.8306 - val_loss: 0.4355 - val_acc: 0.8550\n",
      "Epoch 736/1500\n",
      "Epoch 00735: val_loss did not improve\n",
      "2s - loss: 0.3618 - acc: 0.8696 - val_loss: 0.4176 - val_acc: 0.8520\n",
      "Epoch 737/1500\n",
      "Epoch 00736: val_loss did not improve\n",
      "2s - loss: 0.3987 - acc: 0.8599 - val_loss: 0.3983 - val_acc: 0.8671\n",
      "Epoch 738/1500\n",
      "Epoch 00737: val_loss did not improve\n",
      "2s - loss: 0.4095 - acc: 0.8575 - val_loss: 0.3974 - val_acc: 0.8489\n",
      "Epoch 739/1500\n",
      "Epoch 00738: val_loss did not improve\n",
      "2s - loss: 0.3921 - acc: 0.8471 - val_loss: 0.3767 - val_acc: 0.8701\n",
      "Epoch 740/1500\n",
      "Epoch 00739: val_loss did not improve\n",
      "2s - loss: 0.4313 - acc: 0.8494 - val_loss: 0.3924 - val_acc: 0.8580\n",
      "Epoch 741/1500\n",
      "Epoch 00740: val_loss did not improve\n",
      "2s - loss: 0.3967 - acc: 0.8559 - val_loss: 0.4475 - val_acc: 0.8489\n",
      "Epoch 742/1500\n",
      "Epoch 00741: val_loss did not improve\n",
      "2s - loss: 0.4329 - acc: 0.8407 - val_loss: 0.4843 - val_acc: 0.8399\n",
      "Epoch 743/1500\n",
      "Epoch 00742: val_loss did not improve\n",
      "2s - loss: 0.4156 - acc: 0.8563 - val_loss: 0.5047 - val_acc: 0.8399\n",
      "Epoch 744/1500\n",
      "Epoch 00743: val_loss did not improve\n",
      "2s - loss: 0.3687 - acc: 0.8672 - val_loss: 0.4285 - val_acc: 0.8429\n",
      "Epoch 745/1500\n",
      "Epoch 00744: val_loss did not improve\n",
      "2s - loss: 0.3854 - acc: 0.8567 - val_loss: 0.3704 - val_acc: 0.8792\n",
      "Epoch 746/1500\n",
      "Epoch 00745: val_loss did not improve\n",
      "2s - loss: 0.4112 - acc: 0.8439 - val_loss: 0.4162 - val_acc: 0.8520\n",
      "Epoch 747/1500\n",
      "Epoch 00746: val_loss did not improve\n",
      "2s - loss: 0.3661 - acc: 0.8702 - val_loss: 0.3888 - val_acc: 0.8489\n",
      "Epoch 748/1500\n",
      "Epoch 00747: val_loss did not improve\n",
      "2s - loss: 0.4061 - acc: 0.8448 - val_loss: 0.3995 - val_acc: 0.8489\n",
      "Epoch 749/1500\n",
      "Epoch 00748: val_loss did not improve\n",
      "2s - loss: 0.3650 - acc: 0.8618 - val_loss: 0.4140 - val_acc: 0.8459\n",
      "Epoch 750/1500\n",
      "Epoch 00749: val_loss did not improve\n",
      "2s - loss: 0.3808 - acc: 0.8543 - val_loss: 0.3468 - val_acc: 0.8610\n",
      "Epoch 751/1500\n",
      "Epoch 00750: val_loss did not improve\n",
      "2s - loss: 0.3797 - acc: 0.8634 - val_loss: 0.3358 - val_acc: 0.8610\n",
      "Epoch 752/1500\n",
      "Epoch 00751: val_loss did not improve\n",
      "2s - loss: 0.3959 - acc: 0.8575 - val_loss: 0.3492 - val_acc: 0.8640\n",
      "Epoch 753/1500\n",
      "Epoch 00752: val_loss did not improve\n",
      "2s - loss: 0.4012 - acc: 0.8480 - val_loss: 0.4249 - val_acc: 0.8520\n",
      "Epoch 754/1500\n",
      "Epoch 00753: val_loss did not improve\n",
      "2s - loss: 0.3830 - acc: 0.8567 - val_loss: 0.4505 - val_acc: 0.8369\n",
      "Epoch 755/1500\n",
      "Epoch 00754: val_loss did not improve\n",
      "2s - loss: 0.3769 - acc: 0.8630 - val_loss: 0.3735 - val_acc: 0.8610\n",
      "Epoch 756/1500\n",
      "Epoch 00755: val_loss did not improve\n",
      "2s - loss: 0.3607 - acc: 0.8686 - val_loss: 0.4544 - val_acc: 0.8459\n",
      "Epoch 757/1500\n",
      "Epoch 00756: val_loss did not improve\n",
      "2s - loss: 0.3730 - acc: 0.8650 - val_loss: 0.3493 - val_acc: 0.8520\n",
      "Epoch 758/1500\n",
      "Epoch 00757: val_loss did not improve\n",
      "2s - loss: 0.3915 - acc: 0.8560 - val_loss: 0.3868 - val_acc: 0.8792\n",
      "Epoch 759/1500\n",
      "Epoch 00758: val_loss did not improve\n",
      "2s - loss: 0.4101 - acc: 0.8507 - val_loss: 0.4331 - val_acc: 0.8580\n",
      "Epoch 760/1500\n",
      "Epoch 00759: val_loss did not improve\n",
      "2s - loss: 0.3644 - acc: 0.8617 - val_loss: 0.4098 - val_acc: 0.8369\n",
      "Epoch 761/1500\n",
      "Epoch 00760: val_loss did not improve\n",
      "2s - loss: 0.3685 - acc: 0.8680 - val_loss: 0.4063 - val_acc: 0.8701\n",
      "Epoch 762/1500\n",
      "Epoch 00761: val_loss did not improve\n",
      "2s - loss: 0.3934 - acc: 0.8690 - val_loss: 0.3589 - val_acc: 0.8640\n",
      "Epoch 763/1500\n",
      "Epoch 00762: val_loss did not improve\n",
      "2s - loss: 0.3822 - acc: 0.8615 - val_loss: 0.3897 - val_acc: 0.8671\n",
      "Epoch 764/1500\n",
      "Epoch 00763: val_loss did not improve\n",
      "2s - loss: 0.3570 - acc: 0.8665 - val_loss: 0.3359 - val_acc: 0.8882\n",
      "Epoch 765/1500\n",
      "Epoch 00764: val_loss did not improve\n",
      "2s - loss: 0.3660 - acc: 0.8688 - val_loss: 0.3613 - val_acc: 0.8640\n",
      "Epoch 766/1500\n",
      "Epoch 00765: val_loss did not improve\n",
      "2s - loss: 0.3775 - acc: 0.8584 - val_loss: 0.3577 - val_acc: 0.8671\n",
      "Epoch 767/1500\n",
      "Epoch 00766: val_loss did not improve\n",
      "2s - loss: 0.4151 - acc: 0.8400 - val_loss: 0.3498 - val_acc: 0.8701\n",
      "Epoch 768/1500\n",
      "Epoch 00767: val_loss did not improve\n",
      "2s - loss: 0.4017 - acc: 0.8567 - val_loss: 0.4447 - val_acc: 0.8459\n",
      "Epoch 769/1500\n",
      "Epoch 00768: val_loss did not improve\n",
      "2s - loss: 0.3561 - acc: 0.8661 - val_loss: 0.3872 - val_acc: 0.8852\n",
      "Epoch 770/1500\n",
      "Epoch 00769: val_loss did not improve\n",
      "2s - loss: 0.3472 - acc: 0.8744 - val_loss: 0.3588 - val_acc: 0.8671\n",
      "Epoch 771/1500\n",
      "Epoch 00770: val_loss did not improve\n",
      "2s - loss: 0.3759 - acc: 0.8617 - val_loss: 0.3617 - val_acc: 0.8882\n",
      "Epoch 772/1500\n",
      "Epoch 00771: val_loss did not improve\n",
      "2s - loss: 0.4537 - acc: 0.8425 - val_loss: 0.4044 - val_acc: 0.8429\n",
      "Epoch 773/1500\n",
      "Epoch 00772: val_loss did not improve\n",
      "2s - loss: 0.3520 - acc: 0.8816 - val_loss: 0.4045 - val_acc: 0.8580\n",
      "Epoch 774/1500\n",
      "Epoch 00773: val_loss did not improve\n",
      "2s - loss: 0.4218 - acc: 0.8489 - val_loss: 0.4452 - val_acc: 0.8369\n",
      "Epoch 775/1500\n",
      "Epoch 00774: val_loss did not improve\n",
      "2s - loss: 0.3875 - acc: 0.8591 - val_loss: 0.3246 - val_acc: 0.8882\n",
      "Epoch 776/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00775: val_loss did not improve\n",
      "2s - loss: 0.4011 - acc: 0.8578 - val_loss: 0.3767 - val_acc: 0.8671\n",
      "Epoch 777/1500\n",
      "Epoch 00776: val_loss did not improve\n",
      "2s - loss: 0.4293 - acc: 0.8486 - val_loss: 0.3712 - val_acc: 0.8701\n",
      "Epoch 778/1500\n",
      "Epoch 00777: val_loss did not improve\n",
      "2s - loss: 0.4509 - acc: 0.8306 - val_loss: 0.5021 - val_acc: 0.8218\n",
      "Epoch 779/1500\n",
      "Epoch 00778: val_loss did not improve\n",
      "2s - loss: 0.3921 - acc: 0.8586 - val_loss: 0.4446 - val_acc: 0.8550\n",
      "Epoch 780/1500\n",
      "Epoch 00779: val_loss did not improve\n",
      "2s - loss: 0.4179 - acc: 0.8590 - val_loss: 0.3434 - val_acc: 0.8792\n",
      "Epoch 781/1500\n",
      "Epoch 00780: val_loss did not improve\n",
      "2s - loss: 0.3651 - acc: 0.8598 - val_loss: 0.3710 - val_acc: 0.8520\n",
      "Epoch 782/1500\n",
      "Epoch 00781: val_loss did not improve\n",
      "2s - loss: 0.3844 - acc: 0.8574 - val_loss: 0.4071 - val_acc: 0.8580\n",
      "Epoch 783/1500\n",
      "Epoch 00782: val_loss did not improve\n",
      "2s - loss: 0.3828 - acc: 0.8630 - val_loss: 0.3705 - val_acc: 0.8792\n",
      "Epoch 784/1500\n",
      "Epoch 00783: val_loss did not improve\n",
      "2s - loss: 0.3912 - acc: 0.8622 - val_loss: 0.3329 - val_acc: 0.8852\n",
      "Epoch 785/1500\n",
      "Epoch 00784: val_loss did not improve\n",
      "2s - loss: 0.3927 - acc: 0.8510 - val_loss: 0.5126 - val_acc: 0.8338\n",
      "Epoch 786/1500\n",
      "Epoch 00785: val_loss did not improve\n",
      "2s - loss: 0.3760 - acc: 0.8622 - val_loss: 0.3710 - val_acc: 0.8761\n",
      "Epoch 787/1500\n",
      "Epoch 00786: val_loss did not improve\n",
      "2s - loss: 0.3719 - acc: 0.8591 - val_loss: 0.3212 - val_acc: 0.8943\n",
      "Epoch 788/1500\n",
      "Epoch 00787: val_loss did not improve\n",
      "2s - loss: 0.4216 - acc: 0.8425 - val_loss: 0.3670 - val_acc: 0.8792\n",
      "Epoch 789/1500\n",
      "Epoch 00788: val_loss did not improve\n",
      "2s - loss: 0.3539 - acc: 0.8678 - val_loss: 0.3316 - val_acc: 0.8761\n",
      "Epoch 790/1500\n",
      "Epoch 00789: val_loss did not improve\n",
      "2s - loss: 0.3676 - acc: 0.8693 - val_loss: 0.4021 - val_acc: 0.8640\n",
      "Epoch 791/1500\n",
      "Epoch 00790: val_loss did not improve\n",
      "2s - loss: 0.3623 - acc: 0.8583 - val_loss: 0.3872 - val_acc: 0.8459\n",
      "Epoch 792/1500\n",
      "Epoch 00791: val_loss did not improve\n",
      "2s - loss: 0.4008 - acc: 0.8555 - val_loss: 0.4563 - val_acc: 0.8640\n",
      "Epoch 793/1500\n",
      "Epoch 00792: val_loss did not improve\n",
      "2s - loss: 0.4458 - acc: 0.8513 - val_loss: 0.3801 - val_acc: 0.8550\n",
      "Epoch 794/1500\n",
      "Epoch 00793: val_loss did not improve\n",
      "2s - loss: 0.3791 - acc: 0.8664 - val_loss: 0.3900 - val_acc: 0.8640\n",
      "Epoch 795/1500\n",
      "Epoch 00794: val_loss did not improve\n",
      "2s - loss: 0.3783 - acc: 0.8535 - val_loss: 0.4003 - val_acc: 0.8550\n",
      "Epoch 796/1500\n",
      "Epoch 00795: val_loss did not improve\n",
      "2s - loss: 0.3687 - acc: 0.8678 - val_loss: 0.4029 - val_acc: 0.8489\n",
      "Epoch 797/1500\n",
      "Epoch 00796: val_loss did not improve\n",
      "2s - loss: 0.3477 - acc: 0.8798 - val_loss: 0.3693 - val_acc: 0.8882\n",
      "Epoch 798/1500\n",
      "Epoch 00797: val_loss did not improve\n",
      "2s - loss: 0.3713 - acc: 0.8754 - val_loss: 0.3683 - val_acc: 0.8520\n",
      "Epoch 799/1500\n",
      "Epoch 00798: val_loss did not improve\n",
      "2s - loss: 0.3674 - acc: 0.8745 - val_loss: 0.3330 - val_acc: 0.8822\n",
      "Epoch 800/1500\n",
      "Epoch 00799: val_loss did not improve\n",
      "2s - loss: 0.3701 - acc: 0.8653 - val_loss: 0.4021 - val_acc: 0.8671\n",
      "Epoch 801/1500\n",
      "Epoch 00800: val_loss did not improve\n",
      "2s - loss: 0.4125 - acc: 0.8519 - val_loss: 0.3593 - val_acc: 0.8520\n",
      "Epoch 802/1500\n",
      "Epoch 00801: val_loss did not improve\n",
      "2s - loss: 0.3604 - acc: 0.8686 - val_loss: 0.3952 - val_acc: 0.8761\n",
      "Epoch 803/1500\n",
      "Epoch 00802: val_loss did not improve\n",
      "2s - loss: 0.3637 - acc: 0.8575 - val_loss: 0.3822 - val_acc: 0.8640\n",
      "Epoch 804/1500\n",
      "Epoch 00803: val_loss did not improve\n",
      "2s - loss: 0.3959 - acc: 0.8547 - val_loss: 0.3645 - val_acc: 0.8822\n",
      "Epoch 805/1500\n",
      "Epoch 00804: val_loss did not improve\n",
      "2s - loss: 0.3444 - acc: 0.8647 - val_loss: 0.3419 - val_acc: 0.8610\n",
      "Epoch 806/1500\n",
      "Epoch 00805: val_loss did not improve\n",
      "2s - loss: 0.3791 - acc: 0.8599 - val_loss: 0.4322 - val_acc: 0.8369\n",
      "Epoch 807/1500\n",
      "Epoch 00806: val_loss did not improve\n",
      "2s - loss: 0.3367 - acc: 0.8783 - val_loss: 0.3792 - val_acc: 0.8640\n",
      "Epoch 808/1500\n",
      "Epoch 00807: val_loss did not improve\n",
      "2s - loss: 0.3507 - acc: 0.8760 - val_loss: 0.5023 - val_acc: 0.8248\n",
      "Epoch 809/1500\n",
      "Epoch 00808: val_loss did not improve\n",
      "2s - loss: 0.4138 - acc: 0.8529 - val_loss: 0.3814 - val_acc: 0.8761\n",
      "Epoch 810/1500\n",
      "Epoch 00809: val_loss did not improve\n",
      "2s - loss: 0.3553 - acc: 0.8720 - val_loss: 0.3947 - val_acc: 0.8520\n",
      "Epoch 811/1500\n",
      "Epoch 00810: val_loss did not improve\n",
      "2s - loss: 0.3504 - acc: 0.8669 - val_loss: 0.3538 - val_acc: 0.8792\n",
      "Epoch 812/1500\n",
      "Epoch 00811: val_loss did not improve\n",
      "2s - loss: 0.3595 - acc: 0.8677 - val_loss: 0.3663 - val_acc: 0.8882\n",
      "Epoch 813/1500\n",
      "Epoch 00812: val_loss did not improve\n",
      "2s - loss: 0.3494 - acc: 0.8706 - val_loss: 0.3326 - val_acc: 0.8852\n",
      "Epoch 814/1500\n",
      "Epoch 00813: val_loss did not improve\n",
      "2s - loss: 0.3749 - acc: 0.8607 - val_loss: 0.4340 - val_acc: 0.8278\n",
      "Epoch 815/1500\n",
      "Epoch 00814: val_loss improved from 0.30823 to 0.28972, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.3441 - acc: 0.8665 - val_loss: 0.2897 - val_acc: 0.9063\n",
      "Epoch 816/1500\n",
      "Epoch 00815: val_loss did not improve\n",
      "2s - loss: 0.3491 - acc: 0.8688 - val_loss: 0.4232 - val_acc: 0.8610\n",
      "Epoch 817/1500\n",
      "Epoch 00816: val_loss did not improve\n",
      "2s - loss: 0.3206 - acc: 0.8887 - val_loss: 0.4393 - val_acc: 0.8429\n",
      "Epoch 818/1500\n",
      "Epoch 00817: val_loss did not improve\n",
      "2s - loss: 0.3382 - acc: 0.8810 - val_loss: 0.3771 - val_acc: 0.8671\n",
      "Epoch 819/1500\n",
      "Epoch 00818: val_loss did not improve\n",
      "2s - loss: 0.3735 - acc: 0.8743 - val_loss: 0.3577 - val_acc: 0.8912\n",
      "Epoch 820/1500\n",
      "Epoch 00819: val_loss did not improve\n",
      "2s - loss: 0.3837 - acc: 0.8606 - val_loss: 0.4006 - val_acc: 0.8640\n",
      "Epoch 821/1500\n",
      "Epoch 00820: val_loss did not improve\n",
      "2s - loss: 0.3792 - acc: 0.8728 - val_loss: 0.3686 - val_acc: 0.8671\n",
      "Epoch 822/1500\n",
      "Epoch 00821: val_loss did not improve\n",
      "2s - loss: 0.3584 - acc: 0.8737 - val_loss: 0.4998 - val_acc: 0.7976\n",
      "Epoch 823/1500\n",
      "Epoch 00822: val_loss did not improve\n",
      "2s - loss: 0.3857 - acc: 0.8517 - val_loss: 0.3709 - val_acc: 0.8520\n",
      "Epoch 824/1500\n",
      "Epoch 00823: val_loss did not improve\n",
      "2s - loss: 0.3446 - acc: 0.8848 - val_loss: 0.3764 - val_acc: 0.8731\n",
      "Epoch 825/1500\n",
      "Epoch 00824: val_loss did not improve\n",
      "2s - loss: 0.3604 - acc: 0.8638 - val_loss: 0.3520 - val_acc: 0.8640\n",
      "Epoch 826/1500\n",
      "Epoch 00825: val_loss did not improve\n",
      "2s - loss: 0.4134 - acc: 0.8707 - val_loss: 0.4175 - val_acc: 0.8459\n",
      "Epoch 827/1500\n",
      "Epoch 00826: val_loss did not improve\n",
      "2s - loss: 0.4130 - acc: 0.8546 - val_loss: 0.3614 - val_acc: 0.8640\n",
      "Epoch 828/1500\n",
      "Epoch 00827: val_loss did not improve\n",
      "2s - loss: 0.3524 - acc: 0.8673 - val_loss: 0.3659 - val_acc: 0.8610\n",
      "Epoch 829/1500\n",
      "Epoch 00828: val_loss did not improve\n",
      "2s - loss: 0.3687 - acc: 0.8575 - val_loss: 0.3398 - val_acc: 0.8580\n",
      "Epoch 830/1500\n",
      "Epoch 00829: val_loss did not improve\n",
      "2s - loss: 0.4057 - acc: 0.8513 - val_loss: 0.3402 - val_acc: 0.8580\n",
      "Epoch 831/1500\n",
      "Epoch 00830: val_loss did not improve\n",
      "2s - loss: 0.3451 - acc: 0.8721 - val_loss: 0.3346 - val_acc: 0.8701\n",
      "Epoch 832/1500\n",
      "Epoch 00831: val_loss did not improve\n",
      "2s - loss: 0.3546 - acc: 0.8839 - val_loss: 0.4723 - val_acc: 0.8369\n",
      "Epoch 833/1500\n",
      "Epoch 00832: val_loss did not improve\n",
      "2s - loss: 0.3751 - acc: 0.8527 - val_loss: 0.3119 - val_acc: 0.8852\n",
      "Epoch 834/1500\n",
      "Epoch 00833: val_loss did not improve\n",
      "2s - loss: 0.3780 - acc: 0.8729 - val_loss: 0.4753 - val_acc: 0.8369\n",
      "Epoch 835/1500\n",
      "Epoch 00834: val_loss did not improve\n",
      "2s - loss: 0.3913 - acc: 0.8617 - val_loss: 0.3995 - val_acc: 0.8459\n",
      "Epoch 836/1500\n",
      "Epoch 00835: val_loss did not improve\n",
      "2s - loss: 0.3513 - acc: 0.8775 - val_loss: 0.3582 - val_acc: 0.8792\n",
      "Epoch 837/1500\n",
      "Epoch 00836: val_loss did not improve\n",
      "2s - loss: 0.3608 - acc: 0.8743 - val_loss: 0.3527 - val_acc: 0.8671\n",
      "Epoch 838/1500\n",
      "Epoch 00837: val_loss did not improve\n",
      "2s - loss: 0.3523 - acc: 0.8618 - val_loss: 0.4012 - val_acc: 0.8792\n",
      "Epoch 839/1500\n",
      "Epoch 00838: val_loss did not improve\n",
      "2s - loss: 0.3396 - acc: 0.8744 - val_loss: 0.4256 - val_acc: 0.8338\n",
      "Epoch 840/1500\n",
      "Epoch 00839: val_loss did not improve\n",
      "2s - loss: 0.3380 - acc: 0.8736 - val_loss: 0.4222 - val_acc: 0.8520\n",
      "Epoch 841/1500\n",
      "Epoch 00840: val_loss did not improve\n",
      "2s - loss: 0.3671 - acc: 0.8791 - val_loss: 0.4018 - val_acc: 0.8640\n",
      "Epoch 842/1500\n",
      "Epoch 00841: val_loss did not improve\n",
      "2s - loss: 0.3948 - acc: 0.8488 - val_loss: 0.3198 - val_acc: 0.8943\n",
      "Epoch 843/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00842: val_loss did not improve\n",
      "2s - loss: 0.3438 - acc: 0.8728 - val_loss: 0.2978 - val_acc: 0.8943\n",
      "Epoch 844/1500\n",
      "Epoch 00843: val_loss did not improve\n",
      "2s - loss: 0.4103 - acc: 0.8469 - val_loss: 0.3122 - val_acc: 0.9003\n",
      "Epoch 845/1500\n",
      "Epoch 00844: val_loss did not improve\n",
      "2s - loss: 0.3681 - acc: 0.8706 - val_loss: 0.4041 - val_acc: 0.8489\n",
      "Epoch 846/1500\n",
      "Epoch 00845: val_loss did not improve\n",
      "2s - loss: 0.3922 - acc: 0.8567 - val_loss: 0.3587 - val_acc: 0.8731\n",
      "Epoch 847/1500\n",
      "Epoch 00846: val_loss did not improve\n",
      "2s - loss: 0.3818 - acc: 0.8650 - val_loss: 0.3302 - val_acc: 0.8973\n",
      "Epoch 848/1500\n",
      "Epoch 00847: val_loss did not improve\n",
      "2s - loss: 0.3393 - acc: 0.8743 - val_loss: 0.3734 - val_acc: 0.8852\n",
      "Epoch 849/1500\n",
      "Epoch 00848: val_loss did not improve\n",
      "2s - loss: 0.3436 - acc: 0.8741 - val_loss: 0.3953 - val_acc: 0.8610\n",
      "Epoch 850/1500\n",
      "Epoch 00849: val_loss did not improve\n",
      "2s - loss: 0.3438 - acc: 0.8736 - val_loss: 0.3095 - val_acc: 0.9033\n",
      "Epoch 851/1500\n",
      "Epoch 00850: val_loss improved from 0.28972 to 0.28949, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.3648 - acc: 0.8689 - val_loss: 0.2895 - val_acc: 0.8943\n",
      "Epoch 852/1500\n",
      "Epoch 00851: val_loss did not improve\n",
      "2s - loss: 0.3580 - acc: 0.8807 - val_loss: 0.3583 - val_acc: 0.8761\n",
      "Epoch 853/1500\n",
      "Epoch 00852: val_loss did not improve\n",
      "2s - loss: 0.3617 - acc: 0.8623 - val_loss: 0.3356 - val_acc: 0.8792\n",
      "Epoch 854/1500\n",
      "Epoch 00853: val_loss did not improve\n",
      "2s - loss: 0.3615 - acc: 0.8672 - val_loss: 0.3334 - val_acc: 0.8973\n",
      "Epoch 855/1500\n",
      "Epoch 00854: val_loss did not improve\n",
      "2s - loss: 0.3311 - acc: 0.8887 - val_loss: 0.3449 - val_acc: 0.8882\n",
      "Epoch 856/1500\n",
      "Epoch 00855: val_loss did not improve\n",
      "2s - loss: 0.3622 - acc: 0.8688 - val_loss: 0.3500 - val_acc: 0.8912\n",
      "Epoch 857/1500\n",
      "Epoch 00856: val_loss did not improve\n",
      "2s - loss: 0.3524 - acc: 0.8768 - val_loss: 0.3567 - val_acc: 0.8701\n",
      "Epoch 858/1500\n",
      "Epoch 00857: val_loss did not improve\n",
      "2s - loss: 0.4061 - acc: 0.8552 - val_loss: 0.3570 - val_acc: 0.8701\n",
      "Epoch 859/1500\n",
      "Epoch 00858: val_loss did not improve\n",
      "2s - loss: 0.3461 - acc: 0.8704 - val_loss: 0.3591 - val_acc: 0.8701\n",
      "Epoch 860/1500\n",
      "Epoch 00859: val_loss did not improve\n",
      "2s - loss: 0.3386 - acc: 0.8855 - val_loss: 0.4401 - val_acc: 0.8429\n",
      "Epoch 861/1500\n",
      "Epoch 00860: val_loss did not improve\n",
      "2s - loss: 0.3748 - acc: 0.8712 - val_loss: 0.3804 - val_acc: 0.8610\n",
      "Epoch 862/1500\n",
      "Epoch 00861: val_loss did not improve\n",
      "2s - loss: 0.3644 - acc: 0.8720 - val_loss: 0.3594 - val_acc: 0.8701\n",
      "Epoch 863/1500\n",
      "Epoch 00862: val_loss did not improve\n",
      "2s - loss: 0.3577 - acc: 0.8641 - val_loss: 0.3321 - val_acc: 0.8943\n",
      "Epoch 864/1500\n",
      "Epoch 00863: val_loss did not improve\n",
      "2s - loss: 0.3356 - acc: 0.8682 - val_loss: 0.3478 - val_acc: 0.8701\n",
      "Epoch 865/1500\n",
      "Epoch 00864: val_loss did not improve\n",
      "2s - loss: 0.3535 - acc: 0.8681 - val_loss: 0.3692 - val_acc: 0.8701\n",
      "Epoch 866/1500\n",
      "Epoch 00865: val_loss did not improve\n",
      "2s - loss: 0.3254 - acc: 0.8735 - val_loss: 0.4253 - val_acc: 0.8399\n",
      "Epoch 867/1500\n",
      "Epoch 00866: val_loss did not improve\n",
      "2s - loss: 0.3108 - acc: 0.8823 - val_loss: 0.3407 - val_acc: 0.8761\n",
      "Epoch 868/1500\n",
      "Epoch 00867: val_loss did not improve\n",
      "2s - loss: 0.3540 - acc: 0.8705 - val_loss: 0.3490 - val_acc: 0.8792\n",
      "Epoch 869/1500\n",
      "Epoch 00868: val_loss did not improve\n",
      "2s - loss: 0.3754 - acc: 0.8607 - val_loss: 0.3543 - val_acc: 0.9063\n",
      "Epoch 870/1500\n",
      "Epoch 00869: val_loss did not improve\n",
      "2s - loss: 0.3686 - acc: 0.8688 - val_loss: 0.4343 - val_acc: 0.8489\n",
      "Epoch 871/1500\n",
      "Epoch 00870: val_loss did not improve\n",
      "2s - loss: 0.3372 - acc: 0.8807 - val_loss: 0.3515 - val_acc: 0.8550\n",
      "Epoch 872/1500\n",
      "Epoch 00871: val_loss did not improve\n",
      "2s - loss: 0.3302 - acc: 0.8881 - val_loss: 0.3849 - val_acc: 0.8671\n",
      "Epoch 873/1500\n",
      "Epoch 00872: val_loss did not improve\n",
      "2s - loss: 0.3333 - acc: 0.8890 - val_loss: 0.3422 - val_acc: 0.8792\n",
      "Epoch 874/1500\n",
      "Epoch 00873: val_loss did not improve\n",
      "2s - loss: 0.3389 - acc: 0.8686 - val_loss: 0.3682 - val_acc: 0.8701\n",
      "Epoch 875/1500\n",
      "Epoch 00874: val_loss did not improve\n",
      "2s - loss: 0.3216 - acc: 0.8800 - val_loss: 0.4077 - val_acc: 0.8338\n",
      "Epoch 876/1500\n",
      "Epoch 00875: val_loss did not improve\n",
      "2s - loss: 0.3761 - acc: 0.8631 - val_loss: 0.3366 - val_acc: 0.8912\n",
      "Epoch 877/1500\n",
      "Epoch 00876: val_loss did not improve\n",
      "2s - loss: 0.3294 - acc: 0.8822 - val_loss: 0.2979 - val_acc: 0.9124\n",
      "Epoch 878/1500\n",
      "Epoch 00877: val_loss did not improve\n",
      "2s - loss: 0.3578 - acc: 0.8719 - val_loss: 0.4106 - val_acc: 0.8459\n",
      "Epoch 879/1500\n",
      "Epoch 00878: val_loss did not improve\n",
      "2s - loss: 0.3555 - acc: 0.8609 - val_loss: 0.3461 - val_acc: 0.8731\n",
      "Epoch 880/1500\n",
      "Epoch 00879: val_loss did not improve\n",
      "2s - loss: 0.4043 - acc: 0.8519 - val_loss: 0.3956 - val_acc: 0.8610\n",
      "Epoch 881/1500\n",
      "Epoch 00880: val_loss did not improve\n",
      "2s - loss: 0.3700 - acc: 0.8665 - val_loss: 0.3565 - val_acc: 0.8640\n",
      "Epoch 882/1500\n",
      "Epoch 00881: val_loss did not improve\n",
      "2s - loss: 0.3362 - acc: 0.8743 - val_loss: 0.3527 - val_acc: 0.8822\n",
      "Epoch 883/1500\n",
      "Epoch 00882: val_loss did not improve\n",
      "2s - loss: 0.3463 - acc: 0.8694 - val_loss: 0.4812 - val_acc: 0.8187\n",
      "Epoch 884/1500\n",
      "Epoch 00883: val_loss did not improve\n",
      "2s - loss: 0.3523 - acc: 0.8702 - val_loss: 0.3169 - val_acc: 0.8912\n",
      "Epoch 885/1500\n",
      "Epoch 00884: val_loss did not improve\n",
      "2s - loss: 0.3923 - acc: 0.8609 - val_loss: 0.3344 - val_acc: 0.8822\n",
      "Epoch 886/1500\n",
      "Epoch 00885: val_loss did not improve\n",
      "2s - loss: 0.3209 - acc: 0.8833 - val_loss: 0.3639 - val_acc: 0.8761\n",
      "Epoch 887/1500\n",
      "Epoch 00886: val_loss did not improve\n",
      "2s - loss: 0.3528 - acc: 0.8598 - val_loss: 0.3926 - val_acc: 0.8489\n",
      "Epoch 888/1500\n",
      "Epoch 00887: val_loss did not improve\n",
      "2s - loss: 0.3536 - acc: 0.8760 - val_loss: 0.3732 - val_acc: 0.8671\n",
      "Epoch 889/1500\n",
      "Epoch 00888: val_loss did not improve\n",
      "2s - loss: 0.3038 - acc: 0.8866 - val_loss: 0.3406 - val_acc: 0.9003\n",
      "Epoch 890/1500\n",
      "Epoch 00889: val_loss did not improve\n",
      "2s - loss: 0.3575 - acc: 0.8729 - val_loss: 0.4561 - val_acc: 0.8338\n",
      "Epoch 891/1500\n",
      "Epoch 00890: val_loss did not improve\n",
      "2s - loss: 0.3308 - acc: 0.8792 - val_loss: 0.4056 - val_acc: 0.8792\n",
      "Epoch 892/1500\n",
      "Epoch 00891: val_loss did not improve\n",
      "2s - loss: 0.3241 - acc: 0.8791 - val_loss: 0.4534 - val_acc: 0.8459\n",
      "Epoch 893/1500\n",
      "Epoch 00892: val_loss did not improve\n",
      "2s - loss: 0.3161 - acc: 0.8921 - val_loss: 0.4026 - val_acc: 0.8640\n",
      "Epoch 894/1500\n",
      "Epoch 00893: val_loss did not improve\n",
      "2s - loss: 0.3098 - acc: 0.8801 - val_loss: 0.3500 - val_acc: 0.8792\n",
      "Epoch 895/1500\n",
      "Epoch 00894: val_loss did not improve\n",
      "2s - loss: 0.3246 - acc: 0.8801 - val_loss: 0.3124 - val_acc: 0.8882\n",
      "Epoch 896/1500\n",
      "Epoch 00895: val_loss did not improve\n",
      "2s - loss: 0.3576 - acc: 0.8586 - val_loss: 0.4010 - val_acc: 0.8731\n",
      "Epoch 897/1500\n",
      "Epoch 00896: val_loss did not improve\n",
      "2s - loss: 0.3264 - acc: 0.8731 - val_loss: 0.3322 - val_acc: 0.8912\n",
      "Epoch 898/1500\n",
      "Epoch 00897: val_loss did not improve\n",
      "2s - loss: 0.3759 - acc: 0.8704 - val_loss: 0.3588 - val_acc: 0.8671\n",
      "Epoch 899/1500\n",
      "Epoch 00898: val_loss did not improve\n",
      "2s - loss: 0.3115 - acc: 0.8832 - val_loss: 0.3272 - val_acc: 0.8792\n",
      "Epoch 900/1500\n",
      "Epoch 00899: val_loss did not improve\n",
      "2s - loss: 0.3593 - acc: 0.8727 - val_loss: 0.3393 - val_acc: 0.9003\n",
      "Epoch 901/1500\n",
      "Epoch 00900: val_loss did not improve\n",
      "2s - loss: 0.3369 - acc: 0.8752 - val_loss: 0.3568 - val_acc: 0.8852\n",
      "Epoch 902/1500\n",
      "Epoch 00901: val_loss did not improve\n",
      "2s - loss: 0.3143 - acc: 0.8839 - val_loss: 0.3025 - val_acc: 0.9003\n",
      "Epoch 903/1500\n",
      "Epoch 00902: val_loss did not improve\n",
      "2s - loss: 0.3297 - acc: 0.8814 - val_loss: 0.4376 - val_acc: 0.8489\n",
      "Epoch 904/1500\n",
      "Epoch 00903: val_loss did not improve\n",
      "2s - loss: 0.3667 - acc: 0.8672 - val_loss: 0.3474 - val_acc: 0.8701\n",
      "Epoch 905/1500\n",
      "Epoch 00904: val_loss did not improve\n",
      "2s - loss: 0.3736 - acc: 0.8713 - val_loss: 0.3585 - val_acc: 0.8792\n",
      "Epoch 906/1500\n",
      "Epoch 00905: val_loss did not improve\n",
      "2s - loss: 0.3387 - acc: 0.8808 - val_loss: 0.3737 - val_acc: 0.8610\n",
      "Epoch 907/1500\n",
      "Epoch 00906: val_loss did not improve\n",
      "2s - loss: 0.3190 - acc: 0.8730 - val_loss: 0.2919 - val_acc: 0.8882\n",
      "Epoch 908/1500\n",
      "Epoch 00907: val_loss did not improve\n",
      "2s - loss: 0.3794 - acc: 0.8535 - val_loss: 0.3941 - val_acc: 0.8399\n",
      "Epoch 909/1500\n",
      "Epoch 00908: val_loss did not improve\n",
      "2s - loss: 0.3426 - acc: 0.8775 - val_loss: 0.3319 - val_acc: 0.8852\n",
      "Epoch 910/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00909: val_loss did not improve\n",
      "2s - loss: 0.3265 - acc: 0.8825 - val_loss: 0.3454 - val_acc: 0.8761\n",
      "Epoch 911/1500\n",
      "Epoch 00910: val_loss did not improve\n",
      "2s - loss: 0.3250 - acc: 0.8816 - val_loss: 0.2928 - val_acc: 0.9063\n",
      "Epoch 912/1500\n",
      "Epoch 00911: val_loss did not improve\n",
      "2s - loss: 0.3346 - acc: 0.8738 - val_loss: 0.2914 - val_acc: 0.9003\n",
      "Epoch 913/1500\n",
      "Epoch 00912: val_loss improved from 0.28949 to 0.27095, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.3385 - acc: 0.8670 - val_loss: 0.2709 - val_acc: 0.9184\n",
      "Epoch 914/1500\n",
      "Epoch 00913: val_loss did not improve\n",
      "2s - loss: 0.3072 - acc: 0.8886 - val_loss: 0.4132 - val_acc: 0.8580\n",
      "Epoch 915/1500\n",
      "Epoch 00914: val_loss did not improve\n",
      "2s - loss: 0.3253 - acc: 0.8745 - val_loss: 0.3205 - val_acc: 0.9003\n",
      "Epoch 916/1500\n",
      "Epoch 00915: val_loss did not improve\n",
      "2s - loss: 0.3364 - acc: 0.8783 - val_loss: 0.3791 - val_acc: 0.8882\n",
      "Epoch 917/1500\n",
      "Epoch 00916: val_loss did not improve\n",
      "2s - loss: 0.2897 - acc: 0.8904 - val_loss: 0.4239 - val_acc: 0.8731\n",
      "Epoch 918/1500\n",
      "Epoch 00917: val_loss did not improve\n",
      "2s - loss: 0.3448 - acc: 0.8761 - val_loss: 0.2871 - val_acc: 0.9033\n",
      "Epoch 919/1500\n",
      "Epoch 00918: val_loss did not improve\n",
      "2s - loss: 0.3401 - acc: 0.8682 - val_loss: 0.3267 - val_acc: 0.9063\n",
      "Epoch 920/1500\n",
      "Epoch 00919: val_loss did not improve\n",
      "2s - loss: 0.3089 - acc: 0.8864 - val_loss: 0.3352 - val_acc: 0.8852\n",
      "Epoch 921/1500\n",
      "Epoch 00920: val_loss did not improve\n",
      "2s - loss: 0.3450 - acc: 0.8672 - val_loss: 0.3262 - val_acc: 0.8882\n",
      "Epoch 922/1500\n",
      "Epoch 00921: val_loss did not improve\n",
      "2s - loss: 0.3534 - acc: 0.8744 - val_loss: 0.2918 - val_acc: 0.9003\n",
      "Epoch 923/1500\n",
      "Epoch 00922: val_loss did not improve\n",
      "2s - loss: 0.3573 - acc: 0.8669 - val_loss: 0.3301 - val_acc: 0.8943\n",
      "Epoch 924/1500\n",
      "Epoch 00923: val_loss did not improve\n",
      "2s - loss: 0.3408 - acc: 0.8770 - val_loss: 0.4066 - val_acc: 0.8671\n",
      "Epoch 925/1500\n",
      "Epoch 00924: val_loss did not improve\n",
      "2s - loss: 0.3247 - acc: 0.8832 - val_loss: 0.2821 - val_acc: 0.8973\n",
      "Epoch 926/1500\n",
      "Epoch 00925: val_loss did not improve\n",
      "2s - loss: 0.3471 - acc: 0.8665 - val_loss: 0.3936 - val_acc: 0.8882\n",
      "Epoch 927/1500\n",
      "Epoch 00926: val_loss did not improve\n",
      "2s - loss: 0.3180 - acc: 0.8735 - val_loss: 0.4173 - val_acc: 0.8671\n",
      "Epoch 928/1500\n",
      "Epoch 00927: val_loss did not improve\n",
      "2s - loss: 0.2991 - acc: 0.9008 - val_loss: 0.4030 - val_acc: 0.8671\n",
      "Epoch 929/1500\n",
      "Epoch 00928: val_loss did not improve\n",
      "2s - loss: 0.3247 - acc: 0.8751 - val_loss: 0.2980 - val_acc: 0.9003\n",
      "Epoch 930/1500\n",
      "Epoch 00929: val_loss did not improve\n",
      "2s - loss: 0.3414 - acc: 0.8776 - val_loss: 0.3203 - val_acc: 0.9003\n",
      "Epoch 931/1500\n",
      "Epoch 00930: val_loss did not improve\n",
      "2s - loss: 0.3078 - acc: 0.8823 - val_loss: 0.3235 - val_acc: 0.8973\n",
      "Epoch 932/1500\n",
      "Epoch 00931: val_loss did not improve\n",
      "2s - loss: 0.3327 - acc: 0.8699 - val_loss: 0.3190 - val_acc: 0.8882\n",
      "Epoch 933/1500\n",
      "Epoch 00932: val_loss did not improve\n",
      "2s - loss: 0.3222 - acc: 0.8783 - val_loss: 0.3718 - val_acc: 0.8882\n",
      "Epoch 934/1500\n",
      "Epoch 00933: val_loss did not improve\n",
      "2s - loss: 0.3154 - acc: 0.8816 - val_loss: 0.3513 - val_acc: 0.9063\n",
      "Epoch 935/1500\n",
      "Epoch 00934: val_loss did not improve\n",
      "2s - loss: 0.3412 - acc: 0.8727 - val_loss: 0.3154 - val_acc: 0.9033\n",
      "Epoch 936/1500\n",
      "Epoch 00935: val_loss did not improve\n",
      "2s - loss: 0.2879 - acc: 0.8960 - val_loss: 0.4025 - val_acc: 0.8701\n",
      "Epoch 937/1500\n",
      "Epoch 00936: val_loss did not improve\n",
      "2s - loss: 0.2872 - acc: 0.8968 - val_loss: 0.4866 - val_acc: 0.8429\n",
      "Epoch 938/1500\n",
      "Epoch 00937: val_loss did not improve\n",
      "2s - loss: 0.3336 - acc: 0.8768 - val_loss: 0.2977 - val_acc: 0.9154\n",
      "Epoch 939/1500\n",
      "Epoch 00938: val_loss did not improve\n",
      "2s - loss: 0.3003 - acc: 0.8902 - val_loss: 0.3847 - val_acc: 0.8580\n",
      "Epoch 940/1500\n",
      "Epoch 00939: val_loss did not improve\n",
      "2s - loss: 0.2970 - acc: 0.8974 - val_loss: 0.3969 - val_acc: 0.8882\n",
      "Epoch 941/1500\n",
      "Epoch 00940: val_loss did not improve\n",
      "2s - loss: 0.3174 - acc: 0.8862 - val_loss: 0.3453 - val_acc: 0.9003\n",
      "Epoch 942/1500\n",
      "Epoch 00941: val_loss did not improve\n",
      "2s - loss: 0.3224 - acc: 0.8785 - val_loss: 0.3654 - val_acc: 0.8852\n",
      "Epoch 943/1500\n",
      "Epoch 00942: val_loss did not improve\n",
      "2s - loss: 0.3525 - acc: 0.8802 - val_loss: 0.3531 - val_acc: 0.8822\n",
      "Epoch 944/1500\n",
      "Epoch 00943: val_loss did not improve\n",
      "2s - loss: 0.2854 - acc: 0.8942 - val_loss: 0.3303 - val_acc: 0.8943\n",
      "Epoch 945/1500\n",
      "Epoch 00944: val_loss did not improve\n",
      "2s - loss: 0.3048 - acc: 0.8896 - val_loss: 0.3191 - val_acc: 0.9003\n",
      "Epoch 946/1500\n",
      "Epoch 00945: val_loss did not improve\n",
      "2s - loss: 0.3258 - acc: 0.8776 - val_loss: 0.4137 - val_acc: 0.8852\n",
      "Epoch 947/1500\n",
      "Epoch 00946: val_loss did not improve\n",
      "2s - loss: 0.2830 - acc: 0.8983 - val_loss: 0.2801 - val_acc: 0.8973\n",
      "Epoch 948/1500\n",
      "Epoch 00947: val_loss did not improve\n",
      "2s - loss: 0.3424 - acc: 0.8728 - val_loss: 0.4396 - val_acc: 0.8550\n",
      "Epoch 949/1500\n",
      "Epoch 00948: val_loss improved from 0.27095 to 0.23753, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.3485 - acc: 0.8688 - val_loss: 0.2375 - val_acc: 0.9033\n",
      "Epoch 950/1500\n",
      "Epoch 00949: val_loss did not improve\n",
      "2s - loss: 0.3134 - acc: 0.8873 - val_loss: 0.3268 - val_acc: 0.9033\n",
      "Epoch 951/1500\n",
      "Epoch 00950: val_loss did not improve\n",
      "2s - loss: 0.3308 - acc: 0.8777 - val_loss: 0.3024 - val_acc: 0.8973\n",
      "Epoch 952/1500\n",
      "Epoch 00951: val_loss did not improve\n",
      "2s - loss: 0.3018 - acc: 0.8959 - val_loss: 0.3487 - val_acc: 0.9003\n",
      "Epoch 953/1500\n",
      "Epoch 00952: val_loss did not improve\n",
      "2s - loss: 0.3202 - acc: 0.8749 - val_loss: 0.2955 - val_acc: 0.8973\n",
      "Epoch 954/1500\n",
      "Epoch 00953: val_loss did not improve\n",
      "2s - loss: 0.3395 - acc: 0.8705 - val_loss: 0.3351 - val_acc: 0.8882\n",
      "Epoch 955/1500\n",
      "Epoch 00954: val_loss did not improve\n",
      "2s - loss: 0.3327 - acc: 0.8757 - val_loss: 0.3203 - val_acc: 0.8973\n",
      "Epoch 956/1500\n",
      "Epoch 00955: val_loss did not improve\n",
      "2s - loss: 0.3068 - acc: 0.8882 - val_loss: 0.3786 - val_acc: 0.8792\n",
      "Epoch 957/1500\n",
      "Epoch 00956: val_loss did not improve\n",
      "2s - loss: 0.3350 - acc: 0.8850 - val_loss: 0.3144 - val_acc: 0.9003\n",
      "Epoch 958/1500\n",
      "Epoch 00957: val_loss did not improve\n",
      "2s - loss: 0.3265 - acc: 0.8792 - val_loss: 0.4296 - val_acc: 0.8580\n",
      "Epoch 959/1500\n",
      "Epoch 00958: val_loss did not improve\n",
      "2s - loss: 0.3302 - acc: 0.8776 - val_loss: 0.4026 - val_acc: 0.8671\n",
      "Epoch 960/1500\n",
      "Epoch 00959: val_loss did not improve\n",
      "2s - loss: 0.2976 - acc: 0.8878 - val_loss: 0.3107 - val_acc: 0.8882\n",
      "Epoch 961/1500\n",
      "Epoch 00960: val_loss did not improve\n",
      "2s - loss: 0.3240 - acc: 0.8743 - val_loss: 0.3827 - val_acc: 0.8792\n",
      "Epoch 962/1500\n",
      "Epoch 00961: val_loss did not improve\n",
      "2s - loss: 0.3248 - acc: 0.8840 - val_loss: 0.3378 - val_acc: 0.8973\n",
      "Epoch 963/1500\n",
      "Epoch 00962: val_loss did not improve\n",
      "2s - loss: 0.3506 - acc: 0.8777 - val_loss: 0.3197 - val_acc: 0.8943\n",
      "Epoch 964/1500\n",
      "Epoch 00963: val_loss did not improve\n",
      "2s - loss: 0.3139 - acc: 0.8823 - val_loss: 0.3378 - val_acc: 0.8912\n",
      "Epoch 965/1500\n",
      "Epoch 00964: val_loss did not improve\n",
      "2s - loss: 0.3417 - acc: 0.8639 - val_loss: 0.3058 - val_acc: 0.9063\n",
      "Epoch 966/1500\n",
      "Epoch 00965: val_loss did not improve\n",
      "2s - loss: 0.3081 - acc: 0.8920 - val_loss: 0.3290 - val_acc: 0.8882\n",
      "Epoch 967/1500\n",
      "Epoch 00966: val_loss did not improve\n",
      "2s - loss: 0.3021 - acc: 0.8873 - val_loss: 0.3754 - val_acc: 0.8731\n",
      "Epoch 968/1500\n",
      "Epoch 00967: val_loss did not improve\n",
      "2s - loss: 0.2974 - acc: 0.8959 - val_loss: 0.3234 - val_acc: 0.8912\n",
      "Epoch 969/1500\n",
      "Epoch 00968: val_loss did not improve\n",
      "2s - loss: 0.3155 - acc: 0.8871 - val_loss: 0.3633 - val_acc: 0.8580\n",
      "Epoch 970/1500\n",
      "Epoch 00969: val_loss did not improve\n",
      "2s - loss: 0.3207 - acc: 0.8846 - val_loss: 0.3339 - val_acc: 0.8943\n",
      "Epoch 971/1500\n",
      "Epoch 00970: val_loss did not improve\n",
      "2s - loss: 0.3452 - acc: 0.8760 - val_loss: 0.3422 - val_acc: 0.8943\n",
      "Epoch 972/1500\n",
      "Epoch 00971: val_loss did not improve\n",
      "2s - loss: 0.3085 - acc: 0.8839 - val_loss: 0.3533 - val_acc: 0.8792\n",
      "Epoch 973/1500\n",
      "Epoch 00972: val_loss did not improve\n",
      "2s - loss: 0.2843 - acc: 0.9031 - val_loss: 0.3007 - val_acc: 0.8731\n",
      "Epoch 974/1500\n",
      "Epoch 00973: val_loss did not improve\n",
      "2s - loss: 0.3140 - acc: 0.8849 - val_loss: 0.3265 - val_acc: 0.8822\n",
      "Epoch 975/1500\n",
      "Epoch 00974: val_loss did not improve\n",
      "2s - loss: 0.3473 - acc: 0.8808 - val_loss: 0.3316 - val_acc: 0.8610\n",
      "Epoch 976/1500\n",
      "Epoch 00975: val_loss did not improve\n",
      "2s - loss: 0.3101 - acc: 0.8767 - val_loss: 0.3407 - val_acc: 0.8973\n",
      "Epoch 977/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00976: val_loss did not improve\n",
      "2s - loss: 0.3507 - acc: 0.8739 - val_loss: 0.3182 - val_acc: 0.8973\n",
      "Epoch 978/1500\n",
      "Epoch 00977: val_loss did not improve\n",
      "2s - loss: 0.2981 - acc: 0.8863 - val_loss: 0.3695 - val_acc: 0.8640\n",
      "Epoch 979/1500\n",
      "Epoch 00978: val_loss did not improve\n",
      "2s - loss: 0.3009 - acc: 0.8959 - val_loss: 0.2980 - val_acc: 0.8852\n",
      "Epoch 980/1500\n",
      "Epoch 00979: val_loss did not improve\n",
      "2s - loss: 0.2977 - acc: 0.8862 - val_loss: 0.3273 - val_acc: 0.8943\n",
      "Epoch 981/1500\n",
      "Epoch 00980: val_loss did not improve\n",
      "2s - loss: 0.2967 - acc: 0.9007 - val_loss: 0.3216 - val_acc: 0.9033\n",
      "Epoch 982/1500\n",
      "Epoch 00981: val_loss did not improve\n",
      "2s - loss: 0.3479 - acc: 0.8780 - val_loss: 0.4314 - val_acc: 0.8369\n",
      "Epoch 983/1500\n",
      "Epoch 00982: val_loss did not improve\n",
      "2s - loss: 0.2689 - acc: 0.9017 - val_loss: 0.3456 - val_acc: 0.8701\n",
      "Epoch 984/1500\n",
      "Epoch 00983: val_loss did not improve\n",
      "2s - loss: 0.3371 - acc: 0.8769 - val_loss: 0.3138 - val_acc: 0.9033\n",
      "Epoch 985/1500\n",
      "Epoch 00984: val_loss did not improve\n",
      "2s - loss: 0.2978 - acc: 0.8871 - val_loss: 0.4163 - val_acc: 0.8671\n",
      "Epoch 986/1500\n",
      "Epoch 00985: val_loss did not improve\n",
      "2s - loss: 0.3276 - acc: 0.8760 - val_loss: 0.2917 - val_acc: 0.9033\n",
      "Epoch 987/1500\n",
      "Epoch 00986: val_loss did not improve\n",
      "2s - loss: 0.3132 - acc: 0.8912 - val_loss: 0.2973 - val_acc: 0.9063\n",
      "Epoch 988/1500\n",
      "Epoch 00987: val_loss did not improve\n",
      "2s - loss: 0.2954 - acc: 0.8943 - val_loss: 0.3405 - val_acc: 0.8792\n",
      "Epoch 989/1500\n",
      "Epoch 00988: val_loss did not improve\n",
      "2s - loss: 0.2757 - acc: 0.9071 - val_loss: 0.3669 - val_acc: 0.9003\n",
      "Epoch 990/1500\n",
      "Epoch 00989: val_loss did not improve\n",
      "2s - loss: 0.3351 - acc: 0.8819 - val_loss: 0.4555 - val_acc: 0.8520\n",
      "Epoch 991/1500\n",
      "Epoch 00990: val_loss did not improve\n",
      "2s - loss: 0.3033 - acc: 0.8967 - val_loss: 0.3029 - val_acc: 0.9033\n",
      "Epoch 992/1500\n",
      "Epoch 00991: val_loss did not improve\n",
      "2s - loss: 0.3238 - acc: 0.8831 - val_loss: 0.2999 - val_acc: 0.8973\n",
      "Epoch 993/1500\n",
      "Epoch 00992: val_loss did not improve\n",
      "2s - loss: 0.2938 - acc: 0.8927 - val_loss: 0.3010 - val_acc: 0.8943\n",
      "Epoch 994/1500\n",
      "Epoch 00993: val_loss did not improve\n",
      "2s - loss: 0.3098 - acc: 0.8849 - val_loss: 0.3955 - val_acc: 0.8761\n",
      "Epoch 995/1500\n",
      "Epoch 00994: val_loss did not improve\n",
      "2s - loss: 0.3012 - acc: 0.8913 - val_loss: 0.3891 - val_acc: 0.8761\n",
      "Epoch 996/1500\n",
      "Epoch 00995: val_loss did not improve\n",
      "2s - loss: 0.3068 - acc: 0.8799 - val_loss: 0.3243 - val_acc: 0.9033\n",
      "Epoch 997/1500\n",
      "Epoch 00996: val_loss did not improve\n",
      "2s - loss: 0.3401 - acc: 0.8686 - val_loss: 0.2993 - val_acc: 0.9033\n",
      "Epoch 998/1500\n",
      "Epoch 00997: val_loss did not improve\n",
      "2s - loss: 0.3058 - acc: 0.8960 - val_loss: 0.2897 - val_acc: 0.9305\n",
      "Epoch 999/1500\n",
      "Epoch 00998: val_loss did not improve\n",
      "2s - loss: 0.3203 - acc: 0.8873 - val_loss: 0.3260 - val_acc: 0.8973\n",
      "Epoch 1000/1500\n",
      "Epoch 00999: val_loss did not improve\n",
      "2s - loss: 0.3222 - acc: 0.8768 - val_loss: 0.2986 - val_acc: 0.9094\n",
      "Epoch 1001/1500\n",
      "Epoch 01000: val_loss did not improve\n",
      "2s - loss: 0.3285 - acc: 0.8833 - val_loss: 0.3478 - val_acc: 0.8761\n",
      "Epoch 1002/1500\n",
      "Epoch 01001: val_loss did not improve\n",
      "2s - loss: 0.2974 - acc: 0.8919 - val_loss: 0.2962 - val_acc: 0.9063\n",
      "Epoch 1003/1500\n",
      "Epoch 01002: val_loss did not improve\n",
      "2s - loss: 0.3431 - acc: 0.8614 - val_loss: 0.2423 - val_acc: 0.9094\n",
      "Epoch 1004/1500\n",
      "Epoch 01003: val_loss did not improve\n",
      "2s - loss: 0.3246 - acc: 0.8946 - val_loss: 0.3441 - val_acc: 0.8701\n",
      "Epoch 1005/1500\n",
      "Epoch 01004: val_loss did not improve\n",
      "2s - loss: 0.3082 - acc: 0.8830 - val_loss: 0.3948 - val_acc: 0.8610\n",
      "Epoch 1006/1500\n",
      "Epoch 01005: val_loss did not improve\n",
      "2s - loss: 0.3112 - acc: 0.8882 - val_loss: 0.2915 - val_acc: 0.9124\n",
      "Epoch 1007/1500\n",
      "Epoch 01006: val_loss improved from 0.23753 to 0.20632, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.2805 - acc: 0.8950 - val_loss: 0.2063 - val_acc: 0.9335\n",
      "Epoch 1008/1500\n",
      "Epoch 01007: val_loss did not improve\n",
      "2s - loss: 0.3178 - acc: 0.8905 - val_loss: 0.3488 - val_acc: 0.8731\n",
      "Epoch 1009/1500\n",
      "Epoch 01008: val_loss did not improve\n",
      "2s - loss: 0.2887 - acc: 0.8887 - val_loss: 0.2733 - val_acc: 0.9124\n",
      "Epoch 1010/1500\n",
      "Epoch 01009: val_loss did not improve\n",
      "2s - loss: 0.2965 - acc: 0.8831 - val_loss: 0.3712 - val_acc: 0.8580\n",
      "Epoch 1011/1500\n",
      "Epoch 01010: val_loss did not improve\n",
      "2s - loss: 0.2691 - acc: 0.9063 - val_loss: 0.3065 - val_acc: 0.8912\n",
      "Epoch 1012/1500\n",
      "Epoch 01011: val_loss did not improve\n",
      "2s - loss: 0.3055 - acc: 0.8937 - val_loss: 0.3620 - val_acc: 0.8792\n",
      "Epoch 1013/1500\n",
      "Epoch 01012: val_loss did not improve\n",
      "2s - loss: 0.2672 - acc: 0.8961 - val_loss: 0.2725 - val_acc: 0.9275\n",
      "Epoch 1014/1500\n",
      "Epoch 01013: val_loss did not improve\n",
      "2s - loss: 0.3171 - acc: 0.8913 - val_loss: 0.3342 - val_acc: 0.8973\n",
      "Epoch 1015/1500\n",
      "Epoch 01014: val_loss did not improve\n",
      "2s - loss: 0.2842 - acc: 0.8935 - val_loss: 0.3606 - val_acc: 0.8852\n",
      "Epoch 1016/1500\n",
      "Epoch 01015: val_loss did not improve\n",
      "2s - loss: 0.2766 - acc: 0.8944 - val_loss: 0.4122 - val_acc: 0.8610\n",
      "Epoch 1017/1500\n",
      "Epoch 01016: val_loss did not improve\n",
      "2s - loss: 0.3037 - acc: 0.8823 - val_loss: 0.4867 - val_acc: 0.8580\n",
      "Epoch 1018/1500\n",
      "Epoch 01017: val_loss did not improve\n",
      "2s - loss: 0.3173 - acc: 0.8842 - val_loss: 0.3110 - val_acc: 0.9003\n",
      "Epoch 1019/1500\n",
      "Epoch 01018: val_loss did not improve\n",
      "2s - loss: 0.3106 - acc: 0.8952 - val_loss: 0.3130 - val_acc: 0.8822\n",
      "Epoch 1020/1500\n",
      "Epoch 01019: val_loss did not improve\n",
      "2s - loss: 0.3078 - acc: 0.8792 - val_loss: 0.3028 - val_acc: 0.8943\n",
      "Epoch 1021/1500\n",
      "Epoch 01020: val_loss did not improve\n",
      "2s - loss: 0.2641 - acc: 0.9064 - val_loss: 0.4010 - val_acc: 0.8640\n",
      "Epoch 1022/1500\n",
      "Epoch 01021: val_loss did not improve\n",
      "2s - loss: 0.3117 - acc: 0.8816 - val_loss: 0.3167 - val_acc: 0.9033\n",
      "Epoch 1023/1500\n",
      "Epoch 01022: val_loss did not improve\n",
      "2s - loss: 0.2739 - acc: 0.8976 - val_loss: 0.3741 - val_acc: 0.8882\n",
      "Epoch 1024/1500\n",
      "Epoch 01023: val_loss did not improve\n",
      "2s - loss: 0.2805 - acc: 0.8904 - val_loss: 0.2994 - val_acc: 0.9063\n",
      "Epoch 1025/1500\n",
      "Epoch 01024: val_loss did not improve\n",
      "2s - loss: 0.3178 - acc: 0.8841 - val_loss: 0.3518 - val_acc: 0.9003\n",
      "Epoch 1026/1500\n",
      "Epoch 01025: val_loss did not improve\n",
      "2s - loss: 0.2696 - acc: 0.9031 - val_loss: 0.3399 - val_acc: 0.8912\n",
      "Epoch 1027/1500\n",
      "Epoch 01026: val_loss did not improve\n",
      "2s - loss: 0.3000 - acc: 0.8953 - val_loss: 0.4251 - val_acc: 0.8550\n",
      "Epoch 1028/1500\n",
      "Epoch 01027: val_loss did not improve\n",
      "2s - loss: 0.2911 - acc: 0.8969 - val_loss: 0.3945 - val_acc: 0.8973\n",
      "Epoch 1029/1500\n",
      "Epoch 01028: val_loss did not improve\n",
      "2s - loss: 0.2907 - acc: 0.9006 - val_loss: 0.3283 - val_acc: 0.9033\n",
      "Epoch 1030/1500\n",
      "Epoch 01029: val_loss did not improve\n",
      "2s - loss: 0.2925 - acc: 0.8950 - val_loss: 0.3607 - val_acc: 0.8792\n",
      "Epoch 1031/1500\n",
      "Epoch 01030: val_loss did not improve\n",
      "2s - loss: 0.2644 - acc: 0.8984 - val_loss: 0.2848 - val_acc: 0.9245\n",
      "Epoch 1032/1500\n",
      "Epoch 01031: val_loss did not improve\n",
      "2s - loss: 0.3099 - acc: 0.8864 - val_loss: 0.3005 - val_acc: 0.9033\n",
      "Epoch 1033/1500\n",
      "Epoch 01032: val_loss did not improve\n",
      "2s - loss: 0.2647 - acc: 0.9070 - val_loss: 0.3076 - val_acc: 0.8882\n",
      "Epoch 1034/1500\n",
      "Epoch 01033: val_loss did not improve\n",
      "2s - loss: 0.2878 - acc: 0.8959 - val_loss: 0.2634 - val_acc: 0.9184\n",
      "Epoch 1035/1500\n",
      "Epoch 01034: val_loss did not improve\n",
      "2s - loss: 0.2833 - acc: 0.9046 - val_loss: 0.2875 - val_acc: 0.9215\n",
      "Epoch 1036/1500\n",
      "Epoch 01035: val_loss did not improve\n",
      "2s - loss: 0.2711 - acc: 0.9079 - val_loss: 0.3068 - val_acc: 0.8943\n",
      "Epoch 1037/1500\n",
      "Epoch 01036: val_loss did not improve\n",
      "2s - loss: 0.3098 - acc: 0.8857 - val_loss: 0.2420 - val_acc: 0.9184\n",
      "Epoch 1038/1500\n",
      "Epoch 01037: val_loss did not improve\n",
      "2s - loss: 0.3028 - acc: 0.8928 - val_loss: 0.4002 - val_acc: 0.8882\n",
      "Epoch 1039/1500\n",
      "Epoch 01038: val_loss did not improve\n",
      "2s - loss: 0.2570 - acc: 0.9086 - val_loss: 0.3040 - val_acc: 0.8943\n",
      "Epoch 1040/1500\n",
      "Epoch 01039: val_loss did not improve\n",
      "2s - loss: 0.3271 - acc: 0.8816 - val_loss: 0.2900 - val_acc: 0.9063\n",
      "Epoch 1041/1500\n",
      "Epoch 01040: val_loss did not improve\n",
      "2s - loss: 0.2542 - acc: 0.9118 - val_loss: 0.3154 - val_acc: 0.8973\n",
      "Epoch 1042/1500\n",
      "Epoch 01041: val_loss did not improve\n",
      "2s - loss: 0.3119 - acc: 0.8935 - val_loss: 0.3245 - val_acc: 0.9003\n",
      "Epoch 1043/1500\n",
      "Epoch 01042: val_loss did not improve\n",
      "2s - loss: 0.2719 - acc: 0.8951 - val_loss: 0.3098 - val_acc: 0.8912\n",
      "Epoch 1044/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01043: val_loss did not improve\n",
      "2s - loss: 0.2973 - acc: 0.8903 - val_loss: 0.3155 - val_acc: 0.9033\n",
      "Epoch 1045/1500\n",
      "Epoch 01044: val_loss did not improve\n",
      "2s - loss: 0.2718 - acc: 0.9038 - val_loss: 0.3828 - val_acc: 0.8671\n",
      "Epoch 1046/1500\n",
      "Epoch 01045: val_loss did not improve\n",
      "2s - loss: 0.2884 - acc: 0.9016 - val_loss: 0.3342 - val_acc: 0.9003\n",
      "Epoch 1047/1500\n",
      "Epoch 01046: val_loss did not improve\n",
      "2s - loss: 0.2401 - acc: 0.9078 - val_loss: 0.3210 - val_acc: 0.9184\n",
      "Epoch 1048/1500\n",
      "Epoch 01047: val_loss did not improve\n",
      "2s - loss: 0.2833 - acc: 0.8870 - val_loss: 0.3319 - val_acc: 0.9033\n",
      "Epoch 1049/1500\n",
      "Epoch 01048: val_loss did not improve\n",
      "2s - loss: 0.2675 - acc: 0.9110 - val_loss: 0.3030 - val_acc: 0.8943\n",
      "Epoch 1050/1500\n",
      "Epoch 01049: val_loss did not improve\n",
      "2s - loss: 0.2893 - acc: 0.8847 - val_loss: 0.3033 - val_acc: 0.9063\n",
      "Epoch 1051/1500\n",
      "Epoch 01050: val_loss did not improve\n",
      "2s - loss: 0.2653 - acc: 0.9023 - val_loss: 0.2850 - val_acc: 0.9094\n",
      "Epoch 1052/1500\n",
      "Epoch 01051: val_loss did not improve\n",
      "2s - loss: 0.2850 - acc: 0.8938 - val_loss: 0.3014 - val_acc: 0.9124\n",
      "Epoch 1053/1500\n",
      "Epoch 01052: val_loss did not improve\n",
      "2s - loss: 0.3278 - acc: 0.8776 - val_loss: 0.4061 - val_acc: 0.8882\n",
      "Epoch 1054/1500\n",
      "Epoch 01053: val_loss did not improve\n",
      "2s - loss: 0.2885 - acc: 0.8919 - val_loss: 0.3675 - val_acc: 0.8852\n",
      "Epoch 1055/1500\n",
      "Epoch 01054: val_loss did not improve\n",
      "2s - loss: 0.3053 - acc: 0.8922 - val_loss: 0.3092 - val_acc: 0.9184\n",
      "Epoch 1056/1500\n",
      "Epoch 01055: val_loss did not improve\n",
      "2s - loss: 0.3062 - acc: 0.8855 - val_loss: 0.2761 - val_acc: 0.9154\n",
      "Epoch 1057/1500\n",
      "Epoch 01056: val_loss did not improve\n",
      "2s - loss: 0.3042 - acc: 0.8872 - val_loss: 0.2690 - val_acc: 0.9305\n",
      "Epoch 1058/1500\n",
      "Epoch 01057: val_loss did not improve\n",
      "2s - loss: 0.2779 - acc: 0.9017 - val_loss: 0.4926 - val_acc: 0.8520\n",
      "Epoch 1059/1500\n",
      "Epoch 01058: val_loss did not improve\n",
      "2s - loss: 0.2893 - acc: 0.8898 - val_loss: 0.3450 - val_acc: 0.9124\n",
      "Epoch 1060/1500\n",
      "Epoch 01059: val_loss did not improve\n",
      "2s - loss: 0.2642 - acc: 0.8968 - val_loss: 0.3257 - val_acc: 0.8640\n",
      "Epoch 1061/1500\n",
      "Epoch 01060: val_loss did not improve\n",
      "2s - loss: 0.3093 - acc: 0.8878 - val_loss: 0.2998 - val_acc: 0.9094\n",
      "Epoch 1062/1500\n",
      "Epoch 01061: val_loss did not improve\n",
      "2s - loss: 0.2470 - acc: 0.9062 - val_loss: 0.2860 - val_acc: 0.9094\n",
      "Epoch 1063/1500\n",
      "Epoch 01062: val_loss did not improve\n",
      "2s - loss: 0.2484 - acc: 0.9128 - val_loss: 0.3082 - val_acc: 0.9063\n",
      "Epoch 1064/1500\n",
      "Epoch 01063: val_loss did not improve\n",
      "2s - loss: 0.2607 - acc: 0.9024 - val_loss: 0.2814 - val_acc: 0.9094\n",
      "Epoch 1065/1500\n",
      "Epoch 01064: val_loss did not improve\n",
      "2s - loss: 0.3171 - acc: 0.8968 - val_loss: 0.2914 - val_acc: 0.9063\n",
      "Epoch 1066/1500\n",
      "Epoch 01065: val_loss did not improve\n",
      "2s - loss: 0.2605 - acc: 0.8953 - val_loss: 0.2965 - val_acc: 0.9003\n",
      "Epoch 1067/1500\n",
      "Epoch 01066: val_loss did not improve\n",
      "2s - loss: 0.2929 - acc: 0.8919 - val_loss: 0.3745 - val_acc: 0.8761\n",
      "Epoch 1068/1500\n",
      "Epoch 01067: val_loss did not improve\n",
      "2s - loss: 0.2922 - acc: 0.8872 - val_loss: 0.3502 - val_acc: 0.9063\n",
      "Epoch 1069/1500\n",
      "Epoch 01068: val_loss did not improve\n",
      "2s - loss: 0.2686 - acc: 0.9063 - val_loss: 0.4078 - val_acc: 0.8731\n",
      "Epoch 1070/1500\n",
      "Epoch 01069: val_loss did not improve\n",
      "2s - loss: 0.2368 - acc: 0.9159 - val_loss: 0.2876 - val_acc: 0.9275\n",
      "Epoch 1071/1500\n",
      "Epoch 01070: val_loss did not improve\n",
      "2s - loss: 0.2699 - acc: 0.9072 - val_loss: 0.4229 - val_acc: 0.8822\n",
      "Epoch 1072/1500\n",
      "Epoch 01071: val_loss did not improve\n",
      "2s - loss: 0.2845 - acc: 0.9063 - val_loss: 0.3790 - val_acc: 0.8912\n",
      "Epoch 1073/1500\n",
      "Epoch 01072: val_loss did not improve\n",
      "2s - loss: 0.2859 - acc: 0.8959 - val_loss: 0.3942 - val_acc: 0.8761\n",
      "Epoch 1074/1500\n",
      "Epoch 01073: val_loss did not improve\n",
      "2s - loss: 0.3200 - acc: 0.8855 - val_loss: 0.3223 - val_acc: 0.8973\n",
      "Epoch 1075/1500\n",
      "Epoch 01074: val_loss did not improve\n",
      "2s - loss: 0.2510 - acc: 0.9112 - val_loss: 0.3821 - val_acc: 0.8912\n",
      "Epoch 1076/1500\n",
      "Epoch 01075: val_loss did not improve\n",
      "2s - loss: 0.3030 - acc: 0.8915 - val_loss: 0.2956 - val_acc: 0.9124\n",
      "Epoch 1077/1500\n",
      "Epoch 01076: val_loss did not improve\n",
      "2s - loss: 0.2524 - acc: 0.9072 - val_loss: 0.3534 - val_acc: 0.8943\n",
      "Epoch 1078/1500\n",
      "Epoch 01077: val_loss did not improve\n",
      "2s - loss: 0.3074 - acc: 0.8855 - val_loss: 0.3497 - val_acc: 0.9033\n",
      "Epoch 1079/1500\n",
      "Epoch 01078: val_loss did not improve\n",
      "2s - loss: 0.3014 - acc: 0.8982 - val_loss: 0.3823 - val_acc: 0.8731\n",
      "Epoch 1080/1500\n",
      "Epoch 01079: val_loss did not improve\n",
      "2s - loss: 0.2968 - acc: 0.8936 - val_loss: 0.2779 - val_acc: 0.9094\n",
      "Epoch 1081/1500\n",
      "Epoch 01080: val_loss did not improve\n",
      "2s - loss: 0.2696 - acc: 0.9017 - val_loss: 0.2800 - val_acc: 0.9003\n",
      "Epoch 1082/1500\n",
      "Epoch 01081: val_loss did not improve\n",
      "2s - loss: 0.2912 - acc: 0.8912 - val_loss: 0.3328 - val_acc: 0.9033\n",
      "Epoch 1083/1500\n",
      "Epoch 01082: val_loss did not improve\n",
      "2s - loss: 0.2533 - acc: 0.9040 - val_loss: 0.2998 - val_acc: 0.9184\n",
      "Epoch 1084/1500\n",
      "Epoch 01083: val_loss did not improve\n",
      "2s - loss: 0.2914 - acc: 0.8849 - val_loss: 0.3504 - val_acc: 0.8882\n",
      "Epoch 1085/1500\n",
      "Epoch 01084: val_loss did not improve\n",
      "2s - loss: 0.2693 - acc: 0.9017 - val_loss: 0.4013 - val_acc: 0.8761\n",
      "Epoch 1086/1500\n",
      "Epoch 01085: val_loss did not improve\n",
      "2s - loss: 0.2631 - acc: 0.8994 - val_loss: 0.3002 - val_acc: 0.8943\n",
      "Epoch 1087/1500\n",
      "Epoch 01086: val_loss did not improve\n",
      "2s - loss: 0.2850 - acc: 0.8887 - val_loss: 0.3352 - val_acc: 0.8882\n",
      "Epoch 1088/1500\n",
      "Epoch 01087: val_loss did not improve\n",
      "2s - loss: 0.3016 - acc: 0.8883 - val_loss: 0.3343 - val_acc: 0.8943\n",
      "Epoch 1089/1500\n",
      "Epoch 01088: val_loss did not improve\n",
      "2s - loss: 0.3178 - acc: 0.8906 - val_loss: 0.4059 - val_acc: 0.8852\n",
      "Epoch 1090/1500\n",
      "Epoch 01089: val_loss did not improve\n",
      "2s - loss: 0.3091 - acc: 0.8904 - val_loss: 0.2847 - val_acc: 0.9184\n",
      "Epoch 1091/1500\n",
      "Epoch 01090: val_loss did not improve\n",
      "2s - loss: 0.3319 - acc: 0.8872 - val_loss: 0.3246 - val_acc: 0.9124\n",
      "Epoch 1092/1500\n",
      "Epoch 01091: val_loss did not improve\n",
      "2s - loss: 0.2751 - acc: 0.8915 - val_loss: 0.3097 - val_acc: 0.8852\n",
      "Epoch 1093/1500\n",
      "Epoch 01092: val_loss did not improve\n",
      "2s - loss: 0.2732 - acc: 0.9067 - val_loss: 0.3422 - val_acc: 0.9063\n",
      "Epoch 1094/1500\n",
      "Epoch 01093: val_loss did not improve\n",
      "2s - loss: 0.2878 - acc: 0.8976 - val_loss: 0.3627 - val_acc: 0.8792\n",
      "Epoch 1095/1500\n",
      "Epoch 01094: val_loss did not improve\n",
      "2s - loss: 0.2989 - acc: 0.8855 - val_loss: 0.3012 - val_acc: 0.9154\n",
      "Epoch 1096/1500\n",
      "Epoch 01095: val_loss did not improve\n",
      "2s - loss: 0.2932 - acc: 0.8945 - val_loss: 0.2911 - val_acc: 0.9033\n",
      "Epoch 1097/1500\n",
      "Epoch 01096: val_loss did not improve\n",
      "2s - loss: 0.2705 - acc: 0.8927 - val_loss: 0.3119 - val_acc: 0.9124\n",
      "Epoch 1098/1500\n",
      "Epoch 01097: val_loss did not improve\n",
      "2s - loss: 0.2812 - acc: 0.8999 - val_loss: 0.3342 - val_acc: 0.9154\n",
      "Epoch 1099/1500\n",
      "Epoch 01098: val_loss did not improve\n",
      "2s - loss: 0.2705 - acc: 0.8975 - val_loss: 0.2567 - val_acc: 0.9215\n",
      "Epoch 1100/1500\n",
      "Epoch 01099: val_loss did not improve\n",
      "2s - loss: 0.2454 - acc: 0.9054 - val_loss: 0.3099 - val_acc: 0.9245\n",
      "Epoch 1101/1500\n",
      "Epoch 01100: val_loss did not improve\n",
      "2s - loss: 0.3156 - acc: 0.8903 - val_loss: 0.3056 - val_acc: 0.8973\n",
      "Epoch 1102/1500\n",
      "Epoch 01101: val_loss did not improve\n",
      "2s - loss: 0.3049 - acc: 0.9010 - val_loss: 0.2358 - val_acc: 0.9063\n",
      "Epoch 1103/1500\n",
      "Epoch 01102: val_loss did not improve\n",
      "2s - loss: 0.3057 - acc: 0.8961 - val_loss: 0.2688 - val_acc: 0.9033\n",
      "Epoch 1104/1500\n",
      "Epoch 01103: val_loss did not improve\n",
      "2s - loss: 0.2854 - acc: 0.8976 - val_loss: 0.3408 - val_acc: 0.8973\n",
      "Epoch 1105/1500\n",
      "Epoch 01104: val_loss did not improve\n",
      "2s - loss: 0.2880 - acc: 0.8960 - val_loss: 0.3044 - val_acc: 0.9124\n",
      "Epoch 1106/1500\n",
      "Epoch 01105: val_loss did not improve\n",
      "2s - loss: 0.2858 - acc: 0.8920 - val_loss: 0.2803 - val_acc: 0.9215\n",
      "Epoch 1107/1500\n",
      "Epoch 01106: val_loss did not improve\n",
      "2s - loss: 0.2527 - acc: 0.9056 - val_loss: 0.3547 - val_acc: 0.8943\n",
      "Epoch 1108/1500\n",
      "Epoch 01107: val_loss did not improve\n",
      "2s - loss: 0.2725 - acc: 0.8985 - val_loss: 0.3166 - val_acc: 0.8792\n",
      "Epoch 1109/1500\n",
      "Epoch 01108: val_loss did not improve\n",
      "2s - loss: 0.2707 - acc: 0.9043 - val_loss: 0.2683 - val_acc: 0.9094\n",
      "Epoch 1110/1500\n",
      "Epoch 01109: val_loss did not improve\n",
      "2s - loss: 0.2900 - acc: 0.9022 - val_loss: 0.3156 - val_acc: 0.8882\n",
      "Epoch 1111/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01110: val_loss did not improve\n",
      "2s - loss: 0.2637 - acc: 0.9008 - val_loss: 0.3895 - val_acc: 0.8882\n",
      "Epoch 1112/1500\n",
      "Epoch 01111: val_loss did not improve\n",
      "2s - loss: 0.3080 - acc: 0.8910 - val_loss: 0.3172 - val_acc: 0.9033\n",
      "Epoch 1113/1500\n",
      "Epoch 01112: val_loss did not improve\n",
      "2s - loss: 0.2787 - acc: 0.9006 - val_loss: 0.3356 - val_acc: 0.8943\n",
      "Epoch 1114/1500\n",
      "Epoch 01113: val_loss did not improve\n",
      "2s - loss: 0.2895 - acc: 0.8935 - val_loss: 0.2791 - val_acc: 0.9305\n",
      "Epoch 1115/1500\n",
      "Epoch 01114: val_loss did not improve\n",
      "2s - loss: 0.3050 - acc: 0.8806 - val_loss: 0.2650 - val_acc: 0.9215\n",
      "Epoch 1116/1500\n",
      "Epoch 01115: val_loss did not improve\n",
      "2s - loss: 0.2805 - acc: 0.8952 - val_loss: 0.2835 - val_acc: 0.9063\n",
      "Epoch 1117/1500\n",
      "Epoch 01116: val_loss did not improve\n",
      "2s - loss: 0.2709 - acc: 0.8977 - val_loss: 0.3133 - val_acc: 0.8882\n",
      "Epoch 1118/1500\n",
      "Epoch 01117: val_loss did not improve\n",
      "2s - loss: 0.2782 - acc: 0.9073 - val_loss: 0.3390 - val_acc: 0.9033\n",
      "Epoch 1119/1500\n",
      "Epoch 01118: val_loss did not improve\n",
      "2s - loss: 0.2793 - acc: 0.8912 - val_loss: 0.3047 - val_acc: 0.9245\n",
      "Epoch 1120/1500\n",
      "Epoch 01119: val_loss did not improve\n",
      "2s - loss: 0.2535 - acc: 0.9047 - val_loss: 0.2236 - val_acc: 0.9215\n",
      "Epoch 1121/1500\n",
      "Epoch 01120: val_loss did not improve\n",
      "2s - loss: 0.2677 - acc: 0.9009 - val_loss: 0.4101 - val_acc: 0.8761\n",
      "Epoch 1122/1500\n",
      "Epoch 01121: val_loss did not improve\n",
      "2s - loss: 0.2656 - acc: 0.9031 - val_loss: 0.3382 - val_acc: 0.8852\n",
      "Epoch 1123/1500\n",
      "Epoch 01122: val_loss did not improve\n",
      "2s - loss: 0.2271 - acc: 0.9102 - val_loss: 0.2320 - val_acc: 0.9124\n",
      "Epoch 1124/1500\n",
      "Epoch 01123: val_loss did not improve\n",
      "2s - loss: 0.2698 - acc: 0.9048 - val_loss: 0.3060 - val_acc: 0.8943\n",
      "Epoch 1125/1500\n",
      "Epoch 01124: val_loss did not improve\n",
      "2s - loss: 0.2656 - acc: 0.8903 - val_loss: 0.3366 - val_acc: 0.8882\n",
      "Epoch 1126/1500\n",
      "Epoch 01125: val_loss did not improve\n",
      "2s - loss: 0.2690 - acc: 0.9041 - val_loss: 0.3953 - val_acc: 0.8761\n",
      "Epoch 1127/1500\n",
      "Epoch 01126: val_loss did not improve\n",
      "2s - loss: 0.2838 - acc: 0.8970 - val_loss: 0.3426 - val_acc: 0.8882\n",
      "Epoch 1128/1500\n",
      "Epoch 01127: val_loss did not improve\n",
      "2s - loss: 0.3015 - acc: 0.8707 - val_loss: 0.2990 - val_acc: 0.9033\n",
      "Epoch 1129/1500\n",
      "Epoch 01128: val_loss did not improve\n",
      "2s - loss: 0.2632 - acc: 0.9078 - val_loss: 0.3956 - val_acc: 0.8731\n",
      "Epoch 1130/1500\n",
      "Epoch 01129: val_loss did not improve\n",
      "2s - loss: 0.3037 - acc: 0.8889 - val_loss: 0.3816 - val_acc: 0.8912\n",
      "Epoch 1131/1500\n",
      "Epoch 01130: val_loss did not improve\n",
      "2s - loss: 0.2738 - acc: 0.9080 - val_loss: 0.2675 - val_acc: 0.9305\n",
      "Epoch 1132/1500\n",
      "Epoch 01131: val_loss did not improve\n",
      "2s - loss: 0.2803 - acc: 0.8975 - val_loss: 0.3120 - val_acc: 0.9124\n",
      "Epoch 1133/1500\n",
      "Epoch 01132: val_loss did not improve\n",
      "2s - loss: 0.2681 - acc: 0.8920 - val_loss: 0.2991 - val_acc: 0.9154\n",
      "Epoch 1134/1500\n",
      "Epoch 01133: val_loss did not improve\n",
      "2s - loss: 0.2457 - acc: 0.9063 - val_loss: 0.3173 - val_acc: 0.9094\n",
      "Epoch 1135/1500\n",
      "Epoch 01134: val_loss did not improve\n",
      "2s - loss: 0.2723 - acc: 0.8960 - val_loss: 0.2553 - val_acc: 0.9154\n",
      "Epoch 1136/1500\n",
      "Epoch 01135: val_loss did not improve\n",
      "2s - loss: 0.2403 - acc: 0.9178 - val_loss: 0.3766 - val_acc: 0.8943\n",
      "Epoch 1137/1500\n",
      "Epoch 01136: val_loss did not improve\n",
      "2s - loss: 0.2495 - acc: 0.9161 - val_loss: 0.2710 - val_acc: 0.9003\n",
      "Epoch 1138/1500\n",
      "Epoch 01137: val_loss did not improve\n",
      "2s - loss: 0.2560 - acc: 0.9111 - val_loss: 0.2562 - val_acc: 0.9003\n",
      "Epoch 1139/1500\n",
      "Epoch 01138: val_loss did not improve\n",
      "2s - loss: 0.2631 - acc: 0.9008 - val_loss: 0.2306 - val_acc: 0.9305\n",
      "Epoch 1140/1500\n",
      "Epoch 01139: val_loss did not improve\n",
      "2s - loss: 0.2795 - acc: 0.8872 - val_loss: 0.2504 - val_acc: 0.9335\n",
      "Epoch 1141/1500\n",
      "Epoch 01140: val_loss did not improve\n",
      "2s - loss: 0.2743 - acc: 0.9051 - val_loss: 0.3585 - val_acc: 0.8882\n",
      "Epoch 1142/1500\n",
      "Epoch 01141: val_loss did not improve\n",
      "2s - loss: 0.2512 - acc: 0.9167 - val_loss: 0.2630 - val_acc: 0.9275\n",
      "Epoch 1143/1500\n",
      "Epoch 01142: val_loss did not improve\n",
      "2s - loss: 0.2804 - acc: 0.8936 - val_loss: 0.3318 - val_acc: 0.8882\n",
      "Epoch 1144/1500\n",
      "Epoch 01143: val_loss did not improve\n",
      "2s - loss: 0.2758 - acc: 0.9000 - val_loss: 0.2798 - val_acc: 0.9033\n",
      "Epoch 1145/1500\n",
      "Epoch 01144: val_loss did not improve\n",
      "2s - loss: 0.2675 - acc: 0.9154 - val_loss: 0.3280 - val_acc: 0.8822\n",
      "Epoch 1146/1500\n",
      "Epoch 01145: val_loss did not improve\n",
      "2s - loss: 0.2645 - acc: 0.9086 - val_loss: 0.2527 - val_acc: 0.9275\n",
      "Epoch 1147/1500\n",
      "Epoch 01146: val_loss did not improve\n",
      "2s - loss: 0.3385 - acc: 0.8915 - val_loss: 0.2949 - val_acc: 0.9063\n",
      "Epoch 1148/1500\n",
      "Epoch 01147: val_loss did not improve\n",
      "2s - loss: 0.2389 - acc: 0.9128 - val_loss: 0.2575 - val_acc: 0.9275\n",
      "Epoch 1149/1500\n",
      "Epoch 01148: val_loss did not improve\n",
      "2s - loss: 0.2640 - acc: 0.8968 - val_loss: 0.3109 - val_acc: 0.9154\n",
      "Epoch 1150/1500\n",
      "Epoch 01149: val_loss did not improve\n",
      "2s - loss: 0.2677 - acc: 0.9080 - val_loss: 0.2821 - val_acc: 0.9184\n",
      "Epoch 1151/1500\n",
      "Epoch 01150: val_loss did not improve\n",
      "2s - loss: 0.2885 - acc: 0.8945 - val_loss: 0.2349 - val_acc: 0.9275\n",
      "Epoch 1152/1500\n",
      "Epoch 01151: val_loss did not improve\n",
      "2s - loss: 0.2586 - acc: 0.9030 - val_loss: 0.3370 - val_acc: 0.8882\n",
      "Epoch 1153/1500\n",
      "Epoch 01152: val_loss did not improve\n",
      "2s - loss: 0.2794 - acc: 0.9041 - val_loss: 0.3206 - val_acc: 0.9094\n",
      "Epoch 1154/1500\n",
      "Epoch 01153: val_loss did not improve\n",
      "2s - loss: 0.2629 - acc: 0.8998 - val_loss: 0.3472 - val_acc: 0.9033\n",
      "Epoch 1155/1500\n",
      "Epoch 01154: val_loss did not improve\n",
      "2s - loss: 0.2844 - acc: 0.9007 - val_loss: 0.2704 - val_acc: 0.9335\n",
      "Epoch 1156/1500\n",
      "Epoch 01155: val_loss did not improve\n",
      "2s - loss: 0.2590 - acc: 0.9071 - val_loss: 0.2757 - val_acc: 0.9154\n",
      "Epoch 1157/1500\n",
      "Epoch 01156: val_loss did not improve\n",
      "2s - loss: 0.2708 - acc: 0.8971 - val_loss: 0.3349 - val_acc: 0.8973\n",
      "Epoch 1158/1500\n",
      "Epoch 01157: val_loss did not improve\n",
      "2s - loss: 0.2981 - acc: 0.8968 - val_loss: 0.2976 - val_acc: 0.9063\n",
      "Epoch 1159/1500\n",
      "Epoch 01158: val_loss did not improve\n",
      "2s - loss: 0.2798 - acc: 0.9078 - val_loss: 0.3207 - val_acc: 0.9154\n",
      "Epoch 1160/1500\n",
      "Epoch 01159: val_loss did not improve\n",
      "2s - loss: 0.2594 - acc: 0.9087 - val_loss: 0.2775 - val_acc: 0.9366\n",
      "Epoch 1161/1500\n",
      "Epoch 01160: val_loss did not improve\n",
      "2s - loss: 0.2752 - acc: 0.9079 - val_loss: 0.3155 - val_acc: 0.8943\n",
      "Epoch 1162/1500\n",
      "Epoch 01161: val_loss did not improve\n",
      "2s - loss: 0.2794 - acc: 0.9008 - val_loss: 0.3018 - val_acc: 0.9154\n",
      "Epoch 1163/1500\n",
      "Epoch 01162: val_loss did not improve\n",
      "2s - loss: 0.2840 - acc: 0.9025 - val_loss: 0.2925 - val_acc: 0.9184\n",
      "Epoch 1164/1500\n",
      "Epoch 01163: val_loss did not improve\n",
      "2s - loss: 0.2351 - acc: 0.9136 - val_loss: 0.2082 - val_acc: 0.9215\n",
      "Epoch 1165/1500\n",
      "Epoch 01164: val_loss did not improve\n",
      "2s - loss: 0.2632 - acc: 0.9033 - val_loss: 0.2396 - val_acc: 0.9335\n",
      "Epoch 1166/1500\n",
      "Epoch 01165: val_loss did not improve\n",
      "2s - loss: 0.2487 - acc: 0.9009 - val_loss: 0.2828 - val_acc: 0.9275\n",
      "Epoch 1167/1500\n",
      "Epoch 01166: val_loss did not improve\n",
      "2s - loss: 0.2466 - acc: 0.8975 - val_loss: 0.2785 - val_acc: 0.9063\n",
      "Epoch 1168/1500\n",
      "Epoch 01167: val_loss did not improve\n",
      "2s - loss: 0.2834 - acc: 0.9017 - val_loss: 0.2544 - val_acc: 0.9215\n",
      "Epoch 1169/1500\n",
      "Epoch 01168: val_loss did not improve\n",
      "2s - loss: 0.2744 - acc: 0.8988 - val_loss: 0.2959 - val_acc: 0.9124\n",
      "Epoch 1170/1500\n",
      "Epoch 01169: val_loss did not improve\n",
      "2s - loss: 0.2584 - acc: 0.9110 - val_loss: 0.3464 - val_acc: 0.8852\n",
      "Epoch 1171/1500\n",
      "Epoch 01170: val_loss did not improve\n",
      "2s - loss: 0.2689 - acc: 0.8952 - val_loss: 0.3822 - val_acc: 0.8761\n",
      "Epoch 1172/1500\n",
      "Epoch 01171: val_loss did not improve\n",
      "2s - loss: 0.3037 - acc: 0.8938 - val_loss: 0.4023 - val_acc: 0.8731\n",
      "Epoch 1173/1500\n",
      "Epoch 01172: val_loss did not improve\n",
      "2s - loss: 0.2473 - acc: 0.9103 - val_loss: 0.2641 - val_acc: 0.9215\n",
      "Epoch 1174/1500\n",
      "Epoch 01173: val_loss did not improve\n",
      "2s - loss: 0.2473 - acc: 0.9183 - val_loss: 0.3095 - val_acc: 0.8882\n",
      "Epoch 1175/1500\n",
      "Epoch 01174: val_loss did not improve\n",
      "2s - loss: 0.2560 - acc: 0.9138 - val_loss: 0.2650 - val_acc: 0.9215\n",
      "Epoch 1176/1500\n",
      "Epoch 01175: val_loss did not improve\n",
      "2s - loss: 0.2666 - acc: 0.9041 - val_loss: 0.2632 - val_acc: 0.9184\n",
      "Epoch 1177/1500\n",
      "Epoch 01176: val_loss did not improve\n",
      "2s - loss: 0.2828 - acc: 0.9131 - val_loss: 0.4123 - val_acc: 0.8761\n",
      "Epoch 1178/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01177: val_loss did not improve\n",
      "2s - loss: 0.2388 - acc: 0.9127 - val_loss: 0.2715 - val_acc: 0.9154\n",
      "Epoch 1179/1500\n",
      "Epoch 01178: val_loss did not improve\n",
      "2s - loss: 0.2427 - acc: 0.9075 - val_loss: 0.2982 - val_acc: 0.9184\n",
      "Epoch 1180/1500\n",
      "Epoch 01179: val_loss did not improve\n",
      "2s - loss: 0.2359 - acc: 0.9159 - val_loss: 0.3839 - val_acc: 0.9033\n",
      "Epoch 1181/1500\n",
      "Epoch 01180: val_loss did not improve\n",
      "2s - loss: 0.2320 - acc: 0.9208 - val_loss: 0.3159 - val_acc: 0.9094\n",
      "Epoch 1182/1500\n",
      "Epoch 01181: val_loss did not improve\n",
      "2s - loss: 0.2795 - acc: 0.9001 - val_loss: 0.2929 - val_acc: 0.9003\n",
      "Epoch 1183/1500\n",
      "Epoch 01182: val_loss did not improve\n",
      "2s - loss: 0.2487 - acc: 0.9166 - val_loss: 0.3407 - val_acc: 0.8852\n",
      "Epoch 1184/1500\n",
      "Epoch 01183: val_loss did not improve\n",
      "2s - loss: 0.2631 - acc: 0.9056 - val_loss: 0.3075 - val_acc: 0.9063\n",
      "Epoch 1185/1500\n",
      "Epoch 01184: val_loss did not improve\n",
      "2s - loss: 0.2676 - acc: 0.9032 - val_loss: 0.2857 - val_acc: 0.9094\n",
      "Epoch 1186/1500\n",
      "Epoch 01185: val_loss did not improve\n",
      "2s - loss: 0.2420 - acc: 0.9032 - val_loss: 0.2422 - val_acc: 0.9215\n",
      "Epoch 1187/1500\n",
      "Epoch 01186: val_loss did not improve\n",
      "2s - loss: 0.2770 - acc: 0.8959 - val_loss: 0.4178 - val_acc: 0.8338\n",
      "Epoch 1188/1500\n",
      "Epoch 01187: val_loss did not improve\n",
      "2s - loss: 0.2767 - acc: 0.8943 - val_loss: 0.2633 - val_acc: 0.9245\n",
      "Epoch 1189/1500\n",
      "Epoch 01188: val_loss did not improve\n",
      "2s - loss: 0.2605 - acc: 0.9048 - val_loss: 0.2644 - val_acc: 0.9154\n",
      "Epoch 1190/1500\n",
      "Epoch 01189: val_loss did not improve\n",
      "2s - loss: 0.2537 - acc: 0.9144 - val_loss: 0.3212 - val_acc: 0.9063\n",
      "Epoch 1191/1500\n",
      "Epoch 01190: val_loss did not improve\n",
      "2s - loss: 0.2790 - acc: 0.9047 - val_loss: 0.2577 - val_acc: 0.9215\n",
      "Epoch 1192/1500\n",
      "Epoch 01191: val_loss did not improve\n",
      "2s - loss: 0.2655 - acc: 0.8992 - val_loss: 0.2636 - val_acc: 0.9335\n",
      "Epoch 1193/1500\n",
      "Epoch 01192: val_loss did not improve\n",
      "2s - loss: 0.2536 - acc: 0.9090 - val_loss: 0.2746 - val_acc: 0.9094\n",
      "Epoch 1194/1500\n",
      "Epoch 01193: val_loss did not improve\n",
      "2s - loss: 0.2957 - acc: 0.8965 - val_loss: 0.4041 - val_acc: 0.8852\n",
      "Epoch 1195/1500\n",
      "Epoch 01194: val_loss improved from 0.20632 to 0.20376, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.2417 - acc: 0.9055 - val_loss: 0.2038 - val_acc: 0.9275\n",
      "Epoch 1196/1500\n",
      "Epoch 01195: val_loss did not improve\n",
      "2s - loss: 0.2475 - acc: 0.9144 - val_loss: 0.3270 - val_acc: 0.9184\n",
      "Epoch 1197/1500\n",
      "Epoch 01196: val_loss did not improve\n",
      "2s - loss: 0.2534 - acc: 0.9062 - val_loss: 0.2683 - val_acc: 0.9124\n",
      "Epoch 1198/1500\n",
      "Epoch 01197: val_loss did not improve\n",
      "2s - loss: 0.2321 - acc: 0.9143 - val_loss: 0.2359 - val_acc: 0.9245\n",
      "Epoch 1199/1500\n",
      "Epoch 01198: val_loss improved from 0.20376 to 0.20072, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.2384 - acc: 0.9107 - val_loss: 0.2007 - val_acc: 0.9275\n",
      "Epoch 1200/1500\n",
      "Epoch 01199: val_loss did not improve\n",
      "2s - loss: 0.2447 - acc: 0.9128 - val_loss: 0.3336 - val_acc: 0.9063\n",
      "Epoch 1201/1500\n",
      "Epoch 01200: val_loss did not improve\n",
      "2s - loss: 0.2590 - acc: 0.9032 - val_loss: 0.2196 - val_acc: 0.9366\n",
      "Epoch 1202/1500\n",
      "Epoch 01201: val_loss did not improve\n",
      "2s - loss: 0.2433 - acc: 0.9033 - val_loss: 0.3741 - val_acc: 0.8761\n",
      "Epoch 1203/1500\n",
      "Epoch 01202: val_loss did not improve\n",
      "2s - loss: 0.2558 - acc: 0.9134 - val_loss: 0.2866 - val_acc: 0.9305\n",
      "Epoch 1204/1500\n",
      "Epoch 01203: val_loss did not improve\n",
      "2s - loss: 0.2571 - acc: 0.9104 - val_loss: 0.2762 - val_acc: 0.9124\n",
      "Epoch 1205/1500\n",
      "Epoch 01204: val_loss did not improve\n",
      "2s - loss: 0.2252 - acc: 0.9232 - val_loss: 0.2453 - val_acc: 0.9275\n",
      "Epoch 1206/1500\n",
      "Epoch 01205: val_loss did not improve\n",
      "2s - loss: 0.2456 - acc: 0.9064 - val_loss: 0.2684 - val_acc: 0.9245\n",
      "Epoch 1207/1500\n",
      "Epoch 01206: val_loss did not improve\n",
      "2s - loss: 0.2506 - acc: 0.9198 - val_loss: 0.2205 - val_acc: 0.9426\n",
      "Epoch 1208/1500\n",
      "Epoch 01207: val_loss did not improve\n",
      "2s - loss: 0.2454 - acc: 0.9119 - val_loss: 0.3118 - val_acc: 0.9184\n",
      "Epoch 1209/1500\n",
      "Epoch 01208: val_loss did not improve\n",
      "2s - loss: 0.2452 - acc: 0.9146 - val_loss: 0.2873 - val_acc: 0.9245\n",
      "Epoch 1210/1500\n",
      "Epoch 01209: val_loss did not improve\n",
      "2s - loss: 0.2556 - acc: 0.9086 - val_loss: 0.2853 - val_acc: 0.9275\n",
      "Epoch 1211/1500\n",
      "Epoch 01210: val_loss did not improve\n",
      "2s - loss: 0.2769 - acc: 0.8976 - val_loss: 0.4268 - val_acc: 0.8792\n",
      "Epoch 1212/1500\n",
      "Epoch 01211: val_loss did not improve\n",
      "2s - loss: 0.2651 - acc: 0.9072 - val_loss: 0.2717 - val_acc: 0.9245\n",
      "Epoch 1213/1500\n",
      "Epoch 01212: val_loss did not improve\n",
      "2s - loss: 0.2571 - acc: 0.9071 - val_loss: 0.3140 - val_acc: 0.8852\n",
      "Epoch 1214/1500\n",
      "Epoch 01213: val_loss did not improve\n",
      "2s - loss: 0.2204 - acc: 0.9311 - val_loss: 0.2534 - val_acc: 0.9426\n",
      "Epoch 1215/1500\n",
      "Epoch 01214: val_loss did not improve\n",
      "2s - loss: 0.2417 - acc: 0.9174 - val_loss: 0.2844 - val_acc: 0.9154\n",
      "Epoch 1216/1500\n",
      "Epoch 01215: val_loss did not improve\n",
      "2s - loss: 0.2322 - acc: 0.9166 - val_loss: 0.2403 - val_acc: 0.9094\n",
      "Epoch 1217/1500\n",
      "Epoch 01216: val_loss did not improve\n",
      "2s - loss: 0.2415 - acc: 0.9057 - val_loss: 0.2632 - val_acc: 0.9335\n",
      "Epoch 1218/1500\n",
      "Epoch 01217: val_loss did not improve\n",
      "2s - loss: 0.2388 - acc: 0.9087 - val_loss: 0.2802 - val_acc: 0.9033\n",
      "Epoch 1219/1500\n",
      "Epoch 01218: val_loss did not improve\n",
      "2s - loss: 0.2300 - acc: 0.9225 - val_loss: 0.2902 - val_acc: 0.9063\n",
      "Epoch 1220/1500\n",
      "Epoch 01219: val_loss did not improve\n",
      "2s - loss: 0.2210 - acc: 0.9110 - val_loss: 0.2263 - val_acc: 0.9154\n",
      "Epoch 1221/1500\n",
      "Epoch 01220: val_loss did not improve\n",
      "2s - loss: 0.2811 - acc: 0.8969 - val_loss: 0.3576 - val_acc: 0.8882\n",
      "Epoch 1222/1500\n",
      "Epoch 01221: val_loss did not improve\n",
      "2s - loss: 0.2628 - acc: 0.9052 - val_loss: 0.3704 - val_acc: 0.8792\n",
      "Epoch 1223/1500\n",
      "Epoch 01222: val_loss did not improve\n",
      "2s - loss: 0.2396 - acc: 0.9136 - val_loss: 0.2080 - val_acc: 0.9396\n",
      "Epoch 1224/1500\n",
      "Epoch 01223: val_loss did not improve\n",
      "2s - loss: 0.2408 - acc: 0.9202 - val_loss: 0.2900 - val_acc: 0.9184\n",
      "Epoch 1225/1500\n",
      "Epoch 01224: val_loss did not improve\n",
      "2s - loss: 0.2445 - acc: 0.9127 - val_loss: 0.2940 - val_acc: 0.9184\n",
      "Epoch 1226/1500\n",
      "Epoch 01225: val_loss did not improve\n",
      "2s - loss: 0.2648 - acc: 0.9166 - val_loss: 0.2505 - val_acc: 0.9154\n",
      "Epoch 1227/1500\n",
      "Epoch 01226: val_loss did not improve\n",
      "2s - loss: 0.2753 - acc: 0.9046 - val_loss: 0.2210 - val_acc: 0.9275\n",
      "Epoch 1228/1500\n",
      "Epoch 01227: val_loss did not improve\n",
      "2s - loss: 0.2716 - acc: 0.8978 - val_loss: 0.2828 - val_acc: 0.9426\n",
      "Epoch 1229/1500\n",
      "Epoch 01228: val_loss did not improve\n",
      "2s - loss: 0.2452 - acc: 0.9090 - val_loss: 0.3499 - val_acc: 0.8882\n",
      "Epoch 1230/1500\n",
      "Epoch 01229: val_loss did not improve\n",
      "2s - loss: 0.2569 - acc: 0.9104 - val_loss: 0.2623 - val_acc: 0.9335\n",
      "Epoch 1231/1500\n",
      "Epoch 01230: val_loss did not improve\n",
      "2s - loss: 0.2672 - acc: 0.8977 - val_loss: 0.2870 - val_acc: 0.9215\n",
      "Epoch 1232/1500\n",
      "Epoch 01231: val_loss did not improve\n",
      "2s - loss: 0.2327 - acc: 0.9230 - val_loss: 0.3138 - val_acc: 0.9275\n",
      "Epoch 1233/1500\n",
      "Epoch 01232: val_loss did not improve\n",
      "2s - loss: 0.2243 - acc: 0.9214 - val_loss: 0.2753 - val_acc: 0.9124\n",
      "Epoch 1234/1500\n",
      "Epoch 01233: val_loss did not improve\n",
      "2s - loss: 0.2461 - acc: 0.9095 - val_loss: 0.2284 - val_acc: 0.9124\n",
      "Epoch 1235/1500\n",
      "Epoch 01234: val_loss did not improve\n",
      "2s - loss: 0.2466 - acc: 0.9041 - val_loss: 0.2881 - val_acc: 0.9124\n",
      "Epoch 1236/1500\n",
      "Epoch 01235: val_loss did not improve\n",
      "2s - loss: 0.2743 - acc: 0.8977 - val_loss: 0.3668 - val_acc: 0.8973\n",
      "Epoch 1237/1500\n",
      "Epoch 01236: val_loss did not improve\n",
      "2s - loss: 0.2615 - acc: 0.9040 - val_loss: 0.4179 - val_acc: 0.8852\n",
      "Epoch 1238/1500\n",
      "Epoch 01237: val_loss did not improve\n",
      "2s - loss: 0.2058 - acc: 0.9185 - val_loss: 0.2861 - val_acc: 0.9305\n",
      "Epoch 1239/1500\n",
      "Epoch 01238: val_loss did not improve\n",
      "2s - loss: 0.2314 - acc: 0.9103 - val_loss: 0.3129 - val_acc: 0.8973\n",
      "Epoch 1240/1500\n",
      "Epoch 01239: val_loss did not improve\n",
      "2s - loss: 0.2324 - acc: 0.9194 - val_loss: 0.3011 - val_acc: 0.8912\n",
      "Epoch 1241/1500\n",
      "Epoch 01240: val_loss did not improve\n",
      "2s - loss: 0.2804 - acc: 0.8975 - val_loss: 0.2474 - val_acc: 0.9305\n",
      "Epoch 1242/1500\n",
      "Epoch 01241: val_loss did not improve\n",
      "2s - loss: 0.2307 - acc: 0.9206 - val_loss: 0.2343 - val_acc: 0.9366\n",
      "Epoch 1243/1500\n",
      "Epoch 01242: val_loss did not improve\n",
      "2s - loss: 0.2828 - acc: 0.9007 - val_loss: 0.3007 - val_acc: 0.9063\n",
      "Epoch 1244/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01243: val_loss did not improve\n",
      "2s - loss: 0.2651 - acc: 0.9087 - val_loss: 0.2683 - val_acc: 0.9275\n",
      "Epoch 1245/1500\n",
      "Epoch 01244: val_loss did not improve\n",
      "2s - loss: 0.2472 - acc: 0.9057 - val_loss: 0.3312 - val_acc: 0.8943\n",
      "Epoch 1246/1500\n",
      "Epoch 01245: val_loss did not improve\n",
      "2s - loss: 0.2440 - acc: 0.9130 - val_loss: 0.2384 - val_acc: 0.9154\n",
      "Epoch 1247/1500\n",
      "Epoch 01246: val_loss did not improve\n",
      "2s - loss: 0.2366 - acc: 0.9134 - val_loss: 0.2690 - val_acc: 0.9305\n",
      "Epoch 1248/1500\n",
      "Epoch 01247: val_loss did not improve\n",
      "2s - loss: 0.2187 - acc: 0.9295 - val_loss: 0.3010 - val_acc: 0.9003\n",
      "Epoch 1249/1500\n",
      "Epoch 01248: val_loss did not improve\n",
      "2s - loss: 0.2156 - acc: 0.9191 - val_loss: 0.2729 - val_acc: 0.9094\n",
      "Epoch 1250/1500\n",
      "Epoch 01249: val_loss did not improve\n",
      "2s - loss: 0.2029 - acc: 0.9159 - val_loss: 0.2208 - val_acc: 0.9305\n",
      "Epoch 1251/1500\n",
      "Epoch 01250: val_loss did not improve\n",
      "2s - loss: 0.2433 - acc: 0.9084 - val_loss: 0.2608 - val_acc: 0.9275\n",
      "Epoch 1252/1500\n",
      "Epoch 01251: val_loss did not improve\n",
      "2s - loss: 0.2214 - acc: 0.9167 - val_loss: 0.3066 - val_acc: 0.9033\n",
      "Epoch 1253/1500\n",
      "Epoch 01252: val_loss did not improve\n",
      "2s - loss: 0.2052 - acc: 0.9272 - val_loss: 0.2668 - val_acc: 0.9063\n",
      "Epoch 1254/1500\n",
      "Epoch 01253: val_loss did not improve\n",
      "2s - loss: 0.2784 - acc: 0.8983 - val_loss: 0.3022 - val_acc: 0.9003\n",
      "Epoch 1255/1500\n",
      "Epoch 01254: val_loss did not improve\n",
      "2s - loss: 0.2161 - acc: 0.9199 - val_loss: 0.2722 - val_acc: 0.9275\n",
      "Epoch 1256/1500\n",
      "Epoch 01255: val_loss did not improve\n",
      "2s - loss: 0.2779 - acc: 0.8953 - val_loss: 0.2995 - val_acc: 0.9245\n",
      "Epoch 1257/1500\n",
      "Epoch 01256: val_loss did not improve\n",
      "2s - loss: 0.2266 - acc: 0.9135 - val_loss: 0.2628 - val_acc: 0.9215\n",
      "Epoch 1258/1500\n",
      "Epoch 01257: val_loss did not improve\n",
      "2s - loss: 0.1905 - acc: 0.9287 - val_loss: 0.2757 - val_acc: 0.9305\n",
      "Epoch 1259/1500\n",
      "Epoch 01258: val_loss did not improve\n",
      "2s - loss: 0.2557 - acc: 0.9182 - val_loss: 0.2629 - val_acc: 0.9366\n",
      "Epoch 1260/1500\n",
      "Epoch 01259: val_loss did not improve\n",
      "2s - loss: 0.2141 - acc: 0.9217 - val_loss: 0.3072 - val_acc: 0.9094\n",
      "Epoch 1261/1500\n",
      "Epoch 01260: val_loss did not improve\n",
      "2s - loss: 0.2748 - acc: 0.9039 - val_loss: 0.2886 - val_acc: 0.9124\n",
      "Epoch 1262/1500\n",
      "Epoch 01261: val_loss did not improve\n",
      "2s - loss: 0.2188 - acc: 0.9166 - val_loss: 0.2546 - val_acc: 0.9063\n",
      "Epoch 1263/1500\n",
      "Epoch 01262: val_loss did not improve\n",
      "2s - loss: 0.2461 - acc: 0.9078 - val_loss: 0.2886 - val_acc: 0.9335\n",
      "Epoch 1264/1500\n",
      "Epoch 01263: val_loss did not improve\n",
      "2s - loss: 0.2382 - acc: 0.9162 - val_loss: 0.2551 - val_acc: 0.9184\n",
      "Epoch 1265/1500\n",
      "Epoch 01264: val_loss did not improve\n",
      "2s - loss: 0.2539 - acc: 0.9102 - val_loss: 0.2611 - val_acc: 0.9245\n",
      "Epoch 1266/1500\n",
      "Epoch 01265: val_loss did not improve\n",
      "2s - loss: 0.2823 - acc: 0.9032 - val_loss: 0.2738 - val_acc: 0.9154\n",
      "Epoch 1267/1500\n",
      "Epoch 01266: val_loss did not improve\n",
      "2s - loss: 0.2484 - acc: 0.9073 - val_loss: 0.2221 - val_acc: 0.9154\n",
      "Epoch 1268/1500\n",
      "Epoch 01267: val_loss did not improve\n",
      "2s - loss: 0.2746 - acc: 0.9107 - val_loss: 0.3225 - val_acc: 0.8882\n",
      "Epoch 1269/1500\n",
      "Epoch 01268: val_loss did not improve\n",
      "2s - loss: 0.2135 - acc: 0.9178 - val_loss: 0.2481 - val_acc: 0.9245\n",
      "Epoch 1270/1500\n",
      "Epoch 01269: val_loss did not improve\n",
      "2s - loss: 0.2511 - acc: 0.9081 - val_loss: 0.2651 - val_acc: 0.9245\n",
      "Epoch 1271/1500\n",
      "Epoch 01270: val_loss did not improve\n",
      "2s - loss: 0.2320 - acc: 0.9102 - val_loss: 0.2281 - val_acc: 0.9275\n",
      "Epoch 1272/1500\n",
      "Epoch 01271: val_loss did not improve\n",
      "2s - loss: 0.2691 - acc: 0.9106 - val_loss: 0.3646 - val_acc: 0.9033\n",
      "Epoch 1273/1500\n",
      "Epoch 01272: val_loss did not improve\n",
      "2s - loss: 0.2264 - acc: 0.9130 - val_loss: 0.2936 - val_acc: 0.8882\n",
      "Epoch 1274/1500\n",
      "Epoch 01273: val_loss did not improve\n",
      "2s - loss: 0.2351 - acc: 0.9151 - val_loss: 0.3215 - val_acc: 0.9094\n",
      "Epoch 1275/1500\n",
      "Epoch 01274: val_loss did not improve\n",
      "2s - loss: 0.2309 - acc: 0.9230 - val_loss: 0.2688 - val_acc: 0.9033\n",
      "Epoch 1276/1500\n",
      "Epoch 01275: val_loss did not improve\n",
      "2s - loss: 0.2264 - acc: 0.9143 - val_loss: 0.2786 - val_acc: 0.9124\n",
      "Epoch 1277/1500\n",
      "Epoch 01276: val_loss did not improve\n",
      "2s - loss: 0.2341 - acc: 0.9095 - val_loss: 0.2468 - val_acc: 0.9184\n",
      "Epoch 1278/1500\n",
      "Epoch 01277: val_loss did not improve\n",
      "2s - loss: 0.2333 - acc: 0.9064 - val_loss: 0.2234 - val_acc: 0.9426\n",
      "Epoch 1279/1500\n",
      "Epoch 01278: val_loss did not improve\n",
      "2s - loss: 0.2597 - acc: 0.8962 - val_loss: 0.3003 - val_acc: 0.9033\n",
      "Epoch 1280/1500\n",
      "Epoch 01279: val_loss did not improve\n",
      "2s - loss: 0.2430 - acc: 0.9186 - val_loss: 0.2759 - val_acc: 0.8973\n",
      "Epoch 1281/1500\n",
      "Epoch 01280: val_loss did not improve\n",
      "2s - loss: 0.2246 - acc: 0.9238 - val_loss: 0.2747 - val_acc: 0.9154\n",
      "Epoch 1282/1500\n",
      "Epoch 01281: val_loss did not improve\n",
      "2s - loss: 0.2584 - acc: 0.9067 - val_loss: 0.3032 - val_acc: 0.9124\n",
      "Epoch 1283/1500\n",
      "Epoch 01282: val_loss did not improve\n",
      "2s - loss: 0.2562 - acc: 0.9032 - val_loss: 0.3366 - val_acc: 0.9094\n",
      "Epoch 1284/1500\n",
      "Epoch 01283: val_loss did not improve\n",
      "2s - loss: 0.2311 - acc: 0.9263 - val_loss: 0.3403 - val_acc: 0.9124\n",
      "Epoch 1285/1500\n",
      "Epoch 01284: val_loss did not improve\n",
      "2s - loss: 0.1976 - acc: 0.9335 - val_loss: 0.2666 - val_acc: 0.9335\n",
      "Epoch 1286/1500\n",
      "Epoch 01285: val_loss did not improve\n",
      "2s - loss: 0.2733 - acc: 0.9064 - val_loss: 0.2833 - val_acc: 0.8943\n",
      "Epoch 1287/1500\n",
      "Epoch 01286: val_loss did not improve\n",
      "2s - loss: 0.2363 - acc: 0.9259 - val_loss: 0.4074 - val_acc: 0.8792\n",
      "Epoch 1288/1500\n",
      "Epoch 01287: val_loss did not improve\n",
      "2s - loss: 0.2406 - acc: 0.9096 - val_loss: 0.2895 - val_acc: 0.9033\n",
      "Epoch 1289/1500\n",
      "Epoch 01288: val_loss did not improve\n",
      "2s - loss: 0.2529 - acc: 0.9136 - val_loss: 0.2767 - val_acc: 0.9305\n",
      "Epoch 1290/1500\n",
      "Epoch 01289: val_loss did not improve\n",
      "2s - loss: 0.2649 - acc: 0.9001 - val_loss: 0.2597 - val_acc: 0.9215\n",
      "Epoch 1291/1500\n",
      "Epoch 01290: val_loss did not improve\n",
      "2s - loss: 0.2286 - acc: 0.9142 - val_loss: 0.2676 - val_acc: 0.9245\n",
      "Epoch 1292/1500\n",
      "Epoch 01291: val_loss did not improve\n",
      "2s - loss: 0.2126 - acc: 0.9191 - val_loss: 0.3024 - val_acc: 0.9124\n",
      "Epoch 1293/1500\n",
      "Epoch 01292: val_loss did not improve\n",
      "2s - loss: 0.2439 - acc: 0.9096 - val_loss: 0.2127 - val_acc: 0.9215\n",
      "Epoch 1294/1500\n",
      "Epoch 01293: val_loss did not improve\n",
      "2s - loss: 0.2669 - acc: 0.9039 - val_loss: 0.2573 - val_acc: 0.9335\n",
      "Epoch 1295/1500\n",
      "Epoch 01294: val_loss did not improve\n",
      "2s - loss: 0.2275 - acc: 0.9138 - val_loss: 0.2261 - val_acc: 0.9335\n",
      "Epoch 1296/1500\n",
      "Epoch 01295: val_loss improved from 0.20072 to 0.17793, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.2668 - acc: 0.9015 - val_loss: 0.1779 - val_acc: 0.9426\n",
      "Epoch 1297/1500\n",
      "Epoch 01296: val_loss did not improve\n",
      "2s - loss: 0.2286 - acc: 0.9178 - val_loss: 0.2905 - val_acc: 0.9215\n",
      "Epoch 1298/1500\n",
      "Epoch 01297: val_loss did not improve\n",
      "2s - loss: 0.2463 - acc: 0.9134 - val_loss: 0.3227 - val_acc: 0.9184\n",
      "Epoch 1299/1500\n",
      "Epoch 01298: val_loss did not improve\n",
      "2s - loss: 0.2746 - acc: 0.9122 - val_loss: 0.3086 - val_acc: 0.8943\n",
      "Epoch 1300/1500\n",
      "Epoch 01299: val_loss did not improve\n",
      "2s - loss: 0.2186 - acc: 0.9219 - val_loss: 0.2619 - val_acc: 0.9094\n",
      "Epoch 1301/1500\n",
      "Epoch 01300: val_loss did not improve\n",
      "2s - loss: 0.2175 - acc: 0.9174 - val_loss: 0.2442 - val_acc: 0.9154\n",
      "Epoch 1302/1500\n",
      "Epoch 01301: val_loss did not improve\n",
      "2s - loss: 0.2313 - acc: 0.9150 - val_loss: 0.3774 - val_acc: 0.8943\n",
      "Epoch 1303/1500\n",
      "Epoch 01302: val_loss did not improve\n",
      "2s - loss: 0.2526 - acc: 0.9055 - val_loss: 0.2528 - val_acc: 0.9275\n",
      "Epoch 1304/1500\n",
      "Epoch 01303: val_loss did not improve\n",
      "2s - loss: 0.2528 - acc: 0.9159 - val_loss: 0.2968 - val_acc: 0.9275\n",
      "Epoch 1305/1500\n",
      "Epoch 01304: val_loss did not improve\n",
      "2s - loss: 0.2429 - acc: 0.9071 - val_loss: 0.2621 - val_acc: 0.9215\n",
      "Epoch 1306/1500\n",
      "Epoch 01305: val_loss did not improve\n",
      "2s - loss: 0.2349 - acc: 0.9136 - val_loss: 0.3202 - val_acc: 0.8973\n",
      "Epoch 1307/1500\n",
      "Epoch 01306: val_loss did not improve\n",
      "2s - loss: 0.2231 - acc: 0.9175 - val_loss: 0.2914 - val_acc: 0.9124\n",
      "Epoch 1308/1500\n",
      "Epoch 01307: val_loss did not improve\n",
      "2s - loss: 0.2433 - acc: 0.9136 - val_loss: 0.2086 - val_acc: 0.9305\n",
      "Epoch 1309/1500\n",
      "Epoch 01308: val_loss did not improve\n",
      "2s - loss: 0.2810 - acc: 0.8988 - val_loss: 0.3192 - val_acc: 0.8912\n",
      "Epoch 1310/1500\n",
      "Epoch 01309: val_loss improved from 0.17793 to 0.16771, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.2425 - acc: 0.9087 - val_loss: 0.1677 - val_acc: 0.9577\n",
      "Epoch 1311/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01310: val_loss did not improve\n",
      "2s - loss: 0.2547 - acc: 0.9104 - val_loss: 0.3194 - val_acc: 0.9063\n",
      "Epoch 1312/1500\n",
      "Epoch 01311: val_loss did not improve\n",
      "2s - loss: 0.2447 - acc: 0.9122 - val_loss: 0.2697 - val_acc: 0.9184\n",
      "Epoch 1313/1500\n",
      "Epoch 01312: val_loss did not improve\n",
      "2s - loss: 0.2634 - acc: 0.9084 - val_loss: 0.4231 - val_acc: 0.8671\n",
      "Epoch 1314/1500\n",
      "Epoch 01313: val_loss did not improve\n",
      "2s - loss: 0.2479 - acc: 0.9081 - val_loss: 0.4319 - val_acc: 0.8640\n",
      "Epoch 1315/1500\n",
      "Epoch 01314: val_loss did not improve\n",
      "2s - loss: 0.2077 - acc: 0.9263 - val_loss: 0.2559 - val_acc: 0.9245\n",
      "Epoch 1316/1500\n",
      "Epoch 01315: val_loss did not improve\n",
      "2s - loss: 0.2127 - acc: 0.9222 - val_loss: 0.2511 - val_acc: 0.9245\n",
      "Epoch 1317/1500\n",
      "Epoch 01316: val_loss did not improve\n",
      "2s - loss: 0.2301 - acc: 0.9159 - val_loss: 0.2360 - val_acc: 0.9245\n",
      "Epoch 1318/1500\n",
      "Epoch 01317: val_loss did not improve\n",
      "2s - loss: 0.2091 - acc: 0.9185 - val_loss: 0.2647 - val_acc: 0.9215\n",
      "Epoch 1319/1500\n",
      "Epoch 01318: val_loss did not improve\n",
      "2s - loss: 0.2065 - acc: 0.9222 - val_loss: 0.2893 - val_acc: 0.9124\n",
      "Epoch 1320/1500\n",
      "Epoch 01319: val_loss did not improve\n",
      "2s - loss: 0.2004 - acc: 0.9216 - val_loss: 0.3035 - val_acc: 0.9033\n",
      "Epoch 1321/1500\n",
      "Epoch 01320: val_loss did not improve\n",
      "2s - loss: 0.2045 - acc: 0.9238 - val_loss: 0.3153 - val_acc: 0.9245\n",
      "Epoch 1322/1500\n",
      "Epoch 01321: val_loss did not improve\n",
      "2s - loss: 0.2320 - acc: 0.9238 - val_loss: 0.2660 - val_acc: 0.9184\n",
      "Epoch 1323/1500\n",
      "Epoch 01322: val_loss did not improve\n",
      "2s - loss: 0.2575 - acc: 0.9015 - val_loss: 0.2555 - val_acc: 0.9245\n",
      "Epoch 1324/1500\n",
      "Epoch 01323: val_loss did not improve\n",
      "2s - loss: 0.2449 - acc: 0.9118 - val_loss: 0.2431 - val_acc: 0.9154\n",
      "Epoch 1325/1500\n",
      "Epoch 01324: val_loss did not improve\n",
      "2s - loss: 0.2223 - acc: 0.9248 - val_loss: 0.3210 - val_acc: 0.9033\n",
      "Epoch 1326/1500\n",
      "Epoch 01325: val_loss did not improve\n",
      "2s - loss: 0.2491 - acc: 0.9071 - val_loss: 0.2042 - val_acc: 0.9275\n",
      "Epoch 1327/1500\n",
      "Epoch 01326: val_loss did not improve\n",
      "2s - loss: 0.2164 - acc: 0.9272 - val_loss: 0.2654 - val_acc: 0.9184\n",
      "Epoch 1328/1500\n",
      "Epoch 01327: val_loss did not improve\n",
      "2s - loss: 0.2602 - acc: 0.9062 - val_loss: 0.3157 - val_acc: 0.9305\n",
      "Epoch 1329/1500\n",
      "Epoch 01328: val_loss did not improve\n",
      "2s - loss: 0.2078 - acc: 0.9296 - val_loss: 0.2342 - val_acc: 0.9275\n",
      "Epoch 1330/1500\n",
      "Epoch 01329: val_loss did not improve\n",
      "2s - loss: 0.2339 - acc: 0.9246 - val_loss: 0.3393 - val_acc: 0.9003\n",
      "Epoch 1331/1500\n",
      "Epoch 01330: val_loss did not improve\n",
      "2s - loss: 0.2133 - acc: 0.9241 - val_loss: 0.3083 - val_acc: 0.9063\n",
      "Epoch 1332/1500\n",
      "Epoch 01331: val_loss did not improve\n",
      "2s - loss: 0.2333 - acc: 0.9111 - val_loss: 0.2111 - val_acc: 0.9396\n",
      "Epoch 1333/1500\n",
      "Epoch 01332: val_loss did not improve\n",
      "2s - loss: 0.1960 - acc: 0.9226 - val_loss: 0.2527 - val_acc: 0.9215\n",
      "Epoch 1334/1500\n",
      "Epoch 01333: val_loss did not improve\n",
      "2s - loss: 0.2282 - acc: 0.9150 - val_loss: 0.2532 - val_acc: 0.9215\n",
      "Epoch 1335/1500\n",
      "Epoch 01334: val_loss did not improve\n",
      "2s - loss: 0.2067 - acc: 0.9248 - val_loss: 0.2870 - val_acc: 0.9124\n",
      "Epoch 1336/1500\n",
      "Epoch 01335: val_loss did not improve\n",
      "2s - loss: 0.2107 - acc: 0.9216 - val_loss: 0.2650 - val_acc: 0.9305\n",
      "Epoch 1337/1500\n",
      "Epoch 01336: val_loss did not improve\n",
      "2s - loss: 0.2137 - acc: 0.9272 - val_loss: 0.2502 - val_acc: 0.9215\n",
      "Epoch 1338/1500\n",
      "Epoch 01337: val_loss did not improve\n",
      "2s - loss: 0.2248 - acc: 0.9187 - val_loss: 0.2705 - val_acc: 0.9063\n",
      "Epoch 1339/1500\n",
      "Epoch 01338: val_loss did not improve\n",
      "2s - loss: 0.2393 - acc: 0.9040 - val_loss: 0.3802 - val_acc: 0.8882\n",
      "Epoch 1340/1500\n",
      "Epoch 01339: val_loss did not improve\n",
      "2s - loss: 0.2451 - acc: 0.9119 - val_loss: 0.2896 - val_acc: 0.9215\n",
      "Epoch 1341/1500\n",
      "Epoch 01340: val_loss did not improve\n",
      "2s - loss: 0.2244 - acc: 0.9096 - val_loss: 0.2665 - val_acc: 0.9305\n",
      "Epoch 1342/1500\n",
      "Epoch 01341: val_loss did not improve\n",
      "2s - loss: 0.2338 - acc: 0.9143 - val_loss: 0.3064 - val_acc: 0.9033\n",
      "Epoch 1343/1500\n",
      "Epoch 01342: val_loss did not improve\n",
      "2s - loss: 0.2326 - acc: 0.9195 - val_loss: 0.3759 - val_acc: 0.9033\n",
      "Epoch 1344/1500\n",
      "Epoch 01343: val_loss did not improve\n",
      "2s - loss: 0.2217 - acc: 0.9208 - val_loss: 0.2387 - val_acc: 0.9245\n",
      "Epoch 1345/1500\n",
      "Epoch 01344: val_loss did not improve\n",
      "2s - loss: 0.2221 - acc: 0.9143 - val_loss: 0.2741 - val_acc: 0.9275\n",
      "Epoch 1346/1500\n",
      "Epoch 01345: val_loss did not improve\n",
      "2s - loss: 0.2186 - acc: 0.9238 - val_loss: 0.3054 - val_acc: 0.8882\n",
      "Epoch 1347/1500\n",
      "Epoch 01346: val_loss did not improve\n",
      "2s - loss: 0.2039 - acc: 0.9289 - val_loss: 0.2027 - val_acc: 0.9275\n",
      "Epoch 1348/1500\n",
      "Epoch 01347: val_loss did not improve\n",
      "2s - loss: 0.2475 - acc: 0.9123 - val_loss: 0.2913 - val_acc: 0.9305\n",
      "Epoch 1349/1500\n",
      "Epoch 01348: val_loss did not improve\n",
      "2s - loss: 0.2251 - acc: 0.9208 - val_loss: 0.2523 - val_acc: 0.9215\n",
      "Epoch 1350/1500\n",
      "Epoch 01349: val_loss did not improve\n",
      "2s - loss: 0.2426 - acc: 0.9210 - val_loss: 0.3328 - val_acc: 0.9063\n",
      "Epoch 1351/1500\n",
      "Epoch 01350: val_loss did not improve\n",
      "2s - loss: 0.2271 - acc: 0.9161 - val_loss: 0.2308 - val_acc: 0.9215\n",
      "Epoch 1352/1500\n",
      "Epoch 01351: val_loss did not improve\n",
      "2s - loss: 0.2150 - acc: 0.9199 - val_loss: 0.2558 - val_acc: 0.9275\n",
      "Epoch 1353/1500\n",
      "Epoch 01352: val_loss did not improve\n",
      "2s - loss: 0.1931 - acc: 0.9233 - val_loss: 0.3111 - val_acc: 0.9154\n",
      "Epoch 1354/1500\n",
      "Epoch 01353: val_loss did not improve\n",
      "2s - loss: 0.2281 - acc: 0.9127 - val_loss: 0.2846 - val_acc: 0.9063\n",
      "Epoch 1355/1500\n",
      "Epoch 01354: val_loss did not improve\n",
      "2s - loss: 0.2193 - acc: 0.9216 - val_loss: 0.2475 - val_acc: 0.9335\n",
      "Epoch 1356/1500\n",
      "Epoch 01355: val_loss did not improve\n",
      "2s - loss: 0.2096 - acc: 0.9218 - val_loss: 0.2941 - val_acc: 0.9275\n",
      "Epoch 1357/1500\n",
      "Epoch 01356: val_loss did not improve\n",
      "2s - loss: 0.2413 - acc: 0.9140 - val_loss: 0.2661 - val_acc: 0.9063\n",
      "Epoch 1358/1500\n",
      "Epoch 01357: val_loss did not improve\n",
      "2s - loss: 0.2338 - acc: 0.9218 - val_loss: 0.2447 - val_acc: 0.9245\n",
      "Epoch 1359/1500\n",
      "Epoch 01358: val_loss did not improve\n",
      "2s - loss: 0.2521 - acc: 0.9206 - val_loss: 0.2265 - val_acc: 0.9305\n",
      "Epoch 1360/1500\n",
      "Epoch 01359: val_loss did not improve\n",
      "2s - loss: 0.2146 - acc: 0.9182 - val_loss: 0.3235 - val_acc: 0.9094\n",
      "Epoch 1361/1500\n",
      "Epoch 01360: val_loss did not improve\n",
      "2s - loss: 0.2356 - acc: 0.9198 - val_loss: 0.2615 - val_acc: 0.9335\n",
      "Epoch 1362/1500\n",
      "Epoch 01361: val_loss did not improve\n",
      "2s - loss: 0.2190 - acc: 0.9232 - val_loss: 0.2364 - val_acc: 0.9607\n",
      "Epoch 1363/1500\n",
      "Epoch 01362: val_loss did not improve\n",
      "2s - loss: 0.2032 - acc: 0.9190 - val_loss: 0.2473 - val_acc: 0.9124\n",
      "Epoch 1364/1500\n",
      "Epoch 01363: val_loss did not improve\n",
      "2s - loss: 0.2049 - acc: 0.9257 - val_loss: 0.2609 - val_acc: 0.9335\n",
      "Epoch 1365/1500\n",
      "Epoch 01364: val_loss did not improve\n",
      "2s - loss: 0.2405 - acc: 0.9154 - val_loss: 0.3189 - val_acc: 0.9094\n",
      "Epoch 1366/1500\n",
      "Epoch 01365: val_loss did not improve\n",
      "2s - loss: 0.2027 - acc: 0.9248 - val_loss: 0.1718 - val_acc: 0.9426\n",
      "Epoch 1367/1500\n",
      "Epoch 01366: val_loss did not improve\n",
      "2s - loss: 0.2475 - acc: 0.9175 - val_loss: 0.2663 - val_acc: 0.9184\n",
      "Epoch 1368/1500\n",
      "Epoch 01367: val_loss did not improve\n",
      "2s - loss: 0.2296 - acc: 0.9216 - val_loss: 0.3024 - val_acc: 0.9124\n",
      "Epoch 1369/1500\n",
      "Epoch 01368: val_loss did not improve\n",
      "2s - loss: 0.2161 - acc: 0.9153 - val_loss: 0.2917 - val_acc: 0.9154\n",
      "Epoch 1370/1500\n",
      "Epoch 01369: val_loss did not improve\n",
      "2s - loss: 0.2103 - acc: 0.9214 - val_loss: 0.2496 - val_acc: 0.9305\n",
      "Epoch 1371/1500\n",
      "Epoch 01370: val_loss did not improve\n",
      "2s - loss: 0.2224 - acc: 0.9177 - val_loss: 0.2705 - val_acc: 0.9275\n",
      "Epoch 1372/1500\n",
      "Epoch 01371: val_loss did not improve\n",
      "2s - loss: 0.2186 - acc: 0.9209 - val_loss: 0.2169 - val_acc: 0.9366\n",
      "Epoch 1373/1500\n",
      "Epoch 01372: val_loss did not improve\n",
      "2s - loss: 0.2301 - acc: 0.9174 - val_loss: 0.3071 - val_acc: 0.9305\n",
      "Epoch 1374/1500\n",
      "Epoch 01373: val_loss did not improve\n",
      "2s - loss: 0.1962 - acc: 0.9297 - val_loss: 0.1777 - val_acc: 0.9517\n",
      "Epoch 1375/1500\n",
      "Epoch 01374: val_loss did not improve\n",
      "2s - loss: 0.2096 - acc: 0.9263 - val_loss: 0.2523 - val_acc: 0.9184\n",
      "Epoch 1376/1500\n",
      "Epoch 01375: val_loss did not improve\n",
      "2s - loss: 0.2151 - acc: 0.9185 - val_loss: 0.2968 - val_acc: 0.8973\n",
      "Epoch 1377/1500\n",
      "Epoch 01376: val_loss did not improve\n",
      "2s - loss: 0.2300 - acc: 0.9281 - val_loss: 0.2837 - val_acc: 0.9275\n",
      "Epoch 1378/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01377: val_loss did not improve\n",
      "2s - loss: 0.2148 - acc: 0.9256 - val_loss: 0.2562 - val_acc: 0.9486\n",
      "Epoch 1379/1500\n",
      "Epoch 01378: val_loss did not improve\n",
      "2s - loss: 0.2558 - acc: 0.9122 - val_loss: 0.2535 - val_acc: 0.9456\n",
      "Epoch 1380/1500\n",
      "Epoch 01379: val_loss did not improve\n",
      "2s - loss: 0.2257 - acc: 0.9127 - val_loss: 0.3204 - val_acc: 0.9184\n",
      "Epoch 1381/1500\n",
      "Epoch 01380: val_loss did not improve\n",
      "2s - loss: 0.2323 - acc: 0.9232 - val_loss: 0.3404 - val_acc: 0.9184\n",
      "Epoch 1382/1500\n",
      "Epoch 01381: val_loss did not improve\n",
      "2s - loss: 0.2296 - acc: 0.9256 - val_loss: 0.2365 - val_acc: 0.9335\n",
      "Epoch 1383/1500\n",
      "Epoch 01382: val_loss did not improve\n",
      "2s - loss: 0.2292 - acc: 0.9199 - val_loss: 0.2109 - val_acc: 0.9366\n",
      "Epoch 1384/1500\n",
      "Epoch 01383: val_loss did not improve\n",
      "2s - loss: 0.1846 - acc: 0.9344 - val_loss: 0.2764 - val_acc: 0.9275\n",
      "Epoch 1385/1500\n",
      "Epoch 01384: val_loss did not improve\n",
      "2s - loss: 0.2023 - acc: 0.9281 - val_loss: 0.2658 - val_acc: 0.9063\n",
      "Epoch 1386/1500\n",
      "Epoch 01385: val_loss did not improve\n",
      "2s - loss: 0.2039 - acc: 0.9338 - val_loss: 0.3929 - val_acc: 0.9124\n",
      "Epoch 1387/1500\n",
      "Epoch 01386: val_loss did not improve\n",
      "2s - loss: 0.2477 - acc: 0.9098 - val_loss: 0.2398 - val_acc: 0.9275\n",
      "Epoch 1388/1500\n",
      "Epoch 01387: val_loss did not improve\n",
      "2s - loss: 0.2262 - acc: 0.9169 - val_loss: 0.2780 - val_acc: 0.9063\n",
      "Epoch 1389/1500\n",
      "Epoch 01388: val_loss did not improve\n",
      "2s - loss: 0.2373 - acc: 0.9159 - val_loss: 0.2539 - val_acc: 0.9245\n",
      "Epoch 1390/1500\n",
      "Epoch 01389: val_loss did not improve\n",
      "2s - loss: 0.2265 - acc: 0.9216 - val_loss: 0.2922 - val_acc: 0.9184\n",
      "Epoch 1391/1500\n",
      "Epoch 01390: val_loss did not improve\n",
      "2s - loss: 0.2418 - acc: 0.9099 - val_loss: 0.2662 - val_acc: 0.9124\n",
      "Epoch 1392/1500\n",
      "Epoch 01391: val_loss did not improve\n",
      "2s - loss: 0.2212 - acc: 0.9320 - val_loss: 0.2902 - val_acc: 0.9305\n",
      "Epoch 1393/1500\n",
      "Epoch 01392: val_loss did not improve\n",
      "2s - loss: 0.2201 - acc: 0.9224 - val_loss: 0.3461 - val_acc: 0.9063\n",
      "Epoch 1394/1500\n",
      "Epoch 01393: val_loss did not improve\n",
      "2s - loss: 0.2145 - acc: 0.9206 - val_loss: 0.3073 - val_acc: 0.9184\n",
      "Epoch 1395/1500\n",
      "Epoch 01394: val_loss did not improve\n",
      "2s - loss: 0.2673 - acc: 0.9153 - val_loss: 0.3201 - val_acc: 0.9215\n",
      "Epoch 1396/1500\n",
      "Epoch 01395: val_loss did not improve\n",
      "2s - loss: 0.2419 - acc: 0.9191 - val_loss: 0.2767 - val_acc: 0.9033\n",
      "Epoch 1397/1500\n",
      "Epoch 01396: val_loss did not improve\n",
      "2s - loss: 0.2173 - acc: 0.9257 - val_loss: 0.3158 - val_acc: 0.9063\n",
      "Epoch 1398/1500\n",
      "Epoch 01397: val_loss did not improve\n",
      "2s - loss: 0.2192 - acc: 0.9234 - val_loss: 0.2839 - val_acc: 0.9063\n",
      "Epoch 1399/1500\n",
      "Epoch 01398: val_loss did not improve\n",
      "2s - loss: 0.2323 - acc: 0.9143 - val_loss: 0.2604 - val_acc: 0.9275\n",
      "Epoch 1400/1500\n",
      "Epoch 01399: val_loss did not improve\n",
      "2s - loss: 0.1946 - acc: 0.9351 - val_loss: 0.4201 - val_acc: 0.8943\n",
      "Epoch 1401/1500\n",
      "Epoch 01400: val_loss did not improve\n",
      "2s - loss: 0.2594 - acc: 0.9194 - val_loss: 0.3147 - val_acc: 0.9154\n",
      "Epoch 1402/1500\n",
      "Epoch 01401: val_loss did not improve\n",
      "2s - loss: 0.2272 - acc: 0.9073 - val_loss: 0.2828 - val_acc: 0.9305\n",
      "Epoch 1403/1500\n",
      "Epoch 01402: val_loss did not improve\n",
      "2s - loss: 0.2285 - acc: 0.9224 - val_loss: 0.2958 - val_acc: 0.9124\n",
      "Epoch 1404/1500\n",
      "Epoch 01403: val_loss did not improve\n",
      "2s - loss: 0.2284 - acc: 0.9266 - val_loss: 0.2557 - val_acc: 0.9335\n",
      "Epoch 1405/1500\n",
      "Epoch 01404: val_loss did not improve\n",
      "2s - loss: 0.2542 - acc: 0.9138 - val_loss: 0.2217 - val_acc: 0.9366\n",
      "Epoch 1406/1500\n",
      "Epoch 01405: val_loss did not improve\n",
      "2s - loss: 0.2533 - acc: 0.9057 - val_loss: 0.2845 - val_acc: 0.9033\n",
      "Epoch 1407/1500\n",
      "Epoch 01406: val_loss did not improve\n",
      "2s - loss: 0.2324 - acc: 0.9175 - val_loss: 0.2945 - val_acc: 0.9215\n",
      "Epoch 1408/1500\n",
      "Epoch 01407: val_loss did not improve\n",
      "2s - loss: 0.2358 - acc: 0.9159 - val_loss: 0.2978 - val_acc: 0.9154\n",
      "Epoch 1409/1500\n",
      "Epoch 01408: val_loss did not improve\n",
      "2s - loss: 0.2292 - acc: 0.9159 - val_loss: 0.2961 - val_acc: 0.9033\n",
      "Epoch 1410/1500\n",
      "Epoch 01409: val_loss did not improve\n",
      "2s - loss: 0.2348 - acc: 0.9166 - val_loss: 0.2853 - val_acc: 0.9245\n",
      "Epoch 1411/1500\n",
      "Epoch 01410: val_loss did not improve\n",
      "2s - loss: 0.2131 - acc: 0.9313 - val_loss: 0.2288 - val_acc: 0.9366\n",
      "Epoch 1412/1500\n",
      "Epoch 01411: val_loss did not improve\n",
      "2s - loss: 0.2221 - acc: 0.9198 - val_loss: 0.2978 - val_acc: 0.9124\n",
      "Epoch 1413/1500\n",
      "Epoch 01412: val_loss did not improve\n",
      "2s - loss: 0.2631 - acc: 0.9185 - val_loss: 0.3217 - val_acc: 0.9215\n",
      "Epoch 1414/1500\n",
      "Epoch 01413: val_loss improved from 0.16771 to 0.15867, saving model to piece_model.weights.best.hdf5\n",
      "2s - loss: 0.2516 - acc: 0.9170 - val_loss: 0.1587 - val_acc: 0.9486\n",
      "Epoch 1415/1500\n",
      "Epoch 01414: val_loss did not improve\n",
      "2s - loss: 0.2370 - acc: 0.9217 - val_loss: 0.2419 - val_acc: 0.9335\n",
      "Epoch 1416/1500\n",
      "Epoch 01415: val_loss did not improve\n",
      "2s - loss: 0.2170 - acc: 0.9175 - val_loss: 0.2233 - val_acc: 0.9396\n",
      "Epoch 1417/1500\n",
      "Epoch 01416: val_loss did not improve\n",
      "2s - loss: 0.2218 - acc: 0.9216 - val_loss: 0.2542 - val_acc: 0.9335\n",
      "Epoch 1418/1500\n",
      "Epoch 01417: val_loss did not improve\n",
      "2s - loss: 0.2404 - acc: 0.9198 - val_loss: 0.2795 - val_acc: 0.9275\n",
      "Epoch 1419/1500\n",
      "Epoch 01418: val_loss did not improve\n",
      "2s - loss: 0.2691 - acc: 0.9095 - val_loss: 0.2519 - val_acc: 0.9124\n",
      "Epoch 1420/1500\n",
      "Epoch 01419: val_loss did not improve\n",
      "2s - loss: 0.2417 - acc: 0.9056 - val_loss: 0.3176 - val_acc: 0.9124\n",
      "Epoch 1421/1500\n",
      "Epoch 01420: val_loss did not improve\n",
      "2s - loss: 0.2462 - acc: 0.9120 - val_loss: 0.2017 - val_acc: 0.9305\n",
      "Epoch 1422/1500\n",
      "Epoch 01421: val_loss did not improve\n",
      "2s - loss: 0.2169 - acc: 0.9201 - val_loss: 0.2793 - val_acc: 0.9184\n",
      "Epoch 1423/1500\n",
      "Epoch 01422: val_loss did not improve\n",
      "2s - loss: 0.1935 - acc: 0.9289 - val_loss: 0.3022 - val_acc: 0.9094\n",
      "Epoch 1424/1500\n",
      "Epoch 01423: val_loss did not improve\n",
      "2s - loss: 0.1998 - acc: 0.9222 - val_loss: 0.2595 - val_acc: 0.9305\n",
      "Epoch 1425/1500\n",
      "Epoch 01424: val_loss did not improve\n",
      "2s - loss: 0.2038 - acc: 0.9208 - val_loss: 0.2640 - val_acc: 0.9245\n",
      "Epoch 1426/1500\n",
      "Epoch 01425: val_loss did not improve\n",
      "2s - loss: 0.2091 - acc: 0.9208 - val_loss: 0.1826 - val_acc: 0.9366\n",
      "Epoch 1427/1500\n",
      "Epoch 01426: val_loss did not improve\n",
      "2s - loss: 0.2244 - acc: 0.9090 - val_loss: 0.2716 - val_acc: 0.9275\n",
      "Epoch 1428/1500\n",
      "Epoch 01427: val_loss did not improve\n",
      "2s - loss: 0.2206 - acc: 0.9297 - val_loss: 0.2359 - val_acc: 0.9335\n",
      "Epoch 1429/1500\n",
      "Epoch 01428: val_loss did not improve\n",
      "2s - loss: 0.2402 - acc: 0.9123 - val_loss: 0.3146 - val_acc: 0.9184\n",
      "Epoch 1430/1500\n",
      "Epoch 01429: val_loss did not improve\n",
      "2s - loss: 0.2391 - acc: 0.9139 - val_loss: 0.2438 - val_acc: 0.9305\n",
      "Epoch 1431/1500\n",
      "Epoch 01430: val_loss did not improve\n",
      "2s - loss: 0.1992 - acc: 0.9225 - val_loss: 0.2518 - val_acc: 0.9245\n",
      "Epoch 1432/1500\n",
      "Epoch 01431: val_loss did not improve\n",
      "2s - loss: 0.2111 - acc: 0.9216 - val_loss: 0.2401 - val_acc: 0.9124\n",
      "Epoch 1433/1500\n",
      "Epoch 01432: val_loss did not improve\n",
      "2s - loss: 0.1935 - acc: 0.9265 - val_loss: 0.3519 - val_acc: 0.9033\n",
      "Epoch 1434/1500\n",
      "Epoch 01433: val_loss did not improve\n",
      "2s - loss: 0.2043 - acc: 0.9288 - val_loss: 0.2327 - val_acc: 0.9335\n",
      "Epoch 1435/1500\n",
      "Epoch 01434: val_loss did not improve\n",
      "2s - loss: 0.2150 - acc: 0.9224 - val_loss: 0.3632 - val_acc: 0.8852\n",
      "Epoch 1436/1500\n",
      "Epoch 01435: val_loss did not improve\n",
      "2s - loss: 0.2455 - acc: 0.9158 - val_loss: 0.2648 - val_acc: 0.9245\n",
      "Epoch 1437/1500\n",
      "Epoch 01436: val_loss did not improve\n",
      "2s - loss: 0.2194 - acc: 0.9238 - val_loss: 0.2617 - val_acc: 0.9305\n",
      "Epoch 1438/1500\n",
      "Epoch 01437: val_loss did not improve\n",
      "2s - loss: 0.2192 - acc: 0.9230 - val_loss: 0.2736 - val_acc: 0.9426\n",
      "Epoch 1439/1500\n",
      "Epoch 01438: val_loss did not improve\n",
      "2s - loss: 0.2151 - acc: 0.9185 - val_loss: 0.3489 - val_acc: 0.8973\n",
      "Epoch 1440/1500\n",
      "Epoch 01439: val_loss did not improve\n",
      "2s - loss: 0.2113 - acc: 0.9214 - val_loss: 0.3143 - val_acc: 0.9215\n",
      "Epoch 1441/1500\n",
      "Epoch 01440: val_loss did not improve\n",
      "2s - loss: 0.1986 - acc: 0.9369 - val_loss: 0.3080 - val_acc: 0.9245\n",
      "Epoch 1442/1500\n",
      "Epoch 01441: val_loss did not improve\n",
      "2s - loss: 0.1929 - acc: 0.9320 - val_loss: 0.2733 - val_acc: 0.9154\n",
      "Epoch 1443/1500\n",
      "Epoch 01442: val_loss did not improve\n",
      "2s - loss: 0.1911 - acc: 0.9344 - val_loss: 0.2239 - val_acc: 0.9275\n",
      "Epoch 1444/1500\n",
      "Epoch 01443: val_loss did not improve\n",
      "2s - loss: 0.2008 - acc: 0.9242 - val_loss: 0.2332 - val_acc: 0.9366\n",
      "Epoch 1445/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01444: val_loss did not improve\n",
      "2s - loss: 0.2255 - acc: 0.9120 - val_loss: 0.2626 - val_acc: 0.9335\n",
      "Epoch 1446/1500\n",
      "Epoch 01445: val_loss did not improve\n",
      "2s - loss: 0.1745 - acc: 0.9455 - val_loss: 0.1787 - val_acc: 0.9456\n",
      "Epoch 1447/1500\n",
      "Epoch 01446: val_loss did not improve\n",
      "2s - loss: 0.2481 - acc: 0.9217 - val_loss: 0.2576 - val_acc: 0.9366\n",
      "Epoch 1448/1500\n",
      "Epoch 01447: val_loss did not improve\n",
      "2s - loss: 0.2262 - acc: 0.9266 - val_loss: 0.2179 - val_acc: 0.9335\n",
      "Epoch 1449/1500\n",
      "Epoch 01448: val_loss did not improve\n",
      "2s - loss: 0.2150 - acc: 0.9280 - val_loss: 0.2892 - val_acc: 0.9245\n",
      "Epoch 1450/1500\n",
      "Epoch 01449: val_loss did not improve\n",
      "2s - loss: 0.1711 - acc: 0.9359 - val_loss: 0.2946 - val_acc: 0.9335\n",
      "Epoch 1451/1500\n",
      "Epoch 01450: val_loss did not improve\n",
      "2s - loss: 0.1808 - acc: 0.9353 - val_loss: 0.1949 - val_acc: 0.9426\n",
      "Epoch 1452/1500\n",
      "Epoch 01451: val_loss did not improve\n",
      "2s - loss: 0.2114 - acc: 0.9272 - val_loss: 0.2557 - val_acc: 0.9154\n",
      "Epoch 1453/1500\n",
      "Epoch 01452: val_loss did not improve\n",
      "2s - loss: 0.1991 - acc: 0.9305 - val_loss: 0.2530 - val_acc: 0.9305\n",
      "Epoch 1454/1500\n",
      "Epoch 01453: val_loss did not improve\n",
      "2s - loss: 0.1884 - acc: 0.9280 - val_loss: 0.3235 - val_acc: 0.9184\n",
      "Epoch 1455/1500\n",
      "Epoch 01454: val_loss did not improve\n",
      "2s - loss: 0.1823 - acc: 0.9303 - val_loss: 0.2549 - val_acc: 0.9305\n",
      "Epoch 1456/1500\n",
      "Epoch 01455: val_loss did not improve\n",
      "2s - loss: 0.2312 - acc: 0.9127 - val_loss: 0.2482 - val_acc: 0.9275\n",
      "Epoch 1457/1500\n",
      "Epoch 01456: val_loss did not improve\n",
      "2s - loss: 0.1989 - acc: 0.9272 - val_loss: 0.2429 - val_acc: 0.9305\n",
      "Epoch 1458/1500\n",
      "Epoch 01457: val_loss did not improve\n",
      "2s - loss: 0.2068 - acc: 0.9337 - val_loss: 0.2805 - val_acc: 0.9305\n",
      "Epoch 1459/1500\n",
      "Epoch 01458: val_loss did not improve\n",
      "2s - loss: 0.2046 - acc: 0.9280 - val_loss: 0.2361 - val_acc: 0.9335\n",
      "Epoch 1460/1500\n",
      "Epoch 01459: val_loss did not improve\n",
      "2s - loss: 0.2606 - acc: 0.9290 - val_loss: 0.3089 - val_acc: 0.9184\n",
      "Epoch 1461/1500\n",
      "Epoch 01460: val_loss did not improve\n",
      "2s - loss: 0.2053 - acc: 0.9249 - val_loss: 0.2501 - val_acc: 0.9275\n",
      "Epoch 1462/1500\n",
      "Epoch 01461: val_loss did not improve\n",
      "2s - loss: 0.1934 - acc: 0.9353 - val_loss: 0.2100 - val_acc: 0.9366\n",
      "Epoch 1463/1500\n",
      "Epoch 01462: val_loss did not improve\n",
      "2s - loss: 0.2149 - acc: 0.9224 - val_loss: 0.3044 - val_acc: 0.9184\n",
      "Epoch 1464/1500\n",
      "Epoch 01463: val_loss did not improve\n",
      "2s - loss: 0.2101 - acc: 0.9222 - val_loss: 0.2817 - val_acc: 0.9215\n",
      "Epoch 1465/1500\n",
      "Epoch 01464: val_loss did not improve\n",
      "2s - loss: 0.2054 - acc: 0.9303 - val_loss: 0.2563 - val_acc: 0.9215\n",
      "Epoch 1466/1500\n",
      "Epoch 01465: val_loss did not improve\n",
      "2s - loss: 0.1999 - acc: 0.9267 - val_loss: 0.2733 - val_acc: 0.9124\n",
      "Epoch 1467/1500\n",
      "Epoch 01466: val_loss did not improve\n",
      "2s - loss: 0.2176 - acc: 0.9177 - val_loss: 0.2862 - val_acc: 0.9245\n",
      "Epoch 1468/1500\n",
      "Epoch 01467: val_loss did not improve\n",
      "2s - loss: 0.2061 - acc: 0.9280 - val_loss: 0.2182 - val_acc: 0.9456\n",
      "Epoch 1469/1500\n",
      "Epoch 01468: val_loss did not improve\n",
      "2s - loss: 0.2203 - acc: 0.9216 - val_loss: 0.2295 - val_acc: 0.9366\n",
      "Epoch 1470/1500\n",
      "Epoch 01469: val_loss did not improve\n",
      "2s - loss: 0.2099 - acc: 0.9280 - val_loss: 0.2310 - val_acc: 0.9305\n",
      "Epoch 1471/1500\n",
      "Epoch 01470: val_loss did not improve\n",
      "2s - loss: 0.2557 - acc: 0.9117 - val_loss: 0.3629 - val_acc: 0.9063\n",
      "Epoch 1472/1500\n",
      "Epoch 01471: val_loss did not improve\n",
      "2s - loss: 0.1971 - acc: 0.9256 - val_loss: 0.2746 - val_acc: 0.9215\n",
      "Epoch 1473/1500\n",
      "Epoch 01472: val_loss did not improve\n",
      "2s - loss: 0.1837 - acc: 0.9289 - val_loss: 0.2401 - val_acc: 0.9215\n",
      "Epoch 1474/1500\n",
      "Epoch 01473: val_loss did not improve\n",
      "2s - loss: 0.2153 - acc: 0.9183 - val_loss: 0.3130 - val_acc: 0.9184\n",
      "Epoch 1475/1500\n",
      "Epoch 01474: val_loss did not improve\n",
      "2s - loss: 0.2364 - acc: 0.9143 - val_loss: 0.2609 - val_acc: 0.9335\n",
      "Epoch 1476/1500\n",
      "Epoch 01475: val_loss did not improve\n",
      "2s - loss: 0.1917 - acc: 0.9281 - val_loss: 0.3293 - val_acc: 0.9124\n",
      "Epoch 1477/1500\n",
      "Epoch 01476: val_loss did not improve\n",
      "2s - loss: 0.2103 - acc: 0.9257 - val_loss: 0.2397 - val_acc: 0.9275\n",
      "Epoch 1478/1500\n",
      "Epoch 01477: val_loss did not improve\n",
      "2s - loss: 0.2303 - acc: 0.9183 - val_loss: 0.2480 - val_acc: 0.9245\n",
      "Epoch 1479/1500\n",
      "Epoch 01478: val_loss did not improve\n",
      "2s - loss: 0.2073 - acc: 0.9232 - val_loss: 0.2840 - val_acc: 0.9245\n",
      "Epoch 1480/1500\n",
      "Epoch 01479: val_loss did not improve\n",
      "2s - loss: 0.1843 - acc: 0.9271 - val_loss: 0.3592 - val_acc: 0.9154\n",
      "Epoch 1481/1500\n",
      "Epoch 01480: val_loss did not improve\n",
      "2s - loss: 0.2261 - acc: 0.9088 - val_loss: 0.2586 - val_acc: 0.9124\n",
      "Epoch 1482/1500\n",
      "Epoch 01481: val_loss did not improve\n",
      "2s - loss: 0.2097 - acc: 0.9191 - val_loss: 0.2033 - val_acc: 0.9366\n",
      "Epoch 1483/1500\n",
      "Epoch 01482: val_loss did not improve\n",
      "2s - loss: 0.1978 - acc: 0.9272 - val_loss: 0.3744 - val_acc: 0.9003\n",
      "Epoch 1484/1500\n",
      "Epoch 01483: val_loss did not improve\n",
      "2s - loss: 0.2450 - acc: 0.9241 - val_loss: 0.3238 - val_acc: 0.9215\n",
      "Epoch 1485/1500\n",
      "Epoch 01484: val_loss did not improve\n",
      "2s - loss: 0.1952 - acc: 0.9295 - val_loss: 0.2122 - val_acc: 0.9426\n",
      "Epoch 1486/1500\n",
      "Epoch 01485: val_loss did not improve\n",
      "2s - loss: 0.2362 - acc: 0.9191 - val_loss: 0.2237 - val_acc: 0.9426\n",
      "Epoch 1487/1500\n",
      "Epoch 01486: val_loss did not improve\n",
      "2s - loss: 0.1832 - acc: 0.9250 - val_loss: 0.2408 - val_acc: 0.9305\n",
      "Epoch 1488/1500\n",
      "Epoch 01487: val_loss did not improve\n",
      "2s - loss: 0.2223 - acc: 0.9154 - val_loss: 0.1937 - val_acc: 0.9426\n",
      "Epoch 1489/1500\n",
      "Epoch 01488: val_loss did not improve\n",
      "2s - loss: 0.2008 - acc: 0.9240 - val_loss: 0.2650 - val_acc: 0.9063\n",
      "Epoch 1490/1500\n",
      "Epoch 01489: val_loss did not improve\n",
      "2s - loss: 0.1909 - acc: 0.9266 - val_loss: 0.2114 - val_acc: 0.9456\n",
      "Epoch 1491/1500\n",
      "Epoch 01490: val_loss did not improve\n",
      "2s - loss: 0.2180 - acc: 0.9241 - val_loss: 0.2589 - val_acc: 0.9094\n",
      "Epoch 1492/1500\n",
      "Epoch 01491: val_loss did not improve\n",
      "2s - loss: 0.2162 - acc: 0.9201 - val_loss: 0.3357 - val_acc: 0.9305\n",
      "Epoch 1493/1500\n",
      "Epoch 01492: val_loss did not improve\n",
      "2s - loss: 0.1859 - acc: 0.9375 - val_loss: 0.2366 - val_acc: 0.9366\n",
      "Epoch 1494/1500\n",
      "Epoch 01493: val_loss did not improve\n",
      "2s - loss: 0.2278 - acc: 0.9272 - val_loss: 0.2704 - val_acc: 0.9426\n",
      "Epoch 1495/1500\n",
      "Epoch 01494: val_loss did not improve\n",
      "2s - loss: 0.1916 - acc: 0.9314 - val_loss: 0.3558 - val_acc: 0.9033\n",
      "Epoch 1496/1500\n",
      "Epoch 01495: val_loss did not improve\n",
      "2s - loss: 0.1916 - acc: 0.9282 - val_loss: 0.2936 - val_acc: 0.9245\n",
      "Epoch 1497/1500\n",
      "Epoch 01496: val_loss did not improve\n",
      "2s - loss: 0.1921 - acc: 0.9280 - val_loss: 0.2620 - val_acc: 0.9154\n",
      "Epoch 1498/1500\n",
      "Epoch 01497: val_loss did not improve\n",
      "2s - loss: 0.2127 - acc: 0.9171 - val_loss: 0.2625 - val_acc: 0.9275\n",
      "Epoch 1499/1500\n",
      "Epoch 01498: val_loss did not improve\n",
      "2s - loss: 0.2187 - acc: 0.9198 - val_loss: 0.2758 - val_acc: 0.9154\n",
      "Epoch 1500/1500\n",
      "Epoch 01499: val_loss did not improve\n",
      "2s - loss: 0.1852 - acc: 0.9321 - val_loss: 0.2526 - val_acc: 0.9335\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='piece_model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "piece_hist = piece_model.fit_generator(generator=piece_train_iter, \n",
    "                          steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                          validation_data=piece_valid_iter, \n",
    "                          validation_steps=STEP_SIZE_VALID,\n",
    "                          epochs=1500, \n",
    "                          callbacks=[checkpointer], \n",
    "                          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.910447761194\n"
     ]
    }
   ],
   "source": [
    "#Reset test iterator\n",
    "STEP_SIZE_TEST = piece_test_iter.n/piece_test_iter.batch_size\n",
    "piece_test_iter.reset()\n",
    "# load the weights that yielded the best validation accuracy\n",
    "piece_model.load_weights('piece_model.weights.best.hdf5')\n",
    "# evaluate and print test accuracy\n",
    "score = piece_model.evaluate_generator(generator=piece_test_iter,steps=STEP_SIZE_TEST)\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "piece_test_iter.reset()\n",
    "piece_pred = piece_model.predict_generator(piece_test_iter,steps=STEP_SIZE_TEST,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 3 0 0 1 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
      " 0 0 0 0 0 0 0 3 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 4 4 1 1 4 1 1 1 1 4 1 1\n",
      " 1 1 1 1 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 3 2 2 2 2 2 2 2 0 2 2 2\n",
      " 2 2 2 2 2 2 2 2 0 2 2 1 2 2 2 2 1 4 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3\n",
      " 3 3 3 0 3 3 3 4 0 4 4 4 1 4 4 1 4 4 4 4 4 4 4 4 4 4 0 4 4 5 5 4 4 1 4 4 4\n",
      " 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6]\n"
     ]
    }
   ],
   "source": [
    "predicted_class_indices=np.argmax(piece_pred,axis=1)\n",
    "print(predicted_class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (piece_test_iter.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "truth =  [labels[k] for k in piece_test_iter.classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'bishop',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'king',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'knight',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'pawn',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'rook',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square',\n",
       " 'square']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[labels[k] for k in piece_test_iter.classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bishop/1538778538.3017461.jpg',\n",
       " 'bishop/1539025832.837544.jpg',\n",
       " 'bishop/1539025486.6533895.jpg',\n",
       " 'bishop/1538779519.5865016.jpg',\n",
       " 'bishop/1538778643.4925923.jpg',\n",
       " 'bishop/1539023363.830272.jpg',\n",
       " 'bishop/1539025801.510389.jpg',\n",
       " 'bishop/1538778410.6081142.jpg',\n",
       " 'bishop/1538778215.8878355.jpg',\n",
       " 'bishop/1538778752.467536.jpg',\n",
       " 'bishop/1539025373.0113628.jpg',\n",
       " 'bishop/1539023140.397658.jpg',\n",
       " 'bishop/1539025334.2429533.jpg',\n",
       " 'bishop/1538778214.3746974.jpg',\n",
       " 'bishop/1539023732.561326.jpg',\n",
       " 'bishop/1538777556.4695792.jpg',\n",
       " 'bishop/1539024760.3120384.jpg',\n",
       " 'bishop/1538778091.9695802.jpg',\n",
       " 'bishop/1539024593.521976.jpg',\n",
       " 'bishop/1538779155.2682924.jpg',\n",
       " 'bishop/1538778833.610855.jpg',\n",
       " 'bishop/1538777891.4931645.jpg',\n",
       " 'bishop/1539017656.6888525_5.jpg',\n",
       " 'bishop/1538778427.8668559.jpg',\n",
       " 'bishop/1538777675.2653856.jpg',\n",
       " 'bishop/1538778093.2397656.jpg',\n",
       " 'bishop/1539024725.794323.jpg',\n",
       " 'bishop/1539026035.275597.jpg',\n",
       " 'bishop/1539024424.9014952.jpg',\n",
       " 'bishop/1539024039.3749897.jpg',\n",
       " 'bishop/1538778832.1937056.jpg',\n",
       " 'bishop/1538778729.1405196.jpg',\n",
       " 'bishop/1539025434.58347.jpg',\n",
       " 'bishop/1538778878.3252861.jpg',\n",
       " 'bishop/1539025959.458598.jpg',\n",
       " 'bishop/1539025736.4874806.jpg',\n",
       " 'bishop/1539025415.635129.jpg',\n",
       " 'bishop/1538779144.9958382.jpg',\n",
       " 'bishop/1539017656.2202125_22.jpg',\n",
       " 'bishop/1538779417.7487714.jpg',\n",
       " 'bishop/1538777310.1624303.jpg',\n",
       " 'bishop/1538778958.324379.jpg',\n",
       " 'bishop/1538778656.6147914.jpg',\n",
       " 'bishop/1539025355.3753638.jpg',\n",
       " 'bishop/1538778537.1458335.jpg',\n",
       " 'bishop/1538778653.1963263.jpg',\n",
       " 'bishop/1539025474.8601475.jpg',\n",
       " 'king/1538776272.1332486.jpg',\n",
       " 'king/1539017657.8720484_58.jpg',\n",
       " 'king/1539017656.0015128_29.jpg',\n",
       " 'king/1538776522.496111.jpg',\n",
       " 'king/1538778011.0228102.jpg',\n",
       " 'king/1539023185.6079886.jpg',\n",
       " 'king/1539023823.1830983.jpg',\n",
       " 'king/1538778406.86247.jpg',\n",
       " 'king/1538775504.1042295.jpg',\n",
       " 'king/1539024597.23388.jpg',\n",
       " 'king/1539027947.6755114.jpg',\n",
       " 'king/1538779987.557536.jpg',\n",
       " 'king/1538775155.1271548.jpg',\n",
       " 'king/1538779354.1704876.jpg',\n",
       " 'king/1538774883.9670901.jpg',\n",
       " 'king/1539017656.5013957_31.jpg',\n",
       " 'king/1539022987.3141916.jpg',\n",
       " 'king/1538777095.0775242.jpg',\n",
       " 'king/1539017656.4857748_24.jpg',\n",
       " 'king/1539024744.5240357.jpg',\n",
       " 'king/1539026833.4767778.jpg',\n",
       " 'king/1538779020.8452945.jpg',\n",
       " 'king/1539017657.7939315_15.jpg',\n",
       " 'king/1538777707.301926.jpg',\n",
       " 'king/1538777418.4191957.jpg',\n",
       " 'king/1539023430.39102.jpg',\n",
       " 'king/1538779455.0068257.jpg',\n",
       " 'king/1539025742.555007.jpg',\n",
       " 'king/1538778972.6889744.jpg',\n",
       " 'king/1539023157.5381975.jpg',\n",
       " 'king/1538779136.6977031.jpg',\n",
       " 'king/1539017658.0594935_23.jpg',\n",
       " 'king/1538780110.995072.jpg',\n",
       " 'knight/1538779739.058273.jpg',\n",
       " 'knight/1539017658.809318_29.jpg',\n",
       " 'knight/1538778511.7367067.jpg',\n",
       " 'knight/1539017657.8251727_33.jpg',\n",
       " 'knight/1539028085.6649253.jpg',\n",
       " 'knight/1538778670.1169665.jpg',\n",
       " 'knight/1539026872.2657654.jpg',\n",
       " 'knight/1538779127.0457888.jpg',\n",
       " 'knight/1539017658.0438766_13.jpg',\n",
       " 'knight/1539017657.8407946_39.jpg',\n",
       " 'knight/1538779439.9036374.jpg',\n",
       " 'knight/1538779919.6389072.jpg',\n",
       " 'knight/1538778478.4419494.jpg',\n",
       " 'knight/1539017657.8251727_32.jpg',\n",
       " 'knight/1539023209.5838358.jpg',\n",
       " 'knight/1538778798.7536736.jpg',\n",
       " 'knight/1539023028.0835974.jpg',\n",
       " 'knight/1538778117.6826968.jpg',\n",
       " 'knight/1539017658.3406758_39.jpg',\n",
       " 'knight/1538779362.8107798.jpg',\n",
       " 'knight/1539017657.0682952_45.jpg',\n",
       " 'knight/1538779459.4036417.jpg',\n",
       " 'knight/1539017656.2045898_14.jpg',\n",
       " 'knight/1539028221.7775393.jpg',\n",
       " 'knight/1539017657.2756457_18.jpg',\n",
       " 'knight/1539023214.4817233.jpg',\n",
       " 'knight/1538775492.0805464.jpg',\n",
       " 'knight/1538779329.0645812.jpg',\n",
       " 'knight/1538778725.7980158.jpg',\n",
       " 'knight/1539017657.291264_26.jpg',\n",
       " 'knight/1539017657.2756457_13.jpg',\n",
       " 'knight/1538778212.992307.jpg',\n",
       " 'knight/1538777213.003978.jpg',\n",
       " 'knight/1539017656.7825797_62.jpg',\n",
       " 'knight/1539023817.175047.jpg',\n",
       " 'knight/1539017658.2938118_13.jpg',\n",
       " 'knight/1539017657.0643072_43.jpg',\n",
       " 'knight/1539023081.4048553.jpg',\n",
       " 'knight/1538778197.602519.jpg',\n",
       " 'knight/1539017656.048383_53.jpg',\n",
       " 'knight/1538777112.8554816.jpg',\n",
       " 'knight/1539023160.9380662.jpg',\n",
       " 'knight/1538779559.3820636.jpg',\n",
       " 'knight/1539017658.090735_39.jpg',\n",
       " 'knight/1538778628.9804418.jpg',\n",
       " 'knight/1538776219.5658782.jpg',\n",
       " 'knight/1539017657.5752366_26.jpg',\n",
       " 'knight/1538778356.0347438.jpg',\n",
       " 'knight/1539017656.6732304_1.jpg',\n",
       " 'knight/1539017658.3562984_41.jpg',\n",
       " 'knight/1539023190.5311208.jpg',\n",
       " 'knight/1539017657.5752366_28.jpg',\n",
       " 'knight/1538779701.3311424.jpg',\n",
       " 'knight/1539027337.492997.jpg',\n",
       " 'knight/1539017656.532639_55.jpg',\n",
       " 'knight/1539017658.5749974_28.jpg',\n",
       " 'knight/1538779157.0249271.jpg',\n",
       " 'knight/1539017658.3094337_22.jpg',\n",
       " 'knight/1539017657.7939315_13.jpg',\n",
       " 'knight/1539017657.0012786_23.jpg',\n",
       " 'knight/1539017658.5749974_27.jpg',\n",
       " 'pawn/1538779186.269667.jpg',\n",
       " 'pawn/1539025921.7744703.jpg',\n",
       " 'pawn/1539017659.0342584_5.jpg',\n",
       " 'pawn/1539023428.3806713.jpg',\n",
       " 'pawn/1539023228.1371589.jpg',\n",
       " 'pawn/1539024583.6953154.jpg',\n",
       " 'pawn/1539022975.1625128.jpg',\n",
       " 'pawn/1538776446.2195349.jpg',\n",
       " 'pawn/1539017659.9293435_51.jpg',\n",
       " 'pawn/1539025391.7579608.jpg',\n",
       " 'pawn/1539023017.1607347.jpg',\n",
       " 'pawn/1539024430.756946.jpg',\n",
       " 'pawn/1539026923.465744.jpg',\n",
       " 'pawn/1538778996.169403.jpg',\n",
       " 'pawn/1539017659.0342584_2.jpg',\n",
       " 'pawn/1539025607.6992056.jpg',\n",
       " 'pawn/1539017660.3198786_9.jpg',\n",
       " 'pawn/1539025757.220354.jpg',\n",
       " 'pawn/1539023766.4123085.jpg',\n",
       " 'pawn/1539023368.838083.jpg',\n",
       " 'pawn/1538779001.1267478.jpg',\n",
       " 'pawn/1539027155.227193.jpg',\n",
       " 'pawn/1539025313.0497591.jpg',\n",
       " 'pawn/1538778999.7679274.jpg',\n",
       " 'pawn/1538779184.9161654.jpg',\n",
       " 'pawn/1539017661.8009472_9.jpg',\n",
       " 'pawn/1539017659.273254_5.jpg',\n",
       " 'pawn/1539017659.320111_30.jpg',\n",
       " 'pawn/1539017660.5854392_17.jpg',\n",
       " 'pawn/1538779143.7233405.jpg',\n",
       " 'pawn/1539017660.148043_49.jpg',\n",
       " 'pawn/1539024541.9791014.jpg',\n",
       " 'pawn/1539017660.3511202_27.jpg',\n",
       " 'pawn/1539017661.6291142_49.jpg',\n",
       " 'pawn/1539017664.4256973_53.jpg',\n",
       " 'pawn/1539026875.391828.jpg',\n",
       " 'pawn/1539025362.0886774.jpg',\n",
       " 'pawn/1539017661.8321903_20.jpg',\n",
       " 'pawn/1539017660.804138_1.jpg',\n",
       " 'pawn/1538779112.965819.jpg',\n",
       " 'pawn/1539026980.8596466.jpg',\n",
       " 'pawn/1539026837.0018582.jpg',\n",
       " 'pawn/1538778988.6561909.jpg',\n",
       " 'pawn/1539023466.5992458.jpg',\n",
       " 'pawn/1539017662.058607_12.jpg',\n",
       " 'pawn/1539017660.9134872_58.jpg',\n",
       " 'pawn/1539017664.3632076_13.jpg',\n",
       " 'pawn/1539024357.4180336.jpg',\n",
       " 'pawn/1539025673.4654312.jpg',\n",
       " 'pawn/1539017662.0429847_1.jpg',\n",
       " 'pawn/1539026038.6849546.jpg',\n",
       " 'queen/1538776153.1161563.jpg',\n",
       " 'queen/1539017658.0751133_28.jpg',\n",
       " 'queen/1539023282.7923298.jpg',\n",
       " 'queen/1538774886.4350865.jpg',\n",
       " 'queen/1539026802.3034804.jpg',\n",
       " 'queen/1538779104.4953988.jpg',\n",
       " 'queen/1538776241.9984956.jpg',\n",
       " 'queen/1538777951.5345926.jpg',\n",
       " 'queen/1538775387.0960882.jpg',\n",
       " 'queen/1539017657.5596101_20.jpg',\n",
       " 'queen/1539027143.1900225.jpg',\n",
       " 'queen/1539017656.0171356_37.jpg',\n",
       " 'queen/1539023360.027172.jpg',\n",
       " 'queen/1538775023.0136087.jpg',\n",
       " 'queen/1539027242.9633646.jpg',\n",
       " 'queen/1539024330.4476821.jpg',\n",
       " 'queen/1538776666.2012632.jpg',\n",
       " 'queen/1538778645.0375922.jpg',\n",
       " 'queen/1539017658.809318_21.jpg',\n",
       " 'queen/1539025459.6575894.jpg',\n",
       " 'queen/1539025544.3600874.jpg',\n",
       " 'queen/1538778211.4124253.jpg',\n",
       " 'queen/1539017656.9544163_8.jpg',\n",
       " 'queen/1539028286.9873247.jpg',\n",
       " 'queen/1538777276.2401419.jpg',\n",
       " 'queen/1538774903.7820106.jpg',\n",
       " 'queen/1538775521.7555773.jpg',\n",
       " 'queen/1538777233.7800925.jpg',\n",
       " 'queen/1538779089.730211.jpg',\n",
       " 'queen/1539024135.86198.jpg',\n",
       " 'queen/1538779711.7096522.jpg',\n",
       " 'queen/1539017656.0171356_34.jpg',\n",
       " 'rook/1538779431.4099524.jpg',\n",
       " 'rook/1538779446.819697.jpg',\n",
       " 'rook/1539023040.9695048.jpg',\n",
       " 'rook/1539017656.7825797_63.jpg',\n",
       " 'rook/1539017656.1889713_1.jpg',\n",
       " 'rook/1538778901.7553732.jpg',\n",
       " 'rook/1538778196.1506379.jpg',\n",
       " 'rook/1538778508.6299925.jpg',\n",
       " 'rook/1539017657.1038074_61.jpg',\n",
       " 'rook/1538778101.779042.jpg',\n",
       " 'rook/1538780044.5343263.jpg',\n",
       " 'rook/1538778949.280067.jpg',\n",
       " 'rook/1539027229.5479693.jpg',\n",
       " 'rook/1539025545.9052472.jpg',\n",
       " 'rook/1539026987.1509364.jpg',\n",
       " 'rook/1539017656.7825797_56.jpg',\n",
       " 'rook/1539025399.1357424.jpg',\n",
       " 'rook/1538779366.148124.jpg',\n",
       " 'rook/1539017656.5482595_56.jpg',\n",
       " 'rook/1539017658.2781916_2.jpg',\n",
       " 'rook/1538779524.2512803.jpg',\n",
       " 'rook/1538778590.4654863.jpg',\n",
       " 'rook/1538779051.3667924.jpg',\n",
       " 'rook/1539017658.1376002_63.jpg',\n",
       " 'rook/1539017657.6064732_44.jpg',\n",
       " 'rook/1538779142.3172784.jpg',\n",
       " 'rook/1539017658.121979_59.jpg',\n",
       " 'rook/1538780084.217775.jpg',\n",
       " 'rook/1539025595.0919125.jpg',\n",
       " 'rook/1538778830.4086065.jpg',\n",
       " 'rook/1538776436.5497537.jpg',\n",
       " 'rook/1539017656.6732304_0.jpg',\n",
       " 'rook/1538778420.713808.jpg',\n",
       " 'rook/1538779718.4556568.jpg',\n",
       " 'rook/1539017658.5906172_32.jpg',\n",
       " 'rook/1538779147.5203454.jpg',\n",
       " 'rook/1538778808.3629324.jpg',\n",
       " 'rook/1539025256.5206127.jpg',\n",
       " 'rook/1538778848.9860363.jpg',\n",
       " 'rook/1539025785.8488226.jpg',\n",
       " 'rook/1538779688.6545618.jpg',\n",
       " 'rook/1538779355.8192723.jpg',\n",
       " 'rook/1539017657.8876572_60.jpg',\n",
       " 'rook/1539025702.8716216.jpg',\n",
       " 'rook/1538780007.7006266.jpg',\n",
       " 'rook/1539017658.387541_63.jpg',\n",
       " 'rook/1539017656.2983181_57.jpg',\n",
       " 'rook/1538778246.5786252.jpg',\n",
       " 'rook/1538778565.9025004.jpg',\n",
       " 'rook/1539026043.774472.jpg',\n",
       " 'rook/1538779183.5230417.jpg',\n",
       " 'rook/1539017657.3537493_60.jpg',\n",
       " 'rook/1539017656.4545326_0.jpg',\n",
       " 'rook/1539017657.6220949_52.jpg',\n",
       " 'rook/1539026846.958911.jpg',\n",
       " 'rook/1538779421.4983451.jpg',\n",
       " 'rook/1539027241.4846253.jpg',\n",
       " 'rook/1539017657.3537493_58.jpg',\n",
       " 'rook/1539017657.0325217_32.jpg',\n",
       " 'rook/1539017657.8720484_52.jpg',\n",
       " 'rook/1538780193.7644806.jpg',\n",
       " 'square/1538775507.261289.jpg',\n",
       " 'square/1538775508.9165573.jpg',\n",
       " 'square/1538775478.8303273.jpg',\n",
       " 'square/1538775464.8436766.jpg',\n",
       " 'square/1538775393.9093816.jpg',\n",
       " 'square/1538775477.4368722.jpg',\n",
       " 'square/1538775463.3285487.jpg',\n",
       " 'square/1538775534.8773243.jpg',\n",
       " 'square/1538775490.1613836.jpg',\n",
       " 'square/1538775411.657127.jpg',\n",
       " 'square/1538775556.1137612.jpg',\n",
       " 'square/1538775392.500712.jpg',\n",
       " 'square/1538775395.379885.jpg',\n",
       " 'square/1538775380.3348455.jpg',\n",
       " 'square/1538775554.7329156.jpg',\n",
       " 'square/1538775429.9237733.jpg',\n",
       " 'square/1538775557.4647563.jpg',\n",
       " 'square/1538775397.2511947.jpg',\n",
       " 'square/1538775414.3379526.jpg',\n",
       " 'square/1538775525.8732312.jpg',\n",
       " 'square/1538775589.5684185.jpg',\n",
       " 'square/1538775377.837528.jpg',\n",
       " 'square/1538775473.8751059.jpg',\n",
       " 'square/1538775541.8031077.jpg',\n",
       " 'square/1538775563.8003812.jpg',\n",
       " 'square/1538775413.002367.jpg',\n",
       " 'square/1538775546.3549168.jpg',\n",
       " 'square/1538775428.0259957.jpg',\n",
       " 'square/1538775496.149376.jpg',\n",
       " 'square/1538775553.2582464.jpg',\n",
       " 'square/1538775431.6329722.jpg',\n",
       " 'square/1538775518.2826645.jpg',\n",
       " 'square/1538775528.933519.jpg',\n",
       " 'square/1538775501.9954052.jpg',\n",
       " 'square/1538775593.2157.jpg',\n",
       " 'square/1538775575.8332133.jpg',\n",
       " 'square/1538775381.8908746.jpg',\n",
       " 'square/1538775482.290001.jpg',\n",
       " 'square/1538775527.3804672.jpg',\n",
       " 'square/1538775383.3700583.jpg',\n",
       " 'square/1538775497.691954.jpg',\n",
       " 'square/1538775433.0617445.jpg',\n",
       " 'square/1538775459.9201775.jpg',\n",
       " 'square/1538775578.4874399.jpg',\n",
       " 'square/1538775356.6463027.jpg',\n",
       " 'square/1538775585.983797.jpg',\n",
       " 'square/1538775472.3628545.jpg',\n",
       " 'square/1538775359.6263092.jpg',\n",
       " 'square/1538775583.9372883.jpg',\n",
       " 'square/1538775410.2587154.jpg']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piece_test_iter.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=piece_test_iter.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Truth\": truth,\n",
    "                      \"Predictions\":predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bishop/1538778538.3017461.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bishop/1539025832.837544.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bishop/1539025486.6533895.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bishop/1538779519.5865016.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bishop/1538778643.4925923.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bishop/1539023363.830272.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bishop/1539025801.510389.jpg</td>\n",
       "      <td>pawn</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bishop/1538778410.6081142.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bishop/1538778215.8878355.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bishop/1538778752.467536.jpg</td>\n",
       "      <td>king</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bishop/1539025373.0113628.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bishop/1539023140.397658.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bishop/1539025334.2429533.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bishop/1538778214.3746974.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bishop/1539023732.561326.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bishop/1538777556.4695792.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bishop/1539024760.3120384.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bishop/1538778091.9695802.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bishop/1539024593.521976.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bishop/1538779155.2682924.jpg</td>\n",
       "      <td>pawn</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bishop/1538778833.610855.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bishop/1538777891.4931645.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bishop/1539017656.6888525_5.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bishop/1538778427.8668559.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bishop/1538777675.2653856.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bishop/1538778093.2397656.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bishop/1539024725.794323.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bishop/1539026035.275597.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bishop/1539024424.9014952.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bishop/1539024039.3749897.jpg</td>\n",
       "      <td>bishop</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>square/1538775589.5684185.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>square/1538775377.837528.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>square/1538775473.8751059.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>square/1538775541.8031077.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>square/1538775563.8003812.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>square/1538775413.002367.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>square/1538775546.3549168.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>square/1538775428.0259957.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>square/1538775496.149376.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>square/1538775553.2582464.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>square/1538775431.6329722.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>square/1538775518.2826645.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>square/1538775528.933519.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>square/1538775501.9954052.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>square/1538775593.2157.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>square/1538775575.8332133.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>square/1538775381.8908746.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>square/1538775482.290001.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>square/1538775527.3804672.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>square/1538775383.3700583.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>square/1538775497.691954.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>square/1538775433.0617445.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>square/1538775459.9201775.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>square/1538775578.4874399.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>square/1538775356.6463027.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>square/1538775585.983797.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>square/1538775472.3628545.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>square/1538775359.6263092.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>square/1538775583.9372883.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>square/1538775410.2587154.jpg</td>\n",
       "      <td>square</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Filename Predictions   Truth\n",
       "0      bishop/1538778538.3017461.jpg      bishop  bishop\n",
       "1       bishop/1539025832.837544.jpg      bishop  bishop\n",
       "2      bishop/1539025486.6533895.jpg      bishop  bishop\n",
       "3      bishop/1538779519.5865016.jpg      bishop  bishop\n",
       "4      bishop/1538778643.4925923.jpg      bishop  bishop\n",
       "5       bishop/1539023363.830272.jpg      bishop  bishop\n",
       "6       bishop/1539025801.510389.jpg        pawn  bishop\n",
       "7      bishop/1538778410.6081142.jpg      bishop  bishop\n",
       "8      bishop/1538778215.8878355.jpg      bishop  bishop\n",
       "9       bishop/1538778752.467536.jpg        king  bishop\n",
       "10     bishop/1539025373.0113628.jpg      bishop  bishop\n",
       "11      bishop/1539023140.397658.jpg      bishop  bishop\n",
       "12     bishop/1539025334.2429533.jpg      bishop  bishop\n",
       "13     bishop/1538778214.3746974.jpg      bishop  bishop\n",
       "14      bishop/1539023732.561326.jpg      bishop  bishop\n",
       "15     bishop/1538777556.4695792.jpg      bishop  bishop\n",
       "16     bishop/1539024760.3120384.jpg      bishop  bishop\n",
       "17     bishop/1538778091.9695802.jpg      bishop  bishop\n",
       "18      bishop/1539024593.521976.jpg      bishop  bishop\n",
       "19     bishop/1538779155.2682924.jpg        pawn  bishop\n",
       "20      bishop/1538778833.610855.jpg      bishop  bishop\n",
       "21     bishop/1538777891.4931645.jpg      bishop  bishop\n",
       "22   bishop/1539017656.6888525_5.jpg      bishop  bishop\n",
       "23     bishop/1538778427.8668559.jpg      bishop  bishop\n",
       "24     bishop/1538777675.2653856.jpg      bishop  bishop\n",
       "25     bishop/1538778093.2397656.jpg      bishop  bishop\n",
       "26      bishop/1539024725.794323.jpg      bishop  bishop\n",
       "27      bishop/1539026035.275597.jpg      bishop  bishop\n",
       "28     bishop/1539024424.9014952.jpg      bishop  bishop\n",
       "29     bishop/1539024039.3749897.jpg      bishop  bishop\n",
       "..                               ...         ...     ...\n",
       "305    square/1538775589.5684185.jpg      square  square\n",
       "306     square/1538775377.837528.jpg      square  square\n",
       "307    square/1538775473.8751059.jpg      square  square\n",
       "308    square/1538775541.8031077.jpg      square  square\n",
       "309    square/1538775563.8003812.jpg      square  square\n",
       "310     square/1538775413.002367.jpg      square  square\n",
       "311    square/1538775546.3549168.jpg      square  square\n",
       "312    square/1538775428.0259957.jpg      square  square\n",
       "313     square/1538775496.149376.jpg      square  square\n",
       "314    square/1538775553.2582464.jpg      square  square\n",
       "315    square/1538775431.6329722.jpg      square  square\n",
       "316    square/1538775518.2826645.jpg      square  square\n",
       "317     square/1538775528.933519.jpg      square  square\n",
       "318    square/1538775501.9954052.jpg      square  square\n",
       "319       square/1538775593.2157.jpg      square  square\n",
       "320    square/1538775575.8332133.jpg      square  square\n",
       "321    square/1538775381.8908746.jpg      square  square\n",
       "322     square/1538775482.290001.jpg      square  square\n",
       "323    square/1538775527.3804672.jpg      square  square\n",
       "324    square/1538775383.3700583.jpg      square  square\n",
       "325     square/1538775497.691954.jpg      square  square\n",
       "326    square/1538775433.0617445.jpg      square  square\n",
       "327    square/1538775459.9201775.jpg      square  square\n",
       "328    square/1538775578.4874399.jpg      square  square\n",
       "329    square/1538775356.6463027.jpg      square  square\n",
       "330     square/1538775585.983797.jpg      square  square\n",
       "331    square/1538775472.3628545.jpg      square  square\n",
       "332    square/1538775359.6263092.jpg      square  square\n",
       "333    square/1538775583.9372883.jpg      square  square\n",
       "334    square/1538775410.2587154.jpg      square  square\n",
       "\n",
       "[335 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42  1  1  3  0  0  0]\n",
      " [ 0 26  0  0  7  0  0]\n",
      " [ 2  2 54  1  2  0  0]\n",
      " [ 3  0  0 48  0  0  0]\n",
      " [ 2  3  0  0 25  2  0]\n",
      " [ 0  0  0  1  0 60  0]\n",
      " [ 0  0  0  0  0  0 50]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "import sklearn.metrics as metrics\n",
    "confusion_matrix = metrics.confusion_matrix(y_true=piece_test_iter.classes, y_pred=predicted_class_indices)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'bishop', 1: 'king', 2: 'knight', 3: 'pawn', 4: 'queen', 5: 'rook', 6: 'square'}\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xd4FNX6wPHvmx4ILfTeRClSBASx\nggXsYPkpCAg2LNdy7YgNy73Wey3Xrle5VsSOSlGwI0W6CCKIlCAgvYaQcn5/nNns7GZbSDa7yb6f\n58mzM2fKvjtJ5t05Z+YcMcaglFJKASTFOgCllFLxQ5OCUkqpYpoUlFJKFdOkoJRSqpgmBaWUUsU0\nKSillCqmSSEOichkERkR6zjKSkTGisibEa77jYhcHu2YVGIRkaEi8kWs46hMNCnEiIisFpFcEdkj\nIptEZJyIZAEYY04zxvyvAmPpKyJGRD7yK+/qlH9TUbGoyInIVBHpH2L5WOf317si4yovIjJSRH4o\nxfqtnM+b4ikzxrxljAl6jFRJmhRi6yxjTBbQHegJ3BXDWDYDfUSkrqtsBPBbjOKJG+6TTLwQkerY\nv5lvgywX4GJgm/OqVEQ0KcQBY8x6YDJwOJSsShGRS0VkmYhsd74dtnQt6yQiX4rINueKY4xTniQi\no0XkdxHZKiITRCQ7RBgHgI+Bwc72ycCFwFvulUTkaBH5SUR2Oq9Hu5a1FpFvRWS3iHwJ1PPb9igR\n+VFEdojIIhHpG8nxEZFeIjLT2W6DiDwjImkRHINkERnjHIPdIjJPRJoH+kbpPubON9QZIvKEiGwF\nxopIWxH5yjmWW0TkLRGp7dq+uYh8KCKbnXWeEZE0J6bOrvUaiMg+Eakf4HOuEZEezvRQJ8ZOzvxl\nIvKxa/WTgBnGmLwgh+04oDFwPTDY73j5VOv5Hw/n9/idc8ymiciznvVd614iIuucv8mrRORIEVns\n/I6e8ftcof5+jbP9CmfbZ8XqALyA/aKyR0R2OOufISILRGSX8/5jXW/1nfO6w9mmj/hdbYT5+/1G\nRB5wfve7ReQLEfH5G04EmhTigIg0B04HFgRYNhAYA5wL1Ae+B95xltUApgFTgCbAIcB0Z9PrgEHA\nCc6y7cCzYUJ5He+3ygHAEuBPVyzZwOfA00Bd4N/A5+K9ungbmIdNBg9grzQ82zZ1tn0QyAZuAT4I\ndHIMoBC40dlvH+wJ8ZoIjsFNwBDssa0JXArsi+D9AHoDq4CGwD8AAR5y3qMD0BwY68SQDHwGrAFa\nAU2B8caYA8B4YJhrv0OA6caYzQHe81ugrzN9gvP+x7vm3VcFp2OPZzAjgE+BCc78WSHW9fc2MAf7\nOx4LDA+wTm+gHfaLw5PAncDJQCfgAhE5AUL//bqcCRwJdAEuAAYYY5YBVwEzjTFZxhhPAt6L/Rut\nDZwBXC0ig5xlnmNV29lmpvtNIvj7BbgIuARoAKRh/04TizFGf2LwA6wG9gA7sCeT54BMZ9k3wOXO\n9GTgMtd2SdgTW0vsCWZBkP0vA05yzTcG8oGUAOv2BXKc6RXAYdiT2VDgcuAbZ9lwYI7ftjOBkUAL\noACo7lr2NvCmM3078IbftlOBEf6fOYJj93fgI2c61DFYDgwMUN4KMO5j4XfMRwJrw8QwyPO+2ES1\nOcix7Q2sBcSZnwtcEGSflwETXb+/y7HJBedvpLtr3bVA8yD7qQbsAgY58y8Cn7iWj/X8XvyPh+v3\nWM21/E3X79GzblPX8q3Aha75D4C/h/v7deYNcKxr+QRgtOv38EOY38OTwBMhfq/F+yDE36/rb+Au\n17JrgCkH+z9eWX/0SiG2BhljahtjWhpjrjHG5AZYpyXwlHNpvQNbRyzYb6PNgd+D7Lsl8JFru2XY\nb9wNw8T0BnAt0A/4yG9ZE+zJyW2NE0sTYLsxZq/fMnc8/+eJx4npWGyyCklEDhWRz0Rko4jsAv6J\nt2oq1DEItSycdX4xNBSR8SKy3onhTb8Y1hhjCvx3YoyZjT0J9hWR9tgrmYlB3vNb4DgRaQwkY0+Q\nx4hIK6AWsNCJpTOw0xizLsh+zsGe2Cc5828Bp0V4VdYE2GaMcV9RBXqfTa7p3ADzWc50qL9fj42u\n6X2ubUsQkd4i8rVTTbcTezURaRVPqL/fUsdSVWlSiH/rgCud5OH5yTTG/OgsaxNiu9P8tsswtv0i\nlDew35Am+Z0YwFYltfQrawGsBzYAdcQ2gLqXueN5wy+e6saYh8PEA/A88CvQzhhTE1sdIa79hjoG\nbQOUexJXNVdZI791/LsP/qdT1tmJYZhfDC0keIP0/5z1hwPvG2P2B1rJGLMSeyK6DvjOGLMLe5Ia\nhf22W+SsejreE34gI7Ans7UishF4D0jFVo2A/fzBPvsGIFtE3Mubh3ivcEL9/YYTqAvnt7FJtbkx\npha23UFCrO8W6u9XOTQpxL8XgDtcDY61ROT/nGWfAY1F5O8iki4iNcR7++ELwD88jXoiUt+p3w3J\nGPMHtv76zgCLJwGHishFIpIiIhcCHYHPjDFrsFUj94ltYD0W33rsN4GzRGSA2AbgDLG3wjaL4BjU\nwFaH7HG+bV/tWhbqGLwCPCAi7ZzGyy4iUtfY+vz1wDAnlksJnDz8Y9gD7HTaR251LZuDPZk+LCLV\nnc92jN9nPwebGF4P8z7fYq/UPO0H3/jNQ4j2BCe2k7D19N2cn67AI3jbixYCx4tICxGpBdzh2d71\nexzr/B77ULr2CH+h/n7D2QQ0E1cjOfb3sM0Ys19EeuFNdGCr8IoI/iUh6N9vKT5PladJIc4ZYz7C\n/kOPd6otlgCnOct2A6dg/2k3YtsD+jmbPoX9RvWFiOwGZmHrtyN5zx+MMX8GKN+KPdncjK1Hvg04\n0xizxVnlIuc9tgH34joBOlUdnkbHzdhvkLcS2d/gLc6+dwMvA++69hvqGPwbWwXzBTap/BfIdJZd\n4bz/VmzjaLhvrvdhbx3eiT0hf+iKodB5/0Owdf052AZY92efj/0m+32Y9/kWe+L7LtC82DueOoaI\ndziw0BjzhTFmo+cH27jaRUQON8Z8iT2Gi7E3BvifFIdi20m2Ym8MeBcIdpdTSKH+fiPwFfALsFFE\nPH9j1wD3O3/T9+BtSMe5sv0HMMOprjrKL5Zwf78Kb+OXUiqKRORV4E9jTJmeRRGRC4DzjTEXlE9k\nEb3nu8Cvxph7K+o9VezE3UM5SlU1TkPxucAR5bC7HcAT5bCfoETkSOzV3h9Af+wVXiRtP6oK0KSg\nVBSJyAPYZywectprysQYUxH9+DTCVo/VxVaFXW2MKfEMjaqaolZ95Fwunwn8ZYw5PMBywdZ7n469\n42KkMWZ+VIJRSikVkWg2NI8DTg2x/DTsE5HtsLfcPR/FWJRSSkUgatVHxpjvnLrUYAYCrxt7qTJL\nRGqLSGNjzIZQ+61Xr55p1SrUbpVSSvmbN2/eFmNM2AcYY9mm0BTfJyVznLISSUFERmGvJmjRogVz\n586tkACVUqqqEBH/p7kDqhTPKRhjXjLG9DTG9KxfP5In9ZVSSh2MWCaF9fg+Pt8MfdxcKaViKpZJ\nYSJwsdP9wFHYDr5CticopZSKrqi1KYjIO9gumeuJSA6224NUAGPMC9h+SE4HPJ2AXXKw75Wfn09O\nTg779wfsZ6xKysjIoFmzZqSmpsY6FKVUFRLNu4+GhFlugL+Vx3vl5ORQo0YNWrVqhX38oWozxrB1\n61ZycnJo3bp1rMNRSlUhlaKhOZz9+/dTt27dhEgIACJC3bp1E+rKSClVMapEUgASJiF4JNrnVUpV\njCqTFJRSqkItfg/274p1FOVOk0I52Lp1K926daNbt240atSIpk2bFs8fOHAgon1ccsklLF++PMqR\nKqVCWvQufPd4+PU2LIIPL4fP/h79mCqY9pJaDurWrcvChQsBGDt2LFlZWdxyyy0+6xQPip0UOA+/\n9tprUY9TKRXGR6Ps6/G3hF5v/077unuj/cnPheyqcdOHXilE0cqVK+nYsSNDhw6lU6dObNiwgVGj\nRtGzZ086derE/fffX7zusccey8KFCykoKKB27dqMHj2arl270qdPH/76668YfgqlVAlFBfY1KRn+\ndRg83Q1Wzwi87safYd7/Ki62MqpyVwr3ffoLS/8s33q+jk1qcu9ZnQ5q219//ZXXX3+dnj17AvDw\nww+TnZ1NQUEB/fr14/zzz6djx44+2+zcuZMTTjiBhx9+mJtuuolXX32V0aNHl/lzKFWlFORBwX4o\nLIBq2SACe7dC9brht9271btNYT4c2AOZdSJ/76JC++puUxh3OozdWXLdF461rz1GeMvyczH5uWz/\nTz/MMTdS9+jhNsHMeRmWfgIjnRFSNyyCl/rB/42DjmdHHl8Z6JVClLVt27Y4IQC88847dO/ene7d\nu7Ns2TKWLl1aYpvMzExOO80OY9ujRw9Wr15dUeEqVXm8cQ483AIeawNzXoI3zrXTm34Jvd3GJXa9\nBW/a+Y+uhEdaBV7XGPj5fZs43DxXChsWRh7vflfCePoI5NHWZOeupu60G2CaM9LppFtg9fc26eze\nBC8eD6YQJgwP/7nKSZW7UjjYb/TRUr169eLpFStW8NRTTzFnzhxq167NsGHDAj5rkJaWVjydnJxM\nQUFBhcSqVFwozIfPboTjb4U6LQOv89MrsMZVXTP5Nu/09tX2KmLxBDj1IVs2ZTR0HQJNusFfzhex\nVV9D9+Gw5AM7XxDgppBln8IHl8HW36Hv7d7yosD/k1eNuZcxN99O8+xM8jYuJ2PGY96FE6+H9Bpw\n3E2w27dHn50LP+HZH/cyxlOw5y+Y+YzPOjmrllCr5iHUyEwPfEzKSZVLCvFs165d1KhRg5o1a7Jh\nwwamTp3KqaeGGodIqRjYvNyeVF88DkZ8Bq2PC73+X7/ak3dqZvh95+fCjrVQ/zDve+XthldOgsun\nQ82m8O/2dtmCN+C8/0Ln873bf3A5/P417NsS/D2+uAu2rbLTLftAdhuY/YL9uWSK94S+5AP2LvuS\n4q9t+ftK7Kpg9Y/2JPnrp3DYaSzelMfhGz8madYzJdYFeCHtSU56vCk9unTh0eUDfBcu/dj7ufzU\n2reWMbzqLfAcA5dmU0cxe+1qel94e4ll5UmTQgXq3r07HTt2pH379rRs2ZJjjjkm1iEp5asgD57t\n5Z1/63y4agbUOyTw+rk74Lne0Pn/YMBDsOR96H2VrasP5JNr7Tp35EBKhu97vXJSyfW/fdQ3Kfz8\nXvjP4EkIABMu9l32xZ2Q5r16r17obRPY++7l3gQB5G/5g9Q5zoCQG3+GF4+jS/h3Z3r6reT8Wg+i\n8HxpSkaN8t+pn6iN0RwtPXv2NP6D7CxbtowOHTrEKKLYSdTPraJo/05bT++Wkgl3bYS/lsHbF8KI\niVCnlV22bRU8fYSdr90S/vgWrv4RGrqqcTf9Ah9dBUPegddOs1cKNZvCVT/Ao2Fu48zMtvur0QhW\nfAFvX1Cen7bS+bn3v+h82uUHta2IzDPG9Ay3nl4pKFVVFBXC8knQ/szg39TDCVSvXpBrX2c8BTvW\nwFNdYcyfsGYmFDkNsKnVIXe7nS7Mh01L7bKdOTDredi4GJ5wJYpd62HdnPDx5G6Df7dnTVEDWibp\nrdmmMLKHYctCk4JSVcWP/7F3sfzfOOh0ji3Lz7WNrCfda2/BDKcgRCeLyd4bIHjzfFj7Y/HsgT1b\n2JuSTR0g90ABmeNOCP9eebvDr+PQhGA1rRn9U7bekqpUZTJ5NDwSpMpl2+/2NXeHt2zROzBvHHz1\nYGT7L8gLvizFe9dLwfoFPovS9m0ibaety88cF6BtIJAPD64apCp5NP9CVnS7nWsOXM/FTT4Nu37d\n3hdFPSZNCkrFSlGRrVo5sDeCdQth5rMw+3lbpRLI6h/s66J3YNefdnq+c6eLKfRdd81MWPVt8eye\nPbuZ/eZYZv+6OuCuL7rr3xSs9Vb3pBTmllinuoRIKHFsUVGbsu3gsDPCrnJPnUcCll9+zW20GzSG\ny668kWcu7gOD3w6+k0ZdID3rYKOMmCYFpYIpzLcn7mhZ/rmt2pk2tuSyA3vh9YGwxqmiWfgWTB3j\nu47nW31BHuTv9951s242RZ/fwutfzoE/59uy/Fx4dzhMf8Cu/9qp8PrZ3PreIn6b8RFZjzej98on\n6D3tvIChvp1yHykbS/GgViUyrmBA+JUCOe+/cNsfULetnW9xdNBVrxh+MbcfNpn9t6zxKc+uVQuA\nHi2zqZmRCu3PgLrt4KynSu4ko9bBxVlK2qagVDAP1LO3Wp73SnT2n+98297n981/2Wfw7lA7veob\nuHeHtxHXY+vv8J/ucM6L9olcP0nLP+fi5Z97Cxa/653+3tsL6LR5S3nsl6vK8CEqh08Kj6ZNtxPo\n/PNDJZbtJy3AFvB2wYlclPJVyQXDPoRD3FVkzh2chw6ASyfbpPtgA1uWWg1OuI3m2dV4ZIiTNOq2\ng60r7HRKgAfRrnPurpz9on3Q7t4d8O0j0GNk+A9aDvRKoRz069ePqVOn+pQ9+eSTXH311UG3ycqK\n/mWgKgeR3Bcfibmv2oeu3MT59zN+VyOf3+w7/9WD8OW9vmVOlwdzJpeto7UFGVU/IZyd9wAPyJV0\nbha4of25EYG/4V9w3weYm3+Da2aHfoNDnQdQWx9vX1PSockRdnrEp3Dsjb7rt+3nnU6tFny/Iz+3\nz4iIQN/R9rbcCqBJoRwMGTKE8ePH+5SNHz+eIUNCDlOtqqr8XNi1Abavsd/owXbb8MYg2xGbpxM1\nz7f/Xz70vVrYt9V3f98/TvG3UY8JwwHotf9HKoOO+18NumyjCdARXe+r7B1TZXXuK0x86Hrm3j/I\n9zbdLoOhRR8AJDUDDi9ZbZaSnITUaAgN/J4u9r/dt9WxtiO8pt29ZQOftbcGNwrwuJvn2bDTH4fk\nEJU11bKh0eGhPl1UaFIoB+effz6ff/558YA6q1ev5s8//+SII47gpJNOonv37nTu3JlPPvkkxpGq\nCvHOENtNwVNdbBWPp28dsB2xPeWcKCZ5++x//aNPyduymsJfPvHe+1+FHAhRU73z9OdLFp72SMAT\ndalcNQO6/J93vqHrBNuyDySneufPf9We2E+PYICdWs3Dr9OwEwx+C1ICVU3F9wPDVa9NYfJo+0h6\neWrUGU57OOji7OxsevXqxeTJkxk4cCDjx4/nggsuIDMzk48++oiaNWuyZcsWjjrqKM4++2wdXznW\npoyBZj2g07neMv/fSWme9PesK2KnV/lVE71/qe987nZ4y/fJ3Cm/bOTM344hW/ZE/r4xcmzeU3yZ\ndiuZcoDvCw/nuOQlYbf5fvQp8KSdPjXvYaake7uCP6xZA/YcM5qsGQ9Dx4Fw1tN2QdpBVLH+fQn8\nNsVWuTX06xyzZR+4YbGdrt3CdnYHvj2gHnJy4P3evhoQyNtlty2LI4bbDv3anVK2/USJXimUE3cV\nkqfqyBjDmDFj6NKlCyeffDLr169n06ZNMY5UMetZe6J+sCHcV9tW6/jzr+f3yNsNOfPs9JYVsHM9\nzHvN7mfPX/BkJL3jACt826AMUikSAsA7t15AcloGAKPzr4hom8a1vXXnfxq/uv2kVLJOvAX6XAtn\nPgmZtW15sPENAo1Z4DlR124Ova6A0WsCP9Vdp6X9EfEmjYzarliCfE/OrGPjKmtCANtT69id3q5C\n4kzVu1II8Y0+mgYOHMiNN97I/Pnz2bdvHz169GDcuHFs3ryZefPmkZqaSqtWrQJ2la1ipNC5pXPV\nNyWXFRWWLAOYMAJ+nw7H3eK9i6ep053M9jWwc+1BhXJryrvhVyoHu00mNaTkMwYRy6xD8+xqxV8n\nH7/oKPgwxPpXfmcHwQG4YTEH8vZx28okmOZaJznV/gz4h++2QYauBaBdf9sX0rXz7HrptWDH6tJ9\nlhPvhjb9oPmRvrEkuKqXFGIkKyuLfv36cemllxY3MO/cuZMGDRqQmprK119/zZo1a8LsRR2Uwnxb\nJZPVIPDyH5+B+a/DtXMirxZyP+z1nx4wZDxbXx5E3bwcW+a6rZP19hbCKUv+5GA7Qu+etPIgtyyd\nv0ztsEnhsP3jWJ4xMvDCJk5jqnMl1efQJt5lx94EezZBm77w+1e2Gshdp16nJWnAsEZAny0UrZ0D\n3z9GUnbb4MEM+Ke9Ojv+VrjfdYUx5F3A2NHKPCIZcc0tOdX3TiAIfqWQQPQIlKMhQ4ZwzjnnFFcj\nDR06lLPOOovOnTvTs2dP2rcv2Ue6Kgef/h0Wvgl3/eV73/evk+yl+hd3estCdSi2ZaXt8K3tid5R\nuQC2roRnehLulLN6xoRy/49aXdSQVkmlq3LcbGpRXwJUsQBN69aA7RtYW1SfFkmbA66TF+S+fctJ\nqu3627umUjK8i7pf7B28vkuY3kyTU0lqfQy0DtN9fJ+/BS4PdRVRFoGeG0gwmhTK0aBBg3B3RV6v\nXj1mzpwZcN09eypH/XGl8ItTf1GQZ7uCqN0COg6C8X63BC+fDO8MDr6fcafDnk2Y0/+FuO4MitRV\nKZ+Veptw5prDaEXopOCfOOrVqw+Dp9lbW8df5NMtRkaS/YafWbMu7AmcFJY/eCoE6yrJ8/c96Hk4\neazvlUBWw3AfJ/6l14C6h3g7FExA2tCsKpfPb4Gxfo/7exqFiwpg+n12+MQHAnyvD5UQ5rxsqz6A\nlTM/Lqdgy25Sxplh16nXtpvPvNRvD/UPtXfb+I+G5jwwV7+964GtbN++f9KSk3iuxRO+2x3iuVPG\nSQqpGSWHykwL8SBWZXLdPDjxrlhHETOaFFTs7d8FXz/kbZB0m/WC9wEwgJ9eLrmOJylsXHzwMbiu\nDLZvjY9umjenNeORYeG7oM6q5h0v7K/mA+CcF7wLPQ2nA5+zr8ffZuvjT33YDn950zL7IJeLiHDN\npZfaZZdPhyHjvWMddzi7TJ9Jxb8qU31kjEmo+/8r24h5PvZtgy2/2aESr5kF3zwMc160t/11Ph+q\n17PrHdgHU263g7vcvMx3Hwvegk+uAUn2Ngq/PjD8eyen2RPbkveDrlKH+KjaK6regIZ1IugEzVVt\n06DLAFsF4uFpOG3eyw6M4xqKkmbOXVOe45eWZTtk86jZxP54jPkzcLcM3YYFH65TVTpV4kohIyOD\nrVu3Vu4TZSkYY9i6dSsZGRnhV45Hj7aGVwfY6po1M7wDu0y5HR5r6+0Z1FO++087OPwB18Dqn1xj\nX/27hA6jMPuQsL1N1pbIB385KFcE6GTNT46pxw9H/CuyWyRPujv4Mk9SKMz3TQhunttvj7kBzn0p\n+L7Sqge+93/QsyX791GVVpW4UmjWrBk5OTls3hy44awqysjIoFmzZrEOo/T8E3d+ru9thQA5c+0D\nRV+77lt/rne5vP28TYW0rXcg5J1EtfGOb/BZ4VGcmTyr1O9z84Gr+FfaCz5l+zoOpto5T4W+A8qx\npPkwzujTFSRAlZq/tOrQ4xL7EJ3/SfuYG+Djq6FW0+DbexKr/+9BJaQqkRRSU1Np3TrMAOAq9oyx\nT/66bfzZ9woAbN8//zsL9m0p1e7/KGpI6zC3b+4xmUz6eQPDQ/zlp4r36mNB0SFhk8Jvzf+Pxh2P\nocY399huEIAxw86gaNqXJG3ztodUa9LBNtBGcC/8qV2bQ2oykGyffp3zsu0gzb/LDI8T77ZXA37t\nA3S7yP6E4ql+yqqYXjhVfKsSSUFVEoGeEv7x6ZJl29eUOiEAYRMCwG6qMaPocIb7PFJbeoPy7ufj\n9HsAaDPyJVKSk6Bjf9i8DPL2ULfj8dBiKjzuqmv31OEH6xnzyu/gvZF2sBxP18seva6wSdU/KRx6\nmn2tXtdW4xyMXqPsg3/uvqDiUf32sPnXWEdR5VWJNgUVI+vmwH96lhxOsqiwZNnqH+D54CNT+Zhf\ntjECQtljMplS1IvnCkreRXO0vF6iLIXAbRYLjfdkn5Ls/BvVamo7VOvk9KWUVd+7wW1/2C6WQ2nc\nFa5fYNdt2qPkchEYvQ6aH2XnB78DF74Rep+RSEq2PZLG+40aV35nG7tVVGlSUKHlzIWdOYGXTb3T\njiDl3yvtpFvgn018h7KcdCtsWR69OAO4M79kVYtnlK29pmQj/cSbS3ZScWi3wE/crn74DNvfTqSq\n+XUCV93pkiPQmAH+67pl1PRWP6VnJVZfPSnpwRvLVbnRpKBCe+UkeKJTkIWeRmOB7/8Fa52697mv\n2dfCPPjtC5jxtB1WsIJNKuxVPL2l40jARnxcu3qkS8kxC+plleziYNB5w0ru+Ix/2dfrF8B180MH\n0eq4wHX1f5tttz3uptDbBxLv3+hVpaZtCqr0SjQYG5h+v528ZQXFyeIfsW24nHnfefDQVeSYehTU\n6UA9oGnKTt64rDd8OQlmYL+pT7/Pu9FFE+CDy4sbjJOTApyAj7zcvlavG74TtpFBur6olh36iiCU\ns56Cr//prUZSqhxF9UpBRE4VkeUislJERgdY3kJEvhaRBSKyWEROj2Y8KkJbf/cdLcxfod+37O//\n5Z1+vF10YnJsNTXCr+TISE/j08Of5Ly8sVSrYU/Afds6VT6eRl/Pq6ef/EMHwB3ryivc6KjbFs7/\nb5BRvZQqm6hdKYhIMvAscAqQA/wkIhONMe56hLuACcaY50WkIzAJaBWtmFQECgvguT626sc9HOK2\nP+D9S2wXCf71uiu+iHpYu0w1aso+lhc1J0ty6ZL0R8D1CiSNFON9DuCMc0fS7cRcGmyaDkBmivPN\nv8NZcPNyOxj66LWQ5Fc3n93WjhMM9hmA3O2wNH76RFIqWqJ5pdALWGmMWWWMOQCMB/z7ITBATWe6\nFqC3FsTavzt4B59xP2j2dDf4cwEsHh+TMYR3YbtXOEAqmzwDvXtux3S7dLLPbFKS2EFhxHkwy/2Z\najjVWxm1Snbmdv186D3KTp/1JFwQvTuilIon0WxTaAq4r8NzAP/HUscCX4jIdUB1IOAAqSIyChgF\n0KJFOQyHp4Lb6+oMLtBzBb98DF2HlCyPsmZin1s4QArFtfxHDIM9G+04CMmpkLuNlOwgDzF6GmeD\nDbMZiRuXerveUKqKinVD8xBgnDHmXyLSB3hDRA43xvc/1xjzEvASQM+ePROjg6N4sHVFybI/55ds\nU6hAf5q6NJGtzpyBUd/YybX3/I7+AAAfTUlEQVSzvZ3qBeJ0GV2mpBCqq4iyOOUBb3xKxVg0k8J6\noLlrvplT5nYZ2BEMjTEzRSQDqAfER9/FiSbPryO454Lc3RJB3z3hLClqxeFJqyNevzA1iyeTRvDS\n/iMZV+NF8M9LLXrbH4B6h9peWH2Uw5VCtBxzfawjUKpYNL+e/AS0E5HWIpIGDAYm+q2zFjgJQEQ6\nABlA4vRqF2v7ttk+hjYvh7cHw9sXRrTZ/FUbyvzW5x0Yy5H7nytRbm5ezu8ZJZ+LSE5O4eY7/sm0\n2wZwZGvnVs5gveKO+gZu/d23rDyuFJRKAFG7UjDGFIjItcBUIBl41Rjzi4jcD8w1xkwEbgZeFpEb\nsY3OI02i9H8dDxa9A398Z7ufKIqgN05H988CNPBG6LGmT7Nr9XzySGOzeyzgJkfAnwuQ6vVpe/sM\neLChbfBu3ht2/QkXvQtgG4093UoQ5E8lrXrJO6Q8YzTUP+ygY1cqEUS1TcEYMwl7m6m77B7X9FIg\nzMjdKipW/wCLxtvpUiSEsmrf6xSu+72et6DHSJg3DoZ/BFtWeLtvHvkZ/PcU6HcntAky+lhpvj80\n6QYXfwIt+hxs6EolhFg3NKto27vVfmtO9evrZ9wZgdcvo3vzRzCrqANT00s8qwhAketEfkaXxnDm\nk3D64/buoebebilo3gvu3hK4b59qTlLxH384nDZ9S7e+UglIb3moyoyBx9rYb9zLp5T77ucXHcL4\ngr4+ZQdIoSjYn1WbfhQWeZNC/44N7a2iwTp1C1Y+4B9w2qPQrv9BRK2UCkWTQlW2b5t93bgY3rnQ\n22tpqC4sSmHIgbt4ouB8ANYb2wdQASFG77roXY5rV5861VL58JqjGdjtIG/xTKsOva/UjuGUigJN\nClVZ7nbf+dXf2ddgo3eVUh6pbCKbVvvfZk5RewD6d2rCStOEaXUu8F35mtmQkk79GuksuKc/3VsE\neZ5AKRVTmhSqMv9B7V8fCCvLNuKYL+839Q4NbDcRp3RuzgvDj6TPVc/DiE/twprNoEH7cnxfpVS0\naFKoygJ1U7F9TdDVB9WawPeFh0e064mFvnfxtG/ZxE6kVWdAp0ZUT0+BOp4uJ/QuY6UqC00KVVkp\nbzVduKmA4fljwq63w1Tn+vzreOS8zt7C/g/a7hraDShtlEqpOKK3pFZl/tVHgFk6kUibZ6cVHsHJ\nyQtKlP+n0HapfeGRLZi27C+y0lPsMJHaXYNSlZ4mhaqsqGSXDvLHNxFvvrioLScnL+Dh/MGMTnUe\ndLt6Jn/LOoRhubbzoZcv7hl8BynOsxF120b8nkqp2NKkUNVsXAJblkP+fvjj24PaxX6TSobk81zh\n2RzZsxff/NqY0Xnj4dgboWFHsoHs6hGM+pVV3w5v6X4oTSkV1zQpVBWF+bYN4YWy9xrSNe9lBMO4\ny47h2Hb1mAKQuxrSa5V+Z4dqG4NSlYkmhari9YGwZsZBb76syA5e9NC5nVm0bgdXndCWVvVcncoF\nG6dAKVWlaFKoKsqQEABmOw+fndu9KUN66eh2SiUqvSW1stu+xrYjhDHiwO28VXBS0OXJ2Ebp9JQQ\n3VQopao8vVKo7J7rA/l7w65WSBKF/t8BOp0DDTrB1w+S37Q3BH+uTSmVIDQpVHYRJASwSaHI/wmF\nE++2t4t2H87I6g0ZruMbKZXwtPooQRSZJIwrKezJaOx9fqBGI5KShNRk/XNQKtHpWaASyT1QyKlP\nfse8NU7vp7k7It62ECG105nF83pNoJQKRJNCJfLrxl38unE393/6iy0oxbgIL43oxbAhF/NEl4kA\n7E1vFI0QlVKVnCaFSsRTvbM7r4D/vf5ftm7ZEPG22Vm2a+tGzVpz/YG/seS4Z6MSo1KqctOG5kok\nr6CQhmzjnh2P0Hf3IlhVio2TbEIZfGRz2tS7mV6ts6MTpFKqUtOkUBlsX82EuTlMXpfMuLRH6JC0\nrvT7EPv8gYjQu03dcg5QKVVVaFKoDJ7qygXAmvwL6JB6EAkBIEl/1Uqp8LRNId7t3VI8eWbyzIPf\nT1q1cghGKVXVaVKIZ8bAY96xCEpdbdTvLu90qiYFpVR4mhTiWe72g9vu2BtBkqD3ld6y1MzyiUkp\nVaVpUohnezeXfpuj/gYn3gP3brdDZHrolYJSKgKaFOLU+h25bJv1VkTrzmxzg3fm1H8W334KQNch\n9jVJez9VSoWnSSGOTP55A9e+PZ8P5+dwzMNfkT3vqbDb7ExtQNfB9wRfYeCzMCbyh9yUUolN71OM\nE7v353P1W/MB+Gxx5CfxWlnVIS3ErzEpWe88UkpFTK8U4sSevIJYh6CUUnqlEGtL1u/kowXrKSrr\nWAanPw5ZDcsnKKVUwtKkEGNn/ueHEmXDkr+kg6z1KZtd1J7eSb8G31GvK8o7NKVUAtLqozhTk708\nmPoaQ1Om+5TflX8p08+e5S0Y9U2FxqWUSgx6pVCBNu/Oo2ZmCukpycxYuYU/tpQcSrNv0sKA2+aR\nSvXaDaBFH1g7E9KdZxBEAq6vlFIHI2xSEJHrgDeNMQf5eK3yOPIf08hKT2He3Scz9JXZAdYwPJ0W\neJyD0ef0pnfrbGj6HuzMiW6gSqmEFUn1UUPgJxGZICKniuhX04NhnIbkPXkFHHbXlBLLO8gaVmcM\nDbr96T3bIyKQXgMadIhanEqpxBY2KRhj7gLaAf8FRgIrROSfItI25IbKx4HCopDLT03+KfQO/J9I\nzqxjXw87vQxRKaWUr4jaFIwxRkQ2AhuBAqAO8L6IfGmMuS2aAVYVBwpKJoUkbFkRSRSaIPm5XX84\n58WS5dWy4ZYVUE0HzFFKlZ+wVwoicoOIzAMeBWYAnY0xVwM9gPPCbHuqiCwXkZUiMjrIOheIyFIR\n+UVE3j6Iz1ApBEoK09JuYVXGMPomLaBN5u7AG7Y82iaAQLIaaJ9GSqlyFcmVQjZwrjFmjbvQGFMk\nImcG20hEkoFngVOAHGy7xERjzFLXOu2AO4BjjDHbRaTBwXyIePfbpt2M+fDnEuVtkjYCMC7tMXv9\nFcjRNwRZoJRS5S+SpDAZ2OaZEZGaQAdjzGxjzLIQ2/UCVhpjVjnbjQcGAktd61wBPOu5s8kY81cp\n4497hUWG/k98B8BrqY9QQ3IZemAM9dgZ2Q6S9FESpVTFiSQpPA90d83vCVAWSFPAPVRYDtDbb51D\nAURkBpAMjDXGlLg1R0RGAaMAWrRoEUHI8ePjBeuLp/slLwLgjbSH6JW0PFYhKaVUUJEkBTHG2zGP\nU21UXg+9pWDvbOoLNAO+E5HOxpgd7pWMMS8BLwH07NmzjJ0EVayb31tUoixsQjh5LHQ6RwfGUUpV\nuEhO7qtE5Hrs1QHANcCqCLZbDzR3zTdzytxygNnGmHzgDxH5DZskwtyfGf++/W0zRcZQl53Ukd2s\nNM0i37hmU6jTKmqxKaVUMJFUWF8FHI09oXuqgEZFsN1PQDsRaS0iacBgYKLfOh9jrxIQkXrY6qRI\nEk5c2p9fyMvfrSK/sIgRr87hktd+4pv0m5iWfhuXJX9eih1F2N6glFLlLOyVgtP4O7i0OzbGFIjI\ntcBUbHvBq8aYX0TkfmCuMWais6y/iCwFCoFbjTFbS/te8eKp6St4/pvf+cck2/5en+3UkFwA7k6N\nbGhNAPJzoxGeUkqFFUnfRxnAZUAnIMNTboy5NNy2xphJwCS/sntc0wa4yfmptDrcPYWLerdg4879\nxWXHJv3Mm2kPHdwONSkopWIkkuqjN4BGwADgW2zbQJAnrRJTbn4h//3hD/JdXVl0kTLUgrXXriuU\nUrERSVI4xBhzN7DXGPM/4AxK3lqasIqKvDdDufs3MkTYb+CFftVKo9dBo87lEZpSSpVaJEkh33nd\nISKHA7WAKvnkcWnt2p/PnNXFz/XxzfLNAPSQ5ZyWHKhr7AA6nAnDPvDOJ6eVZ4hKKVUqkdyS+pKI\n1AHuwt49lAXcHdWoKomRr85h/todJco/SL8vsh20Os6+HnKytyw1I/C6SilVAUImBRFJAnY53VB8\nB7SpkKgqgXXb9gVMCN3lt8h3kuQ6/Om1IE9vRVVKxVbIpOA8vXwbMKGC4qk0xnxUsoM7gA/Tx5Zi\nL66Hs69fAPn7yhSTUkqVVSTVR9NE5BbgXaB4UGFjzLbgm1Rdg56dwcJ1O+jQuGbZd2ZcSaF6XUDH\nRlBKxVYkSeFC5/VvrjJDAlYlFRUZFq6zVUYr/yqPu3IrVTdOSqkEEMkTza0rIpB4d/07C2hYM714\nPr/Q94T+0vAefDA/B34vxU77jimn6JRSqnxE8kTzxYHKjTGvl3848Wvioj9DLu/fqRH9UxdFnhTa\nnwkt+5Q9MKWUKkeRVB8d6ZrOAE4C5gMJlRQisurbyNctzA+/jlJKVbBIqo+uc8+LSG1gfNQiqkSE\nIkZ1NLTtcIRTEOYp5qEfwIovYN8WOFEf9VBKxZ+DGSxnL5Aw7QyfLFxPzczUgMuuSP6cO1a9A6d8\nBzQHCfOAeLuT7Y9SSsWpSNoUPsV7m0wS0JEEem7hhvELgy47MnmlndixFhp3haKCCopKKaWiI5Ir\nhcdd0wXAGmNMTpTiqVS6tahthx0yBn75CGY9F3zl29dUWFxKKXWwIkkKa4ENxpj9ACKSKSKtjDGr\noxpZJZBd3em8bsLw0Cv2GgWZtaMfkFJKlVEkvaS+BxS55gudsiorZ/s+Ppwf+mKoWZ1MkvdsjGyH\n4doalFIqTkRytkoxxhzwzDjTVbp/5yEvz+KmCYvIKygMvLxXC766oQ+snxfZDpv2LMfolFIqeiKp\nPtosImc7YyojIgOBLdENK7a27rE50DM+gsesO07ivUlfcN3iY6FBiO6xk9Ph74shdwekpEN2wtys\npZSq5CJJClcBb4nIM858DhDwKeeqIj0liX0HCrnyDXsl0KxOJq9f2otGtTK4bu/TdqVp9wbfwaVT\noEYj+6OUUpVIJA+v/Q4cJSJZzvyeqEcVY2kpvrVq6SlJtKmfZWdMUYAt/NQ9JApRKaVU9IVtUxCR\nf4pIbWPMHmPMHhGpIyIPVkRwsWCMYdOuPJ+ykzs29M5E8ixC0sE8E6iUUrEXydnrNGNMcXeexpjt\nInI6dnjOKmP+2u2c+9yPAZfdNqA9rJ0F6TWhKHDjs4+k5HKOTimlKkYkSSFZRNKNMXlgn1MA0sNs\nU+m8NmN1wPI61VJJThJ4dUDkO0sK3C2GUkrFu0iSwlvAdBF5DRBgJPC/aAYVC2nJgWvSHju/a+Q7\nuXsrJGvVkVKq8oqkofkREVkEnIztA2kq0DLagVW09NTASeHkjg19h80MRROCUqqSi/QstgmbEP4P\n+AP4IGoRxUi63x1Hb1/Rm3YNatiZgv2hN27UGS6dGqXIlFKq4gRNCiJyKDDE+dkCvAuIMaZfBcVW\nodJTfBuHD2tYg7pZ6ZC7HVZOD71xSgakVY9idEopVTFCXSn8CnwPnGmMWQkgIjdWSFQx8MK3vuNo\npnquHMYPgzU/hN5Yb0FVSlURoZ5TOBfYAHwtIi+LyEnYhuaEUNzwvGFR+JU1KSilqoigScEY87Ex\nZjDQHvga+DvQQESeF5H+FRVgRWhzx+cAHH9o/eKy4qSQvy/8DrQXVKVUFRH2bGaM2WuMedsYcxbQ\nDFgA3B71yCrArv35DH5pJkXOzUXGGD6//lhu6X8oSXk74MUTwAR5WC3L1a9RvzGB11FKqUqmVPUe\nxpjtwEvOT6U35eeNzFq1rXi+eloKnZrUotOuGfBIiO6uB70A3YbApqWQ8xO0OKoColVKqehL6Mrw\n/X7jJVRLd+5A+j3I3UanPw5t+kE9p8O7hh3tj1JKVREJXRm+P983Kdx7Zic7sW1V4A2yGngTglJK\nVUEJnRSWb9xDDfYxLPlL3r68F7WqpcLsl+D3rwJvoHcZKaWquIQ+y30wP4f/pL7CWcmzMGmDgfow\n+dbgG2hHd0qpKi5hrxSM059RPXYBIAV5oVa3khL2cCmlEkRUz3IicqqILBeRlSIyOsR654mIEZEK\nGeF+/Y5cHv9iufPexhNE+A1TtSsLpVTVFrXqIxFJBp4FTsGO6/yTiEw0xiz1W68GcAMwO1qx+Lt5\nwsLiW1FbZWfCDiJ7AE37N1JKVXHRvFLoBaw0xqwyxhwAxgMDA6z3APAIEKYr0vJTUOjtCrvRjvnO\nVARXCjWbRCcgpZSKE9FMCk2Bda75HKesmIh0B5obYz6PYhwlNKgZYOC4SKqPqtcr/2CUUiqOxOzu\nIxFJAv6NHckt3LqjgFEALVq0KPN7N6mVWbLwtdPgiOHBN8pqWOb3VUqpeBfNK4X1QHPXfDOnzKMG\ncDjwjYisBo4CJgZqbDbGvGSM6WmM6Vm/fn3/xaW2z3lobWiv5r4LFrzhnR72gW8iMEVlfl+llIp3\n0UwKPwHtRKS1iKQBg4GJnoXGmJ3GmHrGmFbGmFbALOBsY8zcKMYEwNuz1wLwj4EhuqhofQLctAxu\nWWnnqzeIdlhKKRVzUas+MsYUiMi12DGdk4FXjTG/iMj9wFxjzMTQe4gOn64tgvWAWvcQSHYeVMuq\nDwOfhbYnRj84pZSKsai2KRhjJgGT/MruCbJu32jG4pFfaKuBLurdAooKAq/k335wxLAoR6WUUvEh\n4R7RzXduRz20QRYUBblSCFaulFJVXMIlhd378wFnDOZg1UcZNSswIqWUih8JlxRuGL8QgB378qEo\nyB1FklyBESmlVPxIuKSwcN0OAIqKTPArhWDlSilVxSVcUvAoKDKwMsgIa9qmoJRKUAmcFIrg46sC\nL9QH1ZRSCSphk0Lq/m3BF2r1kVIqQSVcUshMtY3If9v6kLew6xDo5noWIVgDtFJKVXEJlxSSBC4/\ntjWpa7/zFqZlwYAH4fDz7HzjLrEJTimlYiyhxmguKjLsyy+kWrrfxy7YD5l14PxXoc/foGHn2ASo\nlFIxllBJYff+AoyBmhl+H7tatne6aY+KDUoppeJIQlUfrdu+D4Bmdar5Lug7JgbRKKVU/EmopLAz\n13ZxUadaqm+nd6kZMYpIKaXiS0IlhbwCe6tpemoyJKVAWg0Y+n6Mo1JKqfiRWEkh395qmp6SZLvN\nPvxcaHdKjKNSSqn4kVhJocAvKXgG0lFKKQUkXFKw1UcZqclQWGCrkJRSShVLqKSw37/6SJOCUkr5\nSKiksPeAHX6zxpLXIX8v7N8Z44iUUiq+JFRS2L73ABmpSaT9PN4W7FgT24CUUirOJFRS2LY3n+xq\naZCaaQvyc2MbkFJKxZmESgp78vKpkZEKmbVtwaEDYhuQUkrFmYRKCvsOFJKZlgyNu9mCo6+PbUBK\nKRVnEi4pVE9Phq8esAVJ+pyCUkq5JVRS2JtXQGaq6zbUpIT6+EopFVZCnRVz8wuplpYc6zCUUipu\nJVRSKK4+UkopFVBiJQX/6iOllFI+EiYpGGOH4tQrBaWUCi5hksIuZyjOTG1TUEqpoBImKSzO2QFA\nh8Y1YxyJUkrFr4RJClv3HACgZbYzPrN7OE6llFJAIiWFvTYpZFdPg+Q06DokxhEppVT8SZik0L5R\nDYYd1YKaGak66ppSSgWRMPdnHnNIPY45pB4UFYEp0gF2lFIqgIS5UihWZAfa0aSglFIlaVJQSilV\nLAGTQr591aSglFIlJGBSKLSv2tCslFIlRDUpiMipIrJcRFaKyOgAy28SkaUislhEpotIy2jGA0Ch\n50pBn2xWSil/UUsKIpIMPAucBnQEhohIR7/VFgA9jTFdgPeBR6MVT7HiNgW9UlBKKX/RvFLoBaw0\nxqwyxhwAxgMD3SsYY742xuxzZmcBzaIYj6VtCkopFVQ0k0JTYJ1rPscpC+YyYHKgBSIySkTmisjc\nzZs3ly0qbVNQSqmg4qKhWUSGAT2BxwItN8a8ZIzpaYzpWb9+/bK9WXH1kbYpKKWUv2jWoawHmrvm\nmzllPkTkZOBO4ARjTF4U47EKtfpIKaWCieaVwk9AOxFpLSJpwGBgonsFETkCeBE42xjzVxRj8dKG\nZqWUCipqScEYUwBcC0wFlgETjDG/iMj9InK2s9pjQBbwnogsFJGJQXZXfvSJZqWUCiqqZ0ZjzCRg\nkl/ZPa7pk6P5/gF5kkKyJgWllPIXFw3NFUrbFJRSKqjESwrapqCUUkEl1tflWc/D5uV2Wq8UlFKq\nhMQ6M05xdb+kzykopVQJiVd95KFPNCulVAmJkxTmvuY7r9VHSilVQuIkhdxtvvPa0KyUUiUkTlLo\ndI7vvLYpKKVUCYmTFGq3hEZdvPNafaSUUiUkTlJISoarvoeaniEbTEzDUUqpeJQ4ScEjo5Z99Yyr\noJRSqlji1aEMeQcWjYc6rWIdiVJKxZ3ESwp1WkLf22MdhVJKxaXEqz5SSikVlCYFpZRSxTQpKKWU\nKqZJQSmlVDFNCkoppYppUlBKKVVMk4JSSqlimhSUUkoVE2MqVx9AIrIZWHOQm9cDtpRjONGgMZZd\nvMcH8R9jvMcHGmNptTTG1A+3UqVLCmUhInONMT1jHUcoGmPZxXt8EP8xxnt8oDFGi1YfKaWUKqZJ\nQSmlVLFESwovxTqACGiMZRfv8UH8xxjv8YHGGBUJ1aaglFIqtES7UlBKKRWCJgWllFLFEiYpiMip\nIrJcRFaKyOgYxdBcRL4WkaUi8ouI3OCUZ4vIlyKywnmt45SLiDztxLxYRLpXYKzJIrJARD5z5luL\nyGwnlndFJM0pT3fmVzrLW1VAbLVF5H0R+VVElolIn3g7hiJyo/M7XiIi74hIRqyPoYi8KiJ/icgS\nV1mpj5uIjHDWXyEiI6Ic32PO73mxiHwkIrVdy+5w4lsuIgNc5VH7Xw8Uo2vZzSJiRKSeM1/hx7Bc\nGGOq/A+QDPwOtAHSgEVAxxjE0Rjo7kzXAH4DOgKPAqOd8tHAI8706cBkQICjgNkVGOtNwNvAZ878\nBGCwM/0CcLUzfQ3wgjM9GHi3AmL7H3C5M50G1I6nYwg0Bf4AMl3HbmSsjyFwPNAdWOIqK9VxA7KB\nVc5rHWe6ThTj6w+kONOPuOLr6PwfpwOtnf/v5Gj/rweK0SlvDkzFPlhbL1bHsFw+Y6wDqJAPCX2A\nqa75O4A74iCuT4BTgOVAY6esMbDcmX4RGOJav3i9KMfVDJgOnAh85vxRb3H9cxYfT+cfoY8zneKs\nJ1GMrZZzwhW/8rg5htiksM75p09xjuGAeDiGQCu/k26pjhswBHjRVe6zXnnH57fsHOAtZ9rnf9hz\nDCvifz1QjMD7QFdgNd6kEJNjWNafRKk+8vyTeuQ4ZTHjVBEcAcwGGhpjNjiLNgINnelYxf0kcBtQ\n5MzXBXYYYwoCxFEco7N8p7N+tLQGNgOvOdVbr4hIdeLoGBpj1gOPA2uBDdhjMo/4OYZupT1usfxf\nuhT7zZsQcVR4fCIyEFhvjFnktyhuYiyNREkKcUVEsoAPgL8bY3a5lxn71SFm9wmLyJnAX8aYebGK\nIYwU7OX788aYI4C92GqPYnFwDOsAA7EJrAlQHTg1VvFEKtbHLRQRuRMoAN6KdSxuIlINGAPcE+tY\nykuiJIX12Do/j2ZOWYUTkVRsQnjLGPOhU7xJRBo7yxsDfznlsYj7GOBsEVkNjMdWIT0F1BaRlABx\nFMfoLK8FbI1ifDlAjjFmtjP/PjZJxNMxPBn4wxiz2RiTD3yIPa7xcgzdSnvcKvx4ishI4ExgqJO4\n4im+ttjkv8j5n2kGzBeRRnEUY6kkSlL4CWjn3P2Rhm3Mm1jRQYiIAP8Flhlj/u1aNBHw3IEwAtvW\n4Cm/2LmL4Shgp+tSPyqMMXcYY5oZY1phj9NXxpihwNfA+UFi9MR+vrN+1L5tGmM2AutE5DCn6CRg\nKXF0DLHVRkeJSDXnd+6JMS6OoZ/SHrepQH8RqeNcEfV3yqJCRE7FVmWebYzZ5xf3YOfOrdZAO2AO\nFfy/boz52RjTwBjTyvmfycHeTLKRODmGpRbrRo2K+sHeCfAb9s6EO2MUw7HYy/PFwELn53Rs/fF0\nYAUwDch21hfgWSfmn4GeFRxvX7x3H7XB/tOtBN4D0p3yDGd+pbO8TQXE1Q2Y6xzHj7F3cMTVMQTu\nA34FlgBvYO+SiekxBN7BtnHkY09elx3MccPW7a90fi6JcnwrsfXvnv+XF1zr3+nEtxw4zVUetf/1\nQDH6LV+Nt6G5wo9hefxoNxdKKaWKJUr1kVJKqQhoUlBKKVVMk4JSSqlimhSUUkoV06SglFKqmCYF\npfyISKGILHT9lFtPmyLSKlAPm0rFi5TwqyiVcHKNMd1iHYRSsaBXCkpFSERWi8ijIvKziMwRkUOc\n8lYi8pXTZ/50EWnhlDd0xgBY5Pwc7ewqWUReFjvewhcikhmzD6WUH00KSpWU6Vd9dKFr2U5jTGfg\nGWxvsgD/Af5njOmC7bDtaaf8aeBbY0xXbP9Mvzjl7YBnjTGdgB3AeVH+PEpFTJ9oVsqPiOwxxmQF\nKF8NnGiMWeV0bLjRGFNXRLZgxyTId8o3GGPqichmoJkxJs+1j1bAl8aYds787UCqMebB6H8ypcLT\nKwWlSscEmS6NPNd0Idq2p+KIJgWlSudC1+tMZ/pHbG+cAEOB753p6cDVUDzmda2KClKpg6XfUJQq\nKVNEFrrmpxhjPLel1hGRxdhv+0OcsuuwI8Hdih0V7hKn/AbgJRG5DHtFcDW2h02l4pa2KSgVIadN\noacxZkusY1EqWrT6SCmlVDG9UlBKKVVMrxSUUkoV06SglFKqmCYFpZRSxTQpKKWUKqZJQSmlVLH/\nB7bKKELQVD0KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f747810bba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(piece_hist.history['acc'])\n",
    "plt.plot(piece_hist.history['val_acc'])\n",
    "plt.title('Piece Model accuracy w/ Augmentation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f74780ba160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(piece_hist.history['loss'])\n",
    "plt.plot(piece_hist.history['val_loss'])\n",
    "plt.title('Piece Model Log Loss w/ Augmentation')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42  1  1  3  0  0  0]\n",
      " [ 0 26  0  0  7  0  0]\n",
      " [ 2  2 54  1  2  0  0]\n",
      " [ 3  0  0 48  0  0  0]\n",
      " [ 2  3  0  0 25  2  0]\n",
      " [ 0  0  0  1  0 60  0]\n",
      " [ 0  0  0  0  0  0 50]]\n",
      "{0: 'bishop', 1: 'king', 2: 'knight', 3: 'pawn', 4: 'queen', 5: 'rook', 6: 'square'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cmatrix = confusion_matrix(piece_test_iter.classes,predicted_class_indices)\n",
    "print(cmatrix)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8FfXZ/vHPFRJ2BQQFAmigbriA\nC1B9EAUXUAsurYWiVvlpS2ttXVq3x2rFtmqttnWpWrFaEJdHigurClUChiISEJV9E9kVlS2gQJL7\n98dM4jFmOYcsMwfvN695Zc6cWa7MOdznm+/MmZGZ4ZxzLn1lRB3AOedc9Xghd865NOeF3Dnn0pwX\ncuecS3NeyJ1zLs15IXfOuTTnhdw55yIkqbmkMZIWS1ok6WRJB0iaImlZ+LNFZevwQu6cc9F6EHjN\nzI4EugKLgFuAN8zsMOCN8HGF5F8Ics65aEhqBswDOllCMZa0BOhtZhsktQVyzeyIitaTWftRa1/B\nr8+L3adRq0fnRR0h7TSt3zDqCOXa8uWOqCO4GlK4e52qu449n65Mut7UP/A7PwOGJkwabmbDEx53\nBDYB/5LUFZgDXAu0NrMN4TwbgdaVbWefKOTOORdHYdEeXsksmcAJwK/MbJakBynTjWJmJqnSDw/v\nI3fOuVQUFyU/VG0tsNbMZoWPxxAU9o/DLhXCn59UthIv5M45l4qiwuSHKpjZRmCNpJL+7zOAhcA4\n4PJw2uXA2MrW410rzjmXArPiml7lr4BnJdUHVgL/j6CRPVrSlcBHwMDKVuCF3DnnUlFcs4XczOYB\n3cp56oxk1+GF3DnnUlHzLfJq80LunHOpSO4gZp3yQu6cc6nwFrlzzqU3S+JslLrmhdw551JRwwc7\na4IXcuecS0UMu1b8C0HKoNGvH6DhlbcD0OCSX9P4lkdpdOPDNBh0DWTUizTe44/fx+rVc5kzZ0qk\nOcqKa64GDeozeeoYcmeMI2/WRG6+9ZqoI5Xq17c3C+ZPZ/HCPG668eqo43xNXLPFMlfNfrOzRiRV\nyCXlSJpfzvR/SjqqkuVyJZV3fmRsZJ06gOJP1pQ+Lpw7jZ1/+gVf3PcryKpP5kl9I0wHo0b9m/PO\nuyzSDOWJa65du3ZzYf/L6N3zPHr3PJ/Tz+zFid27Rh2LjIwMHnrwLvoPuJRju/Zh0KAL6Nz5sKhj\nAfHNFtdcWHHyQx2pVovczH5iZgtrKkxdU7OW1OvcjcK3v2pVFi2aUzpevHopGc1aRhGtVF7eO2ze\nvCXSDOWJay6AHTt2ApCVlUlWZiZxuFRzj+7Hs2LFKj78cDV79uxh9OixnDegX9SxgPhmi2uumvyK\nfk1JpZBnSno2vIPFGEmNS1rckupJGiFpvqQPJF2fsNwPJb0jaamkXgCSGkr6Vzjvu5L6hNOHSBob\nrneZpDtq8pctq8EFP2H3hBHlf3Jm1COzWx8KF8+tzQiuFmRkZDA1byyLVswkd+oM5ua/H3Ukstu1\nYc3a9aWP167bQHZ2mwgTfSWu2eKai+Li5Ic6kkohPwJ41Mw6A9uAXyQ8dxzQzsyOMbNjgX8lPJdp\nZj2A64CSwnw1wdUZjwUGAyMllVyMugfwA6ALwYdAuV0zkoZKypeU/9T7H6XwawTqHdUNK9hK8doV\n5T7f4KKfU7RyAcUfpu0fHN9axcXF9DnlfLp0PpUTTuzCkXH4c9ztM8yKkh7qSiqFfI2ZzQjHnwFO\nSXhuJdBJ0sOSziYo9CVeCn/OAXLC8VPCdWBmiwkuCnN4+NwUM/vMzL4Il03cTikzG25m3cys2xVd\nDknh1wjU63gU9Y7uQePbnqDBj2+k3mFdaHDJrwHI6vsj1KQZu8c+mfJ6XXxs27qdvLdmccaZvaKO\nwvp1G+nQPrv0cft2bVm/fmOEib4S12xxzZXufeRlOxpLH5vZZoJ7zeUCPwf+mTDfrvBnEcmd7ljh\ndmrS7olPs/P3V7Dzjz9l16j7KFr2Prue/SuZ3z2LzCOO58tn7ocY9K261LRs2YL9m+0HQMOGDTit\nT0+WLVsZcSqYnT+PQw/tSE5OB7Kyshg48HzGT5gcdSwgvtnimivdu1YOlnRyOH4xkFfyhKRWQIaZ\nvQjcRnBh9Mq8BVwSLns4cDCwJHzurPAO0o2AC4AZ5a+idjS46Bdov+Y0uubPNPrNA2T1HVSXm/+G\np59+mNzcVzj88E4sXz6LIUOizVMirrlatzmIVyaMYtp/xzEl90WmTZ3B5Ndyo45FUVER1153G5Mm\nPsf893MZM2Y8CxcujToWEN9scc0VxxZ5UjdflpQDvAbkAycSXPj8x8Ak4AZgD0G/eMkHw/+a2auS\ncoEbzCw/LPb5ZpYT9oc/RnDpxkLg12Y2VdIQguLdDGgPPGNmd1aVz+/ZuW/we3a62lYT9+z88p1/\nJ11vGvb4YbW3l4ykvtlpZquAI8t5qnfC+Dda4WbWO2H8U8I+cjP7kuDi6eVZa2YXJJPLOefqnH9F\n3znn0lwMv6Ifq0JuZiOAERHHcM65inmL3Dnn0pwXcuecS29WtCfqCN/ghdw551LhfeTOOZfmvGvF\nOefSnLfInXMuzXmL3Dnn0py3yGtH87/PqXqmOjbmgNOijlChiz6fFnWEcsX1q/BHtugQdYRyLd68\npuqZXM0rrLsbRiRrnyjkzjlXZ7xF7pxzaa6G+8glrQK2E1zqu9DMukk6AHiB4PpUq4CB4eXCy1Wt\ne3Y659y3Tu1cxraPmR1nZiV3RLsFeMPMDgPeCB9XyAu5c86lom5uLHE+MDIcH0lwee8KeSF3zrlU\n1HyL3IDJkuZIGhpOa21mG8LxjUDrylbgfeTOOZeKFM5aCQvz0IRJw81seJnZTjGzdZIOAqZIWpz4\npJmZpEpvZuGF3DnnUpHCvXzDol22cJedZ1348xNJLwM9gI8ltTWzDZLaAp9Utg7vWnHOuVTUYB+5\npCaS9isZB/oC84FxwOXhbJcDYytbj7fInXMuFTV7+mFr4GVJENTj58zsNUmzgdGSrgQ+AgZWthIv\n5M45l4oa/EKQma0EupYz/TPgjGTX44XcOedSUVQUdYJv8D7yUL++vVkwfzqLF+Zx041XR5ajUfYB\n9Hzxt5w+/c+cPu3PdPrJ2aXPdbqyL2e8dT+nT/szR98+OLKMJeKyz8qKY66c7xzMmDeeLh3eXv4G\nlw4dFHWsUnHcZxDTXHVzHnlKaqVFLikHmGBmxyRM6wZcZmbX1MY2qyMjI4OHHryLs88dzNq1G3h7\n5iTGT5jMokXL6jxLcWEx84c9y9YPVpHZpCG9J9/Fpukf0ODAZrTp142pZ9xC8e5C6rfav86zJYrT\nPkuHXKtWrOaiMy4rzfjme+N5Y1I8Ll4W130W11xxvIxtnbXIzSw/jkUcoEf341mxYhUffriaPXv2\nMHr0WM4b0C+SLLs+2cLWD1YBULjjS7YvW0fDNi3oePmZLHt4HMW7g3NYd3+6LZJ8JeK0z9IhV6KT\nenVjzap1bFi7MeooQHz3WVxz1dJX9Kul1gu5pE6S3pV0o6QJ4bRhkp6SlCtppaRrEua/XdISSXmS\nnpd0Q21nzG7XhjVr15c+XrtuA9nZbWp7s1Vq3KEVzY7JYfPcFTTt1IaWJx3BqZN+zykv307z4zpF\nmi2u+yyuuRKdc+FZTHp5ctQxSsV1n8U1lxVb0kNdqdVCLukI4EVgCDC7zNNHAv0ITn6/Q1KWpO7A\nDwiO4p4DdKMCkoZKypeUX1wcz+tYV0e9xg3o8c/r+eB3oygs+AJl1qN+86ZMP/d3zP/9c3QfHss/\nblwVMrMy6d23F5PHvxl1FLe3YthHXpuF/ECCk9gvMbP3ynl+opntMrNPCb611BroCYw1sy/NbDsw\nvqKVm9lwM+tmZt0yMppUK+j6dRvp0D679HH7dm1Zvz66P3uVWY8eT17PmpdmsGFS8Pn3xfrPWR+O\nb3l3BRQb9VvuF1nGuO2zEnHNVaLXGSez6IMlfLbp86ijlIrrPotrLoqKkh/qSG0W8q3AauCUCp7f\nlTBeRISnQs7On8ehh3YkJ6cDWVlZDBx4PuMnRPen7/F/G0rBsnWseHxS6bQNr+XTqudRADTp1AZl\nZbL7s+1RRYzdPot7rhLnXtg3Vt0qEN99FtdccWyR12bx3A1cCLwuqQBYX8X8ADOAxyXdE2brTxXX\nKagJRUVFXHvdbUya+Bz1MjIYMfIFFi5cWtubLdcBPY7g4B/2YuvC1fT5z90ALLxnNB89n8sJf/sZ\np+feS/HuQuZe81gk+UrEaZ+lQy6ARo0bcvKpPbjzhj9FHeVr4rrP4porjmetyFK4AEzSK004/VBS\nc2AK8AdgqJn1lzQMKDCz+8P55wP9zWxV+NzFwMcEXS6vmdkTlW0vs367ujuqkCS/Z+e+w+/Zue8o\n3L1O1V3Hzgd+lnS9aXzd49XeXjJqpUVuZquAY8LxLUD38Klx4bRhZeY/JuHh/WY2TFJjYDoQvzsr\nO+e+vWLYIo/jV/SHSzoKaAiMNLO5UQdyzrlSdXhaYbJiV8jN7OKoMzjnXIVieK2V2BVy55yLM/Ou\nFeecS3PeteKcc2muDq+hkiwv5M45lwpvkTvnXJor9IOdzjmX3rxrxTnn0px3rdSOxlkNoo7wDZdt\nfzvqCBUqmHZ/1BHK1bzPzVFHKNfqgk+ijuBixE8/dM65dOctcuecS3NeyJ1zLs35V/Sdcy691eW9\nOJPlhdw551Lhhdw559Kcn7XinHNpLoYt8tq8+bJzzu17ii35IQmS6kl6V9KE8HFHSbMkLZf0gqT6\nVa3DC7lzzqXAioqTHpJ0LbAo4fG9wN/M7FBgM3BlVSvwQu6cc6mowRa5pPbA94B/ho8FnA6MCWcZ\nCVxQ1Xq8kDvnXAqs2JIeJA2VlJ8wDC2zugeAm4CS5ntLYIuZFYaP1wLtqsrkhRxo164tEyY9yzv5\nrzNr9mtc9YshUUcqFbds5/zmb/zgtkcZePtjDB72+NeeG/nqf+k6ZBibt++IKB08/vh9rF49lzlz\npkSWoTxxex3L6te3NwvmT2fxwjxuuvHqqOOUimWuFFrkZjbczLolDMNLViOpP/CJmc2pbiQ/awUo\nLCrkt7fezXvzFtC0aROm543jzTfzWLJ4edTRYpntnzdfTov9mnxt2sbPtjJzwQratmwWUarAqFH/\n5rHHRvLkk3+LNEdZcXwdS2RkZPDQg3dx9rmDWbt2A2/PnMT4CZNZtGiZ5ypPzZ192BM4T9K5QENg\nf+BBoLmkzLBV3h5YV9WKvEUOfLxxE+/NWwBAQcEOlixZTnZ2m4hTBeKcLdF9z7/G9QPPQhHnyMt7\nh82bt0Sc4pvi/Dr26H48K1as4sMPV7Nnzx5Gjx7LeQP6RR0rtrmssDjpodL1mP2vmbU3sxzgR8Cb\nZnYJMBW4KJztcmBsVZmqXcgl5UhaLOlZSYskjZHUWNLvJM2WNF/ScAUOkjQnXK6rJJN0cPh4Rbjc\nCEkPSfqvpJWSLqo8Qc06+OB2dOl6NPmz59XlZpMSi2wSP79/FD+643HG5OYDMHXuYg5qsT9HHByP\nwhR3sXgdE2S3a8OatetLH69dtyEWHzJxzUVxCsPeuRn4taTlBH3mT1a1QE11rRwBXGlmMyQ9BfwC\n+LuZ/R5A0iigv5mNl9RQ0v5ALyAf6CUpj6CvaGdw0Ja2wCnAkcA4vjqCWyo8aDAUoEH9ltTP3L/a\nv0STJo0Z9dyj3HLTH9i+vaDa66tJcck24rdX0LrF/ny2rYCf3zeKjm1b8c8Jb/GPG34cWaZ0EpfX\n0e292rjWipnlArnh+EqgRyrL11TXyhozmxGOP0NQhPuEJ7V/QHA6zdHh8/8l6Bs6Fbg7/NkLeCth\nfa+YWbGZLQRal7fBxIMINVHEMzMzeea5Rxn9wjjGj3u92uurSXHK1rpFsK9b7t+U0084kvzFH7Fu\n02YG3v4Y5/zmb3y8eRs/uuNxPt2yPdKccRSn1zHR+nUb6dA+u/Rx+3ZtWb9+Y4SJAnHNVQct8pTV\nVCEv+xFlwKPARWZ2LPAEQWc+wHSCwn0IQd9PV4LCn1jIdyWM10m36yOP/YklS1bwyMNV/hVT5+KS\nbeeu3ez4Ylfp+MwFKzimYza5D9/Eq3+5nlf/cj2tW+zP/935M1o13y/SrHEUl9exrNn58zj00I7k\n5HQgKyuLgQPPZ/yEyVHHim2uVE4/rCs1VcgPlnRyOH4xkBeOfyqpKV913ENQsC8FlplZMfA5cG7C\nMnXupJO7Mfji73PqaSeTN3MCeTMn0Ldf76jifE2csn2+tYAhdz/FD29/jEvufIJeXQ6nZ5fDIslS\nkaeffpjc3Fc4/PBOLF8+iyFDBkUdCYjX61hWUVER1153G5MmPsf893MZM2Y8CxcujTpWbHPFsUUu\ns+p9akjKAV4j6O8+EVgI/Bi4FRgMbASWAh+Z2bBwmTXAH8xsuKRbgR+ZWZfwuRHABDMbEz4uMLOm\nlWXYv0mn+F3FJsY++c9dUUcoV1zv2Vm/XjzP0t25Z1fVM7mvKdy9rtp/4X/2vdOSrjctJ06rkx6F\nmnqHFprZpWWm3RYO32BmHRLG7yboKy95PKTMvJUWceecq0sWv6vY+heCnHMuJftiITezVcAx1Y/i\nnHPx5y1y55xLc17InXMuzVlR1Bei+CYv5M45lwJvkTvnXJqzYm+RO+dcWvMWuXPOpTkzb5E751xa\n8xZ5LYnjV6i3fBnd7c6q0vS0G6KOUK7tjw2OOkK59rvq+agjuBgp9rNWnHMuvfnBTuecS3NeyJ1z\nLs1V84KxtcILuXPOpcBb5M45l+b89EPnnEtzRX7WinPOpTdvkTvnXJrzPnLnnEtzftaKc86lOW+R\nO+dcmisqzog6wjfEL1EEGjSoz+SpY8idMY68WRO5+dZroo5Uql/f3iyYP53FC/O46caro47zNXHL\nVlRsDBoxjV+NmQXArI828aMR0xg4YhpDns1j9eZor38Tt/2VKK7Z4pjLLPmhrqRUyCXlSJqfwvyT\nJDWvYp5cSd3KmX6cpHNTybe3du3azYX9L6N3z/Po3fN8Tj+zFyd271oXm65URkYGDz14F/0HXMqx\nXfswaNAFdO58WNSxgHhme27OSjq23K/08V2TP+DuAScweshpnHNUe56YuTSybHHcXyXimi2uuYpN\nSQ9VkdRQ0juS3pO0QNKd4fSOkmZJWi7pBUn1K1tPrbbIzexcM9uyl4sfB9RJIQfYsWMnAFlZmWRl\nZmIxOKLRo/vxrFixig8/XM2ePXsYPXos5w3oF3UsIH7ZPt7+BW+t+ITvdzm4dJqAHbsKASjYtYcD\nmzaMKF389leiuGaLay4zJT0kYRdwupl1Jah5Z0s6CbgX+JuZHQpsBq6sbCV7XcgldZL0rqQbJb0k\n6TVJyyT9OWGeVZJaheO3S1oiKU/S85ISr6X6w/BTaamkXuGnz++BQZLmSRq0tzmTlZGRwdS8sSxa\nMZPcqTOYm/9+bW+yStnt2rBm7frSx2vXbSA7u02Eib4St2z3vbGA63p3Rgn/d+44uyu/HDOLvo9O\nYeKCtVzx3UMjyxe3/ZUortnimqsmu1YsUBA+zAoHA04HxoTTRwIXVLaevSrkko4AXgSGAJsIPkkG\nAccSFN8OZebvDvwA6AqcA5TtSsk0sx7AdcAdZrYb+B3wgpkdZ2YvlJNhqKR8Sflf7t66N7/G1xQX\nF9PnlPPp0vlUTjixC0fG4E84l5zpyz+mReP6HNXm6714z+Sv5O8XfZfJvziL847pwF/eXBhRQrcv\nSaVrJbFOhcPQsuuTVE/SPOATYAqwAthiZoXhLGuBdpVl2puzVg4ExgLfN7OFko4H3jCzrWGohcAh\nwJqEZXoCY83sS+BLSePLrPOl8OccICeZEGY2HBgO0Gr/w2usH2Tb1u3kvTWLM87sxeJFy2pqtXtl\n/bqNdGifXfq4fbu2rF+/McJEX4lTtnnrPmfa8o/JW/kfdhcVs2PXHn45ZharPi/g2OwWAPTrnM3V\n/54VST6I1/4qK67Z4porlbNWEutUJfMUAceFxxNfBo5MNdPetMi3AquBUxKm7UoYLyL1D4iS5fdm\n2Wpr2bIF+zcLDpI1bNiA0/r0ZNmylXUd4xtm58/j0EM7kpPTgaysLAYOPJ/xEyZHHQuIV7ZrTuvM\n5F+cxas/P5M/DTiB7ge34oHvd6dg1x4++jz4q/XtVZ/SsWXTSPJBvPZXWXHNFtdclsKQ0nqD44lT\ngZOB5pJKamF7YF1ly+5N0dwNXAi8LqmgqplDM4DHJd0TbrM/VXxKAduB/aqYp0a0bnMQf//HvdSr\nl0FGRgZjX36Vya/l1sWmK1VUVMS1193GpInPUS8jgxEjX2DhwujOvEgU52wAmRkZ/K5fV37zSj4Z\nEvs1zOLOc6I7EynO+yuu2eKaK5mzUZIl6UBgj5ltkdQIOIvgQOdU4CLg/4DLCXpBKl5PKmdnSMoB\nJpjZMeGfAVOAUcDhZvbLcJ4JwP1mlitpFdDNzD6VNAy4GPiYoC/oNTN7QlIucIOZ5YcHRvPNLEfS\nAcDrBJ3/95TXT16iJrtWakqc79kZV37PTlfbCnevq3YVntHmoqTrTc+NYyrdnqQuBAcz6xH0kIw2\ns99L6kRQxA8A3gUuNbNdFa0npRa5ma0CjgnHtwDdy5mnf8J4TsJT95vZMEmNgekE/eGYWe+E+T8l\n7CM3s8/LW79zzkWpuAbXZWbvA8eXM30l0CPZ9dRlf/RwSUcBDYGRZja3DrftnHM1wvgWX2vFzC6u\nq20551xtKfTrkTvnXHr7VrfInXNuX1CTfeQ1xQu5c86lwFvkzjmX5rxF7pxzaa7IW+TOOZfeYnin\nNy/kzjmXimJvkdeO3UWFVc9Ux5o3bBJ1hArF9fIBcf0q/LMte0cdoVw/3TYz6ggV2rmnwm+Tp73Y\nXQ+EfaSQO+dcXfGDnc45l+aK5V0rzjmX1oqiDlAOL+TOOZcCP2vFOefSnJ+14pxzac7PWnHOuTTn\nXSvOOZfm/PRD55xLc0XeInfOufTmLXLnnEtzcSzkGVEHiIN27doyYdKzvJP/OrNmv8ZVvxgSdaRS\nDRrUZ/LUMeTOGEferIncfOs1UUcq1a9vbxbMn87ihXncdOPVUccpFZdcjbIPoPeY33L2tD9zdu69\nHPaTfgAc/ZvvM2Duw/Sdcjd9p9xN29O7RpYR4v3+j8trmciU/FBXvEUOFBYV8ttb7+a9eQto2rQJ\n0/PG8eabeSxZvDzqaOzatZsL+1/Gjh07yczMZOLk5/nPlGnMmf1epLkyMjJ46MG7OPvcwaxdu4G3\nZ05i/ITJLFq0zHOFrLCY9+58ls0frCKzSUP6vv5HPp4+H4Clw19lyT8m1Xmm8sT1/R+n1zKRt8hj\n6uONm3hv3gIACgp2sGTJcrKz20Sc6is7duwEICsrk6zMTMyiP5O1R/fjWbFiFR9+uJo9e/YwevRY\nzhvQL+pYscr15Sdb2PzBKgAKd3zJtmXradSmRSRZKhPX93+cXstERSkMdSXlQi7pt5KWSsqT9Lyk\nGyTlSuoWPt9K0qpwvJ6k+yTNlvS+pJ8lrOfGhOl3htNyJC2S9ISkBZImS2pUQ79rUg4+uB1duh5N\n/ux5dbnZSmVkZDA1byyLVswkd+oM5ua/H3Ukstu1Yc3a9aWP167bEIv//HHN1bh9K5ofewifzV0B\nwGFX9KXfG/fQ/a8/JatZ44jTfSVO7/+4vpbFSn6oKykVckknAj8CjgPOBbpXsciVwFYz6x7O+1NJ\nHSX1BQ4DeoTrOlHSqeEyhwGPmNnRwBbgBxVkGSopX1L+7sJtqfwaFWrSpDGjnnuUW276A9u3F9TI\nOmtCcXExfU45ny6dT+WEE7twZOfDoo7kUpDZuAE9n7yOd383isKCL1g+8j9MPOl6Xj/zVr78ZAvH\n3XFJ1BGB+L7/46Y4haGupNoi7wW8bGY7zWwbMK6K+fsCl0maB8wCWhIU6r7h8C4wFzgynA7woZmV\nNAfmADnlrdjMhptZNzPrVj9z/xR/jW/KzMzkmeceZfQL4xg/7vVqr682bNu6nby3ZnHGmb2ijsL6\ndRvp0D679HH7dm1Zv35jhIkCcculzHr8z5PX8dFLM1g3KR+AXZ9uw4oNzFjxzFRaHv+dyPKViOP7\nP26vZYl9oZBXpDBhXQ0Tpgv4lZkdFw4dzWxyOP2ehOmHmtmT4TKJtxYpoo4OyD7y2J9YsmQFjzz8\nZNUz16GWLVuwf7P9AGjYsAGn9enJsmUrI04Fs/PnceihHcnJ6UBWVhYDB57P+AmTo44Vu1w9/vpT\nti9bx9LHXy2d1vCg5qXj7c/txtbFa6OI9jVxfP/H7bUsYSkMVZHUQdJUSQvD7uRrw+kHSJoiaVn4\ns9KDK6kWyenACEn3hMsOAB4HVgEnAu8AFyXM/zpwlaQ3zWyPpMOBdeH0P0h61swKJLUD9qSYpcac\ndHI3Bl/8febPX0zezAkA/H7Y/Ux+PTeqSKVatzmIv//jXurVyyAjI4OxL7/K5Neiz1VUVMS1193G\npInPUS8jgxEjX2DhwqVRx4pVrlY9Difnh73YsnA1fafcDcAH97zAwRf+D82PPgTM2LFmE/k3PRVJ\nvhJxff/H6bVMVMN934XAb8xsrqT9gDmSpgBDgDfM7E+SbgFuAW6uaCVK9QwISb8FLgc+AVYTdI1M\nAEYTtKAnApeaWY6kDOCPBAVfwCbgAjPbGn7y/CRcbQFwabj8BDM7JtzWDUBTMxtWWab9m3SK/jSO\nMurXi++ZnXG9Z2dc+T07UxfXe3YW7l5X7TJ8zyGXJl1v/vejZ1LanqSxwN/DobeZbZDUFsg1syMq\nWi7lamNmdwF3hRsdFk5bDHRJmO22cHoxcGs4lF3Pg8CD5WzimIR57k81n3PO1abiFC5kK2koMDRh\n0nAzG17BvDnA8QTHE1ub2YbwqY1A68q2E99mo3POxVAqBzHDol1u4U4kqSnwInCdmW1Twn1Bzcwk\nVfrpUa1CXlWXh3PO7Wtquh9XUhZBEX/WzF4KJ38sqW1C18onla3Dv9npnHMpqMnTDxU0vZ8EFpnZ\nXxOeGkdwLJLw59jK1uNdK845l4LCyns5UtUT+DHwQfh9GwiOKf4JGC3pSuAjYGBlK/FC7pxzKajJ\nMm5meVDh3ZzPSHY9Xsidcy6/fp/7AAASQklEQVQFcbz6oRdy55xLQSqnH9YVL+TOOZeC+JVxL+TO\nOZcS71qpJXH8OnAcM8VdZka9qCOU65LPcqOOUK4v1r8VdYQKNcqO/gqdtaUohm3yfaKQO+dcXfEW\nuXPOpTnzFrlzzqU3b5E751ya89MPnXMuzcWvjHshd865lBTGsJR7IXfOuRT4wU7nnEtzfrDTOefS\nnLfInXMuzXmL3Dnn0lyRxa9F7rd6C/Xr25sF86ezeGEeN914ddRxSsU1F8Qz2+OP38fq1XOZM2dK\n1FG+IU77a9v2Aq7/7R8ZMPinDLh4KPPmL2Lrtu385NpbOXfQlfzk2lvZum17pBkhXvusRDGW9FBX\nYlnIJRXU5fYyMjJ46MG76D/gUo7t2odBgy6gc+fD6jJCWuWC+GYbNerfnHfeZVHH+Ia47a8/PfAP\nen63G+Off4KXRj5Cp0M68M9Rozmp23FMeuFJTup2HE8+MzqyfBC/fVbCUvhXV2q9kCsQyw+MEj26\nH8+KFav48MPV7Nmzh9Gjx3LegH5Rx4ptLohvtry8d9i8eUvUMb4hTvtre8EO5rw3nx+E28/KymL/\n/Zoy9a2ZnH/OmQCcf86ZvDl9ZiT5SsRpnyWqyZsv15RaKbCSciQtkfQ0MB/4saQPJM2XdG/CfIPL\nm57wfCtJMyV9rzZylshu14Y1a9eXPl67bgPZ2W1qc5NJiWsuiHe2OIrT/lq3fiMtmjfjtrv+ykVD\nruZ39zzAzi++5LPNWziw1QEAtGrZgs8i/kCM0z5L9G3rWjkMeBQ4C/gDcDpwHNBd0gWSsoF7y04v\nWVhSa2Ai8Dszm1h25ZKGSsqXlF9cvKMWfw3n9i2FRUUsWrqcQRd+jzEjHqFRo4Y8Oerr3SiSkCq6\nJ/C327eta+UjM3sb6A7kmtkmMysEngVOrWQ6QBbwBnCTmZV71MrMhptZNzPrlpHRpFpB16/bSIf2\n2aWP27dry/r1G6u1zpoQ11wQ72xxFKf91eagVrQ+sBVdjj4SgL69T2Hh0uW0bNGcTZ9+DsCmTz/n\ngObNIslXIk77LFGRWdJDXanNQl6dZnIhMAeokw6x2fnzOPTQjuTkdCArK4uBA89n/ITJdbHptMwF\n8c4WR3HaX61aHkCbgw7kw4/WAvD2nHl8J+dgep9yEmNf/Q8AY1/9D316nRxJvhJx2meJ4ti1Uhfn\nkb8DPCSpFbAZGAw8XMl0CC4wdgXwb0k3m9k3+s9rUlFREddedxuTJj5HvYwMRox8gYULl9bmJtM6\nF8Q329NPP0yvXifTqlULli+fxR//+FdGjHgh6lix21+3Xn8VN9/5Z/YU7qFDdlv+cOv1mBm/uf1u\nXprwOtltDuIvf7g1snwQv31WIo5fCJLVQvNfUg4wwcyOCR8PBm4FBEw0s5urmF5gZk0lNQDGAWPN\n7NGKtpdZv138ztB3KYvrPTsLi4uijlAuv2dn6gp3r6t2x3//g7+XdL2ZsHpinRxoqJUWuZmtAo5J\nePw88Hw581U0vWn4cxd11L3inHPJ8BtLOOdcmquNXozqivUXdZxzLm6KsKSHqkh6StInkuYnTDtA\n0hRJy8KfLapajxdy55xLQQ2ftTICOLvMtFuAN8zsMILTsG+paiVeyJ1zLgVmlvSQxLqmA5+XmXw+\nMDIcHwlcQBW8j9w551JQBwc7W5vZhnB8I9C6qgW8Re6ccylI5Sv6iZcSCYehKW0raNZX+cnhLXLn\nnEtBKl+9N7PhwPAUN/GxpLZmtkFSW+CTqhbwFrlzzqWgDr6iPw64PBy/HBhb1QLeInfOuRTUZB+5\npOeB3kArSWuBO4A/AaMlXQl8BAysaj1eyF1sxPWr8HEV16/BA2x/7qqoI9SamvxCkJkNruCpM1JZ\njxdy55xLgX9F3znn0lxd3jAiWV7InXMuBUUWvwvZeiF3zrkUxPGiWV7InXMuBd5H7pxzac77yJ1z\nLs0Ve9eKc86lN2+RO+dcmvOzVpxzLs1514pzzqW5OHat+NUPQ/369mbB/OksXpjHTTdeHXWcUnHN\nBfHN5rlSF6ds59z3Ehc9NJ6BD0/g4kcmArB15y5+9tQUBvz1FX721BS2fbErsnzFZkkPdcULOZCR\nkcFDD95F/wGXcmzXPgwadAGdOx8WdazY5oL4ZvNcqYtjtieuPIvRv+rPc1d/D4Cnps/nu99py/hf\nX8B3v9OWp6YtiCxbKjeWqCuxKuSS6kWx3R7dj2fFilV8+OFq9uzZw+jRYzlvQL8ooqRFLohvNs+V\nujhnK5G7aC0Dju8EwIDjOzF10ZrIshRZUdJDXamykEtqImmipPckzZc0SNLZkhZLmivpIUkTwnmH\nSbohYdn5knLC8VckzZG0IPF2R5IKJP1F0nvAyZJOlDQtnPf18A4ZtSq7XRvWrF1f+njtug1kZ7ep\n7c1WKa65IL7ZPFfq4pZNgqv+9QaDH5nImHeWAvBZwRccuH9jAFrt14jPCr6ILF9N3ny5piRzsPNs\nYL2ZfQ9AUjNgPnA6sBx4IcltXWFmn0tqBMyW9KKZfQY0AWaZ2W8kZQHTgPPNbJOkQcBdwBVlVxZ+\nGAwFUL1mZGQ0STKGcy7O/vXTs2ndrDGfF3zBz//1Bh0PbPa15yUhFFG69P2K/gfAXyTdC0wAtgMf\nmtkyAEnPEBbUKlwj6cJwvANwGPAZUAS8GE4/AjgGmCIJoB6wgXIk3gsvs367au3Z9es20qF9dunj\n9u3asn79xuqsskbENRfEN5vnSl3csrVuFrS8D2jaiD5HdWD+2k9p2bQRm7bt5MD9G7Np204OaNow\nsnxxvGhWlV0rZrYUOIGgoP8ROK+S2QvLrLMhgKTewJnAyWbWFXi35DngS7PSziQBC8zsuHA41sz6\npvD77JXZ+fM49NCO5OR0ICsri4EDz2f8hMm1vdm0zQXxzea5UhenbF/s3sOOXXtKx2cu38ChrZtz\n2pHtGf/uSgDGv7uS3p3bR5IP4nnWSpUtcknZwOdm9oykLcAvgRxJ3zGzFUDirYpWAf3D5U4AOobT\nmwGbzWynpCOBkyrY3BLgQEknm9nMsKvlcDOr1UPURUVFXHvdbUya+Bz1MjIYMfIFFi5cWpubTOtc\nEN9snit1ccr2WcGX/PrZaQAUFhdzTpeO9Dy8HUe3b8VNz0/n5TnLyW7ehD//6NRI8kE8zyNXVX8m\nSOoH3AcUA3uAq4BWwAPATuAt4Dtm1j/s/x4LtANmAScD5xB0j7wC5BAU6+bAMDPLlVRgZk0Ttncc\n8BBB8c8EHjCzJyrLWN2uFedczYrrPTsbXXRbtTvXD2x2RNL1ZtPWJXXSmV9li9zMXgdeL+epI6G0\n2+SGcN4vgIq6Qs6pYP1NyzyeB0T3ceucc5WIYx+5f0XfOedSsE9ea8XMcoHcaidxzrk04C1y55xL\nc+l6HrlzzrmQt8idcy7N+Y0lnHMuze2TBzudc+7bJI5dK7G6jK1zzsVdTV+PPLya7BJJyyXdsjeZ\nvEXunHMpqMkWeXgPhkeAs4C1BFeGHWdmC1NZjxdy55xLQQ33kfcAlpvZSgBJ/wecD3z7Cnnh7nU1\ndj0DSUPDS+TGiudKTVxzQXyzea7kpFJvEu+bEBpe5ndpByTe7mgt8N1UM3kf+Tclc231KHiu1MQ1\nF8Q3m+eqYWY23My6JQy18oHkhdw556KzjuBGOyXah9NS4oXcOeeiMxs4TFJHSfWBHwHjUl3JPtFH\nXsNi0xdXhudKTVxzQXyzea46ZmaFkn5JcKnwesBTe3MjnSpvLOGccy7evGvFOefSnBdy55xLc/tc\nIZeUI2l+OdP/KemoSpbLldStdtOVu91v5JXUTdJDdZ0ljip6PSuZf5Kk5lXMU+5rLek4SefuTc59\nmaSCqDO4yn1rDnaa2U+izpAsM8sH8qPOkY7MrDqF+DigGzCphuLEjiQRHBuL37VYa4CkemZWFHWO\nurbPtchDmZKelbRI0hhJjUtaYZLqSRohab6kDyRdn7DcDyW9I2mppF4AkhpK+lc477uS+oTTh0ga\nG653maQ7qhtaUqdwGzdKmhBOGybpqXA7KyVdkzD/7eHFdvIkPS/phhrIkCNpcTn773eSZof7bbgC\nB0maEy7XVZJJOjh8vCJcboSkhyT9N8x/UQ3tn5ckvRbu+z8nzLNKUqsk9s/XXuvw1K/fA4MkzZM0\nKIk8vw2XL11/YmtfUitJq8LxepLuC/fh+5J+lrCeGxOm35nwOiyS9ISkBZImS2q0l/stJ9wPTwPz\ngR+H7+f5ku5NmG9wedMTnm8laaak76W4/SaSJkp6L1z3IAUXilosaW74/kh8v9+QsOx8STnh+CuS\n5oT7Y2jCPAWS/iLpPeBkSSdKmhbO+7qktinusvRjZvvUAOQABvQMHz8F3EBwX9FuwInAlIT5m4c/\nc4G/hOPnAv8Jx39DcEoQwJHAaqAhMATYALQEGhH8B+m2l3nnA0cA7wJdgd7AhPD5YcB/gQZAK+Az\nIAvoDswLs+wHLANuqMX9d0DCPKOAAeH4AmB/4JcE58ReAhwCzAyfHwH8m6DRcBTBdSWqu3+GACuB\nZuHv/xHQIZx/VbifKtw/lbzWQ4C/J5nrROADoHH4+y9PfJ+F87QCVoXjQ4HbwvEGBH9xdQT6Epxe\np3AfTQBODX/vQuC4cJnRwKXVeE2LgZOAbIL38IEEf5G/CVxQ0fRw+QKgNTALOGsvtv8D4ImEx80I\nvpZ+WPh7j+br7/cbEuadD+SE4weEP0v+v7UMHxswMBzPIvj/cmD4eBDh/999edhXW+RrzGxGOP4M\ncErCcyuBTpIelnQ2sC3huZfCn3MI3vyEyz4DYGaLCYrG4eFzU8zsMzP7Ilw2cTupOBAYC1xiZu+V\n8/xEM9tlZp8CnxD8p+oJjDWzL81sOzB+L7ddnvL2Xx9JsyR9AJwOHB0+/98wy6nA3eHPXsBbCet7\nxcyKLbiiW+u9yFPe/nnDzLaa2ZcEFxg6pMwyVe2f8l7rVPQCXjaznWa2jaq/xNEXuEzSPIKC2JKg\nkPUNh3eBuQSNhcPCZT40s3nVzFniIzN7m+ADLtfMNplZIfAswWtW0XQIiuMbwE1mNmUvtv0BcJak\nexX8pduR4HdbZkG1fSbJ9VwTtrrfJvg2ZMl+KgJeDMePAI4BpoT7+jaCb0vu0/bVPvKyJ8eXPjaz\nzZK6Av2AnwMDgSvCp3eFP4tIbt9UuJ0UbSVoDZ1C+Vc925Uwnmy26ijv93qUoKW5RtIwgpYuwHSC\nonYIQbG9OZx/YsLyifn35gJn5e2f6u6TVF/rZBXyVZdlw4TpAn5lZq8nziypH3CPmT1eZnoO3/wd\n96prJbSjGssWEnyQ9AOmpbqwmS2VdALBXz9/JPhQqGxbiQ3MhgCSegNnAieb2U5JuXy1f7+0r/rF\nBSwws5NTzZnO9tUW+cGSSl7Ii4G8kifC/tMMM3uR4NP6hCrW9RZBdwGSDgcOBpaEz50l6YCw7/IC\nYEb5q6jSbuBCghbbxUkuMwMYoKAPvynQfy+3XZ6K9t+n4bYS+7nfAi4FlllwAO1zgv+wedScuto/\n2wm6YZIxHbhAUiNJ+wEDwumrCLpd4Ov76XXgKklZELyXJDUJp18RZkRSO0kHJZlhb7wDnBb2d9cD\nBhMU54qmQ/DBfAVwpKSbU92gpGxgp5k9A9wH/A+QI+k74SyDE2ZfRfh/Miz+HcPpzYDNYRE/kqCb\nqDxLgANL3r+SsiQdXcG8+4x9tUW+BLha0lMELbjH+Oo/WjvgX5JKPsT+t4p1PQo8FnYpFAJDzGyX\nJAje/C8S/On2jAVnm+wVM9shqT8wBfhDEvPPljQOeB/4mODP1617u/0yytt/LQj6JTcS9IWX5Fil\nYGdMDyflAe3NbHMNZSnZTuL+GZXE/Huzf6YCt4R/kt9jZi9Usv65kl4A3iPo7irZJ/cDo8ODcYl/\nlfyToGtkbri/NhH0QU+W1BmYGb6nCgg+GGvlzAsz26DgLjRTCVqvE81sLEBF08PliiQNBsZJ2m5m\nj6aw2WOB+yQVA3uAqwiOH0yUtJOgMVDyAfoiwQf2AoIuqKXh9NeAn0taRPD+fLuC32+3ggPqD0lq\nRlDjHiA4lrPP8q/o7yVJQwi6Gn4ZYYamZlYgqTFBIR1qZnOruc4cggNPx9RAxEjVxv6pZFvDgAIz\nu7821r8vC7tNbjCzmvyr8ltlX22Rf1sMV/Alp4bAyNoqUmnM94/7VvAWuXPOpbl99WCnc859a3gh\nd865NOeF3Dnn0pwXcuecS3NeyJ1zLs39f9ULzL0GiokNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7478070080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(cmatrix, annot=True, xticklabels=['bishop','king','pawn','knight','queen','rook','square'],yticklabels=['bishop','king','pawn','knight','queen','rook','square'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.269091051679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss_result = log_loss(piece_test_iter.classes,piece_pred)\n",
    "print(log_loss_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
